/home/maleci/.local/lib/python3.6/site-packages/pkg_resources/__init__.py:119: PkgResourcesDeprecationWarning: 0.18ubuntu0.18.04.1 is an invalid version and will not be supported in a future release
  PkgResourcesDeprecationWarning,
/home/maleci/.local/lib/python3.6/site-packages/torchvision-0.10.0-py3.6-linux-aarch64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The _functional_video module is deprecated. Please use the functional module instead.
  "The _functional_video module is deprecated. Please use the functional module instead."
/home/maleci/.local/lib/python3.6/site-packages/torchvision-0.10.0-py3.6-linux-aarch64.egg/torchvision/transforms/_transforms_video.py:26: UserWarning: The _transforms_video module is deprecated. Please use the transforms module instead.
  "The _transforms_video module is deprecated. Please use the transforms module instead."
INFO:benchmark:==== Benchmarking X3DLearner (xs) ====
INFO:benchmark:== Benchmarking learner.infer ==
ERROR:pytorch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:pytorch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
Machine info:
  cpu:
    architecture: aarch64
    cores:
      physical: 8
      total: 8
    frequency: 2.27 GHz
    model: ARMv8 Processor rev 0 (v8l)
  gpus: null
  memory:
    available: 25.35 GB
    total: 31.17 GB
    used: 8.80 GB
  system:
    node: xavier
    release: 4.9.140-tegra
    system: Linux

Model device: cuda:0
Allocated GPU memory prior to inference: 15445504 (14.73 MB)
Allocated GPU memory after to inference: 15445504 (14.73 MB)
Max allocated GPU memory during inference: 3897797632 (3.63 GB)
Warming up with batch_size=32:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=32:  20%|██        | 2/10 [00:00<00:00, 11.94it/s]Warming up with batch_size=32:  40%|████      | 4/10 [00:00<00:00, 12.42it/s]Warming up with batch_size=32:  60%|██████    | 6/10 [00:00<00:00, 12.55it/s]Warming up with batch_size=32:  80%|████████  | 8/10 [00:00<00:00, 12.42it/s]Warming up with batch_size=32: 100%|██████████| 10/10 [00:00<00:00, 12.08it/s]Warming up with batch_size=32: 100%|██████████| 10/10 [00:00<00:00, 12.20it/s]
Measuring inference with batch_size=32:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=32:   2%|▏         | 2/100 [00:00<00:07, 12.64it/s]Measuring inference with batch_size=32:   4%|▍         | 4/100 [00:00<00:08, 11.33it/s]Measuring inference with batch_size=32:   6%|▌         | 6/100 [00:00<00:07, 11.90it/s]Measuring inference with batch_size=32:   8%|▊         | 8/100 [00:00<00:07, 11.57it/s]Measuring inference with batch_size=32:  10%|█         | 10/100 [00:00<00:08, 10.27it/s]Measuring inference with batch_size=32:  12%|█▏        | 12/100 [00:01<00:08, 10.30it/s]Measuring inference with batch_size=32:  14%|█▍        | 14/100 [00:01<00:08, 10.33it/s]Measuring inference with batch_size=32:  16%|█▌        | 16/100 [00:01<00:07, 10.70it/s]Measuring inference with batch_size=32:  18%|█▊        | 18/100 [00:01<00:07, 11.08it/s]Measuring inference with batch_size=32:  20%|██        | 20/100 [00:01<00:07, 11.34it/s]Measuring inference with batch_size=32:  22%|██▏       | 22/100 [00:01<00:06, 11.56it/s]Measuring inference with batch_size=32:  24%|██▍       | 24/100 [00:02<00:06, 11.85it/s]Measuring inference with batch_size=32:  26%|██▌       | 26/100 [00:02<00:06, 11.78it/s]Measuring inference with batch_size=32:  28%|██▊       | 28/100 [00:02<00:06, 11.67it/s]Measuring inference with batch_size=32:  30%|███       | 30/100 [00:02<00:05, 12.06it/s]Measuring inference with batch_size=32:  32%|███▏      | 32/100 [00:02<00:05, 12.15it/s]Measuring inference with batch_size=32:  34%|███▍      | 34/100 [00:02<00:05, 12.24it/s]Measuring inference with batch_size=32:  36%|███▌      | 36/100 [00:03<00:05, 12.39it/s]Measuring inference with batch_size=32:  38%|███▊      | 38/100 [00:03<00:04, 12.60it/s]Measuring inference with batch_size=32:  40%|████      | 40/100 [00:03<00:04, 12.50it/s]Measuring inference with batch_size=32:  42%|████▏     | 42/100 [00:03<00:04, 12.42it/s]Measuring inference with batch_size=32:  44%|████▍     | 44/100 [00:03<00:04, 12.20it/s]Measuring inference with batch_size=32:  46%|████▌     | 46/100 [00:03<00:04, 12.19it/s]Measuring inference with batch_size=32:  48%|████▊     | 48/100 [00:04<00:04, 11.97it/s]Measuring inference with batch_size=32:  50%|█████     | 50/100 [00:04<00:04, 12.00it/s]Measuring inference with batch_size=32:  52%|█████▏    | 52/100 [00:04<00:03, 12.35it/s]Measuring inference with batch_size=32:  54%|█████▍    | 54/100 [00:04<00:03, 12.66it/s]Measuring inference with batch_size=32:  56%|█████▌    | 56/100 [00:04<00:03, 12.40it/s]Measuring inference with batch_size=32:  58%|█████▊    | 58/100 [00:04<00:03, 12.75it/s]Measuring inference with batch_size=32:  60%|██████    | 60/100 [00:05<00:03, 12.71it/s]Measuring inference with batch_size=32:  62%|██████▏   | 62/100 [00:05<00:03, 12.55it/s]Measuring inference with batch_size=32:  64%|██████▍   | 64/100 [00:05<00:02, 12.42it/s]Measuring inference with batch_size=32:  66%|██████▌   | 66/100 [00:05<00:02, 12.65it/s]Measuring inference with batch_size=32:  68%|██████▊   | 68/100 [00:05<00:02, 12.75it/s]Measuring inference with batch_size=32:  70%|███████   | 70/100 [00:05<00:02, 12.55it/s]Measuring inference with batch_size=32:  72%|███████▏  | 72/100 [00:06<00:02, 12.11it/s]Measuring inference with batch_size=32:  74%|███████▍  | 74/100 [00:06<00:02, 12.07it/s]Measuring inference with batch_size=32:  76%|███████▌  | 76/100 [00:06<00:02, 11.87it/s]Measuring inference with batch_size=32:  78%|███████▊  | 78/100 [00:06<00:01, 11.98it/s]Measuring inference with batch_size=32:  80%|████████  | 80/100 [00:06<00:01, 12.16it/s]Measuring inference with batch_size=32:  82%|████████▏ | 82/100 [00:06<00:01, 12.17it/s]Measuring inference with batch_size=32:  84%|████████▍ | 84/100 [00:07<00:01, 11.97it/s]Measuring inference with batch_size=32:  86%|████████▌ | 86/100 [00:07<00:01, 12.10it/s]Measuring inference with batch_size=32:  88%|████████▊ | 88/100 [00:07<00:00, 12.31it/s]Measuring inference with batch_size=32:  90%|█████████ | 90/100 [00:07<00:00, 12.51it/s]Measuring inference with batch_size=32:  92%|█████████▏| 92/100 [00:07<00:00, 12.28it/s]Measuring inference with batch_size=32:  94%|█████████▍| 94/100 [00:07<00:00, 12.20it/s]Measuring inference with batch_size=32:  96%|█████████▌| 96/100 [00:07<00:00, 12.39it/s]Measuring inference with batch_size=32:  98%|█████████▊| 98/100 [00:08<00:00, 12.62it/s]Measuring inference with batch_size=32: 100%|██████████| 100/100 [00:08<00:00, 12.85it/s]Measuring inference with batch_size=32: 100%|██████████| 100/100 [00:08<00:00, 12.07it/s]
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "8.149 \xB5s +/- 4.717 \xB5s [3.099 \xB5s, 25.988 \xB5s]"
      batches_per_second: 167.01 K +/- 83.64 K [38.48 K, 322.64 K]
    metrics:
      batches_per_second_max: 322638.76923076925
      batches_per_second_mean: 167005.46149121196
      batches_per_second_min: 38479.85321100918
      batches_per_second_std: 83640.87718799805
      seconds_per_batch_max: 2.5987625122070312e-05
      seconds_per_batch_mean: 8.149147033691406e-06
      seconds_per_batch_min: 3.0994415283203125e-06
      seconds_per_batch_std: 4.717119872216311e-06
  gpu_to_cpu:
    human_readable:
      batch_latency: "249.166 \xB5s +/- 117.620 \xB5s [168.085 \xB5s, 921.488 \xB5\
        s]"
      batches_per_second: 4.34 K +/- 804.45 [1.09 K, 5.95 K]
    metrics:
      batches_per_second_max: 5949.367375886525
      batches_per_second_mean: 4344.989833217205
      batches_per_second_min: 1085.201552393273
      batches_per_second_std: 804.4486154704803
      seconds_per_batch_max: 0.0009214878082275391
      seconds_per_batch_mean: 0.0002491664886474609
      seconds_per_batch_min: 0.00016808509826660156
      seconds_per_batch_std: 0.00011762032912553236
  on_device_inference:
    human_readable:
      batch_latency: 82.112 ms +/- 7.691 ms [71.126 ms, 123.313 ms]
      batches_per_second: 12.27 +/- 1.00 [8.11, 14.06]
    metrics:
      batches_per_second_max: 14.059606533857597
      batches_per_second_mean: 12.270608911618615
      batches_per_second_min: 8.109464029187315
      batches_per_second_std: 0.9967878760241653
      seconds_per_batch_max: 0.12331271171569824
      seconds_per_batch_mean: 0.0821117877960205
      seconds_per_batch_min: 0.07112574577331543
      seconds_per_batch_std: 0.007691479317305406
  total:
    human_readable:
      batch_latency: 82.369 ms +/- 7.707 ms [71.410 ms, 123.571 ms]
      batches_per_second: 12.23 +/- 0.99 [8.09, 14.00]
    metrics:
      batches_per_second_max: 14.003699326241845
      batches_per_second_mean: 12.232189614445026
      batches_per_second_min: 8.092487690431723
      batches_per_second_std: 0.9936963693311884
      seconds_per_batch_max: 0.12357139587402344
      seconds_per_batch_mean: 0.08236910343170166
      seconds_per_batch_min: 0.07140970230102539
      seconds_per_batch_std: 0.0077074720408578205

Inference energy: 25.83007706872622 J (7.175021407979505e-06 kWh)
Energy (batch_size=1):
  joules: 25.83007706872622
  kWh: 7.175021407979505e-06

Warming up with batch_size=32:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=32:  10%|█         | 1/10 [00:00<00:08,  1.10it/s]Warming up with batch_size=32:  20%|██        | 2/10 [00:01<00:07,  1.11it/s]Warming up with batch_size=32:  30%|███       | 3/10 [00:02<00:06,  1.14it/s]Warming up with batch_size=32:  40%|████      | 4/10 [00:03<00:05,  1.17it/s]Warming up with batch_size=32:  50%|█████     | 5/10 [00:04<00:04,  1.19it/s]Warming up with batch_size=32:  60%|██████    | 6/10 [00:05<00:03,  1.17it/s]Warming up with batch_size=32:  70%|███████   | 7/10 [00:06<00:02,  1.14it/s]Warming up with batch_size=32:  80%|████████  | 8/10 [00:06<00:01,  1.15it/s]Warming up with batch_size=32:  90%|█████████ | 9/10 [00:07<00:00,  1.09it/s]Warming up with batch_size=32: 100%|██████████| 10/10 [00:08<00:00,  1.07it/s]Warming up with batch_size=32: 100%|██████████| 10/10 [00:08<00:00,  1.12it/s]
Measuring inference with batch_size=32:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=32:   1%|          | 1/100 [00:00<01:32,  1.07it/s]Measuring inference with batch_size=32:   2%|▏         | 2/100 [00:01<01:30,  1.08it/s]Measuring inference with batch_size=32:   3%|▎         | 3/100 [00:02<01:27,  1.11it/s]Measuring inference with batch_size=32:   4%|▍         | 4/100 [00:03<01:24,  1.14it/s]Measuring inference with batch_size=32:   5%|▌         | 5/100 [00:04<01:21,  1.16it/s]Measuring inference with batch_size=32:   6%|▌         | 6/100 [00:05<01:19,  1.18it/s]Measuring inference with batch_size=32:   7%|▋         | 7/100 [00:06<01:17,  1.19it/s]Measuring inference with batch_size=32:   8%|▊         | 8/100 [00:06<01:16,  1.20it/s]Measuring inference with batch_size=32:   9%|▉         | 9/100 [00:07<01:15,  1.21it/s]Measuring inference with batch_size=32:  10%|█         | 10/100 [00:08<01:14,  1.21it/s]Measuring inference with batch_size=32:  11%|█         | 11/100 [00:09<01:13,  1.22it/s]Measuring inference with batch_size=32:  12%|█▏        | 12/100 [00:10<01:12,  1.22it/s]Measuring inference with batch_size=32:  13%|█▎        | 13/100 [00:10<01:11,  1.22it/s]Measuring inference with batch_size=32:  14%|█▍        | 14/100 [00:11<01:10,  1.22it/s]Measuring inference with batch_size=32:  15%|█▌        | 15/100 [00:12<01:09,  1.22it/s]Measuring inference with batch_size=32:  16%|█▌        | 16/100 [00:13<01:09,  1.22it/s]Measuring inference with batch_size=32:  17%|█▋        | 17/100 [00:14<01:11,  1.17it/s]Measuring inference with batch_size=32:  18%|█▊        | 18/100 [00:15<01:13,  1.12it/s]Measuring inference with batch_size=32:  19%|█▉        | 19/100 [00:16<01:12,  1.12it/s]Measuring inference with batch_size=32:  20%|██        | 20/100 [00:17<01:10,  1.13it/s]Measuring inference with batch_size=32:  21%|██        | 21/100 [00:17<01:09,  1.14it/s]Measuring inference with batch_size=32:  22%|██▏       | 22/100 [00:18<01:07,  1.16it/s]Measuring inference with batch_size=32:  23%|██▎       | 23/100 [00:19<01:05,  1.18it/s]Measuring inference with batch_size=32:  24%|██▍       | 24/100 [00:20<01:03,  1.19it/s]Measuring inference with batch_size=32:  25%|██▌       | 25/100 [00:21<01:02,  1.20it/s]Measuring inference with batch_size=32:  26%|██▌       | 26/100 [00:22<01:01,  1.20it/s]Measuring inference with batch_size=32:  27%|██▋       | 27/100 [00:22<01:00,  1.21it/s]Measuring inference with batch_size=32:  28%|██▊       | 28/100 [00:23<00:59,  1.22it/s]Measuring inference with batch_size=32:  29%|██▉       | 29/100 [00:24<00:58,  1.22it/s]Measuring inference with batch_size=32:  30%|███       | 30/100 [00:25<00:57,  1.22it/s]Measuring inference with batch_size=32:  31%|███       | 31/100 [00:26<00:56,  1.22it/s]Measuring inference with batch_size=32:  32%|███▏      | 32/100 [00:26<00:55,  1.22it/s]Measuring inference with batch_size=32:  33%|███▎      | 33/100 [00:27<00:54,  1.22it/s]Measuring inference with batch_size=32:  34%|███▍      | 34/100 [00:28<00:54,  1.22it/s]Measuring inference with batch_size=32:  35%|███▌      | 35/100 [00:29<00:53,  1.22it/s]Measuring inference with batch_size=32:  36%|███▌      | 36/100 [00:30<00:52,  1.22it/s]Measuring inference with batch_size=32:  37%|███▋      | 37/100 [00:31<00:51,  1.22it/s]Measuring inference with batch_size=32:  38%|███▊      | 38/100 [00:31<00:50,  1.22it/s]Measuring inference with batch_size=32:  39%|███▉      | 39/100 [00:32<00:49,  1.22it/s]Measuring inference with batch_size=32:  40%|████      | 40/100 [00:33<00:48,  1.23it/s]Measuring inference with batch_size=32:  41%|████      | 41/100 [00:34<00:48,  1.22it/s]Measuring inference with batch_size=32:  42%|████▏     | 42/100 [00:35<00:47,  1.23it/s]Measuring inference with batch_size=32:  43%|████▎     | 43/100 [00:35<00:46,  1.22it/s]Measuring inference with batch_size=32:  44%|████▍     | 44/100 [00:36<00:45,  1.22it/s]Measuring inference with batch_size=32:  45%|████▌     | 45/100 [00:37<00:44,  1.22it/s]Measuring inference with batch_size=32:  46%|████▌     | 46/100 [00:38<00:44,  1.22it/s]Measuring inference with batch_size=32:  47%|████▋     | 47/100 [00:39<00:43,  1.23it/s]Measuring inference with batch_size=32:  48%|████▊     | 48/100 [00:40<00:42,  1.22it/s]Measuring inference with batch_size=32:  49%|████▉     | 49/100 [00:40<00:41,  1.22it/s]Measuring inference with batch_size=32:  50%|█████     | 50/100 [00:41<00:40,  1.23it/s]Measuring inference with batch_size=32:  51%|█████     | 51/100 [00:42<00:39,  1.23it/s]Measuring inference with batch_size=32:  52%|█████▏    | 52/100 [00:43<00:39,  1.23it/s]Measuring inference with batch_size=32:  53%|█████▎    | 53/100 [00:44<00:38,  1.23it/s]Measuring inference with batch_size=32:  54%|█████▍    | 54/100 [00:44<00:37,  1.23it/s]Measuring inference with batch_size=32:  55%|█████▌    | 55/100 [00:45<00:36,  1.22it/s]Measuring inference with batch_size=32:  56%|█████▌    | 56/100 [00:46<00:36,  1.22it/s]Measuring inference with batch_size=32:  57%|█████▋    | 57/100 [00:47<00:35,  1.22it/s]Measuring inference with batch_size=32:  58%|█████▊    | 58/100 [00:48<00:34,  1.22it/s]Measuring inference with batch_size=32:  59%|█████▉    | 59/100 [00:49<00:33,  1.22it/s]Measuring inference with batch_size=32:  60%|██████    | 60/100 [00:49<00:32,  1.22it/s]Measuring inference with batch_size=32:  61%|██████    | 61/100 [00:50<00:31,  1.22it/s]Measuring inference with batch_size=32:  62%|██████▏   | 62/100 [00:51<00:31,  1.22it/s]Measuring inference with batch_size=32:  63%|██████▎   | 63/100 [00:52<00:30,  1.22it/s]Measuring inference with batch_size=32:  64%|██████▍   | 64/100 [00:53<00:29,  1.22it/s]Measuring inference with batch_size=32:  65%|██████▌   | 65/100 [00:53<00:28,  1.23it/s]Measuring inference with batch_size=32:  66%|██████▌   | 66/100 [00:54<00:27,  1.23it/s]Measuring inference with batch_size=32:  67%|██████▋   | 67/100 [00:55<00:26,  1.23it/s]Measuring inference with batch_size=32:  68%|██████▊   | 68/100 [00:56<00:26,  1.23it/s]Measuring inference with batch_size=32:  69%|██████▉   | 69/100 [00:57<00:25,  1.23it/s]Measuring inference with batch_size=32:  70%|███████   | 70/100 [00:57<00:24,  1.23it/s]Measuring inference with batch_size=32:  71%|███████   | 71/100 [00:58<00:23,  1.23it/s]Measuring inference with batch_size=32:  72%|███████▏  | 72/100 [00:59<00:22,  1.22it/s]Measuring inference with batch_size=32:  73%|███████▎  | 73/100 [01:00<00:22,  1.22it/s]Measuring inference with batch_size=32:  74%|███████▍  | 74/100 [01:01<00:21,  1.22it/s]Measuring inference with batch_size=32:  75%|███████▌  | 75/100 [01:02<00:20,  1.23it/s]Measuring inference with batch_size=32:  76%|███████▌  | 76/100 [01:02<00:19,  1.23it/s]Measuring inference with batch_size=32:  77%|███████▋  | 77/100 [01:03<00:18,  1.23it/s]Measuring inference with batch_size=32:  78%|███████▊  | 78/100 [01:04<00:17,  1.23it/s]Measuring inference with batch_size=32:  79%|███████▉  | 79/100 [01:05<00:17,  1.22it/s]Measuring inference with batch_size=32:  80%|████████  | 80/100 [01:06<00:16,  1.22it/s]Measuring inference with batch_size=32:  81%|████████  | 81/100 [01:06<00:15,  1.22it/s]Measuring inference with batch_size=32:  82%|████████▏ | 82/100 [01:07<00:14,  1.22it/s]Measuring inference with batch_size=32:  83%|████████▎ | 83/100 [01:08<00:14,  1.14it/s]Measuring inference with batch_size=32:  84%|████████▍ | 84/100 [01:09<00:14,  1.13it/s]Measuring inference with batch_size=32:  85%|████████▌ | 85/100 [01:10<00:13,  1.11it/s]Measuring inference with batch_size=32:  86%|████████▌ | 86/100 [01:11<00:12,  1.10it/s]Measuring inference with batch_size=32:  87%|████████▋ | 87/100 [01:12<00:11,  1.10it/s]Measuring inference with batch_size=32:  88%|████████▊ | 88/100 [01:13<00:11,  1.08it/s]Measuring inference with batch_size=32:  89%|████████▉ | 89/100 [01:14<00:10,  1.09it/s]Measuring inference with batch_size=32:  90%|█████████ | 90/100 [01:15<00:08,  1.12it/s]Measuring inference with batch_size=32:  91%|█████████ | 91/100 [01:16<00:08,  1.12it/s]Measuring inference with batch_size=32:  92%|█████████▏| 92/100 [01:16<00:07,  1.14it/s]Measuring inference with batch_size=32:  93%|█████████▎| 93/100 [01:17<00:06,  1.14it/s]Measuring inference with batch_size=32:  94%|█████████▍| 94/100 [01:18<00:05,  1.16it/s]Measuring inference with batch_size=32:  95%|█████████▌| 95/100 [01:19<00:04,  1.17it/s]Measuring inference with batch_size=32:  96%|█████████▌| 96/100 [01:20<00:03,  1.18it/s]Measuring inference with batch_size=32:  97%|█████████▋| 97/100 [01:21<00:02,  1.13it/s]Measuring inference with batch_size=32:  98%|█████████▊| 98/100 [01:22<00:01,  1.14it/s]Measuring inference with batch_size=32:  99%|█████████▉| 99/100 [01:22<00:00,  1.15it/s]Measuring inference with batch_size=32: 100%|██████████| 100/100 [01:23<00:00,  1.13it/s]Measuring inference with batch_size=32: 100%|██████████| 100/100 [01:23<00:00,  1.19it/s]
INFO:benchmark:learner.infer:
  device: cuda
  energy:
    batch_size_1:
      joules: 25.83007706872622
      kWh: 7.175021407979505e-06
    batch_size_32:
      joules: 19.65728767654101
      kWh: 5.460357687928058e-06
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 25.35 GB
      total: 31.17 GB
      used: 8.80 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  max_inference_memory: 3897797632
  post_inference_memory: 15445504
  pre_inference_memory: 15445504
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "8.149 \xB5s +/- 4.717 \xB5s [3.099 \xB5s, 25.988 \xB5s]"
          batches_per_second: 167.01 K +/- 83.64 K [38.48 K, 322.64 K]
        metrics:
          batches_per_second_max: 322638.76923076925
          batches_per_second_mean: 167005.46149121196
          batches_per_second_min: 38479.85321100918
          batches_per_second_std: 83640.87718799805
          seconds_per_batch_max: 2.5987625122070312e-05
          seconds_per_batch_mean: 8.149147033691406e-06
          seconds_per_batch_min: 3.0994415283203125e-06
          seconds_per_batch_std: 4.717119872216311e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: "249.166 \xB5s +/- 117.620 \xB5s [168.085 \xB5s, 921.488\
            \ \xB5s]"
          batches_per_second: 4.34 K +/- 804.45 [1.09 K, 5.95 K]
        metrics:
          batches_per_second_max: 5949.367375886525
          batches_per_second_mean: 4344.989833217205
          batches_per_second_min: 1085.201552393273
          batches_per_second_std: 804.4486154704803
          seconds_per_batch_max: 0.0009214878082275391
          seconds_per_batch_mean: 0.0002491664886474609
          seconds_per_batch_min: 0.00016808509826660156
          seconds_per_batch_std: 0.00011762032912553236
      on_device_inference:
        human_readable:
          batch_latency: 82.112 ms +/- 7.691 ms [71.126 ms, 123.313 ms]
          batches_per_second: 12.27 +/- 1.00 [8.11, 14.06]
        metrics:
          batches_per_second_max: 14.059606533857597
          batches_per_second_mean: 12.270608911618615
          batches_per_second_min: 8.109464029187315
          batches_per_second_std: 0.9967878760241653
          seconds_per_batch_max: 0.12331271171569824
          seconds_per_batch_mean: 0.0821117877960205
          seconds_per_batch_min: 0.07112574577331543
          seconds_per_batch_std: 0.007691479317305406
      total:
        human_readable:
          batch_latency: 82.369 ms +/- 7.707 ms [71.410 ms, 123.571 ms]
          batches_per_second: 12.23 +/- 0.99 [8.09, 14.00]
        metrics:
          batches_per_second_max: 14.003699326241845
          batches_per_second_mean: 12.232189614445026
          batches_per_second_min: 8.092487690431723
          batches_per_second_std: 0.9936963693311884
          seconds_per_batch_max: 0.12357139587402344
          seconds_per_batch_mean: 0.08236910343170166
          seconds_per_batch_min: 0.07140970230102539
          seconds_per_batch_std: 0.0077074720408578205
    batch_size_32:
      cpu_to_gpu:
        human_readable:
          batch_latency: "10.877 \xB5s +/- 7.394 \xB5s [5.245 \xB5s, 46.253 \xB5s]"
          batches_per_second: 116.88 K +/- 45.15 K [21.62 K, 190.65 K]
        metrics:
          batches_per_second_max: 190650.18181818182
          batches_per_second_mean: 116883.30904799167
          batches_per_second_min: 21620.123711340206
          batches_per_second_std: 45152.36555495278
          seconds_per_batch_max: 4.6253204345703125e-05
          seconds_per_batch_mean: 1.087665557861328e-05
          seconds_per_batch_min: 5.245208740234375e-06
          seconds_per_batch_std: 7.393727251919244e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: "3.462 ms +/- 523.820 \xB5s [2.506 ms, 6.625 ms]"
          batches_per_second: 293.68 +/- 33.64 [150.93, 399.00]
        metrics:
          batches_per_second_max: 399.0015220700152
          batches_per_second_mean: 293.6769298164237
          batches_per_second_min: 150.9339666774623
          batches_per_second_std: 33.63668506873702
          seconds_per_batch_max: 0.00662541389465332
          seconds_per_batch_mean: 0.003461930751800537
          seconds_per_batch_min: 0.002506256103515625
          seconds_per_batch_std: 0.0005238199469653666
      on_device_inference:
        human_readable:
          batch_latency: 834.839 ms +/- 43.843 ms [804.304 ms, 1.008 s]
          batches_per_second: 1.20 +/- 0.06 [0.99, 1.24]
        metrics:
          batches_per_second_max: 1.2433104283181518
          batches_per_second_mean: 1.200823105076038
          batches_per_second_min: 0.9917593467252914
          batches_per_second_std: 0.05695118367021662
          seconds_per_batch_max: 1.0083091259002686
          seconds_per_batch_mean: 0.8348386216163636
          seconds_per_batch_min: 0.8043043613433838
          seconds_per_batch_std: 0.04384346709208074
      total:
        human_readable:
          batch_latency: 838.311 ms +/- 43.998 ms [808.089 ms, 1.012 s]
          batches_per_second: 1.20 +/- 0.06 [0.99, 1.24]
        metrics:
          batches_per_second_max: 1.2374874276082495
          batches_per_second_mean: 1.1958447900646185
          batches_per_second_min: 0.9885646864280425
          batches_per_second_std: 0.05667884086750684
          seconds_per_batch_max: 1.0115675926208496
          seconds_per_batch_mean: 0.8383114290237427
          seconds_per_batch_min: 0.808089017868042
          seconds_per_batch_std: 0.043998039274449945

INFO:benchmark:== Benchmarking model directly ==
Timing results (batch_size=32):
  cpu_to_gpu:
    human_readable:
      batch_latency: "10.877 \xB5s +/- 7.394 \xB5s [5.245 \xB5s, 46.253 \xB5s]"
      batches_per_second: 116.88 K +/- 45.15 K [21.62 K, 190.65 K]
    metrics:
      batches_per_second_max: 190650.18181818182
      batches_per_second_mean: 116883.30904799167
      batches_per_second_min: 21620.123711340206
      batches_per_second_std: 45152.36555495278
      seconds_per_batch_max: 4.6253204345703125e-05
      seconds_per_batch_mean: 1.087665557861328e-05
      seconds_per_batch_min: 5.245208740234375e-06
      seconds_per_batch_std: 7.393727251919244e-06
  gpu_to_cpu:
    human_readable:
      batch_latency: "3.462 ms +/- 523.820 \xB5s [2.506 ms, 6.625 ms]"
      batches_per_second: 293.68 +/- 33.64 [150.93, 399.00]
    metrics:
      batches_per_second_max: 399.0015220700152
      batches_per_second_mean: 293.6769298164237
      batches_per_second_min: 150.9339666774623
      batches_per_second_std: 33.63668506873702
      seconds_per_batch_max: 0.00662541389465332
      seconds_per_batch_mean: 0.003461930751800537
      seconds_per_batch_min: 0.002506256103515625
      seconds_per_batch_std: 0.0005238199469653666
  on_device_inference:
    human_readable:
      batch_latency: 834.839 ms +/- 43.843 ms [804.304 ms, 1.008 s]
      batches_per_second: 1.20 +/- 0.06 [0.99, 1.24]
    metrics:
      batches_per_second_max: 1.2433104283181518
      batches_per_second_mean: 1.200823105076038
      batches_per_second_min: 0.9917593467252914
      batches_per_second_std: 0.05695118367021662
      seconds_per_batch_max: 1.0083091259002686
      seconds_per_batch_mean: 0.8348386216163636
      seconds_per_batch_min: 0.8043043613433838
      seconds_per_batch_std: 0.04384346709208074
  total:
    human_readable:
      batch_latency: 838.311 ms +/- 43.998 ms [808.089 ms, 1.012 s]
      batches_per_second: 1.20 +/- 0.06 [0.99, 1.24]
    metrics:
      batches_per_second_max: 1.2374874276082495
      batches_per_second_mean: 1.1958447900646185
      batches_per_second_min: 0.9885646864280425
      batches_per_second_std: 0.05667884086750684
      seconds_per_batch_max: 1.0115675926208496
      seconds_per_batch_mean: 0.8383114290237427
      seconds_per_batch_min: 0.808089017868042
      seconds_per_batch_std: 0.043998039274449945

Inference energy: 19.65728767654101 J (5.460357687928058e-06 kWh)
Energy (batch_size=32):
  joules: 19.65728767654101
  kWh: 5.460357687928058e-06

Machine info:
  cpu:
    architecture: aarch64
    cores:
      physical: 8
      total: 8
    frequency: 2.27 GHz
    model: ARMv8 Processor rev 0 (v8l)
  gpus: null
  memory:
    available: 20.27 GB
    total: 31.17 GB
    used: 12.78 GB
  system:
    node: xavier
    release: 4.9.140-tegra
    system: Linux

Model device: cuda:0
Model parameters: 3794322 (3.79 M)
Model FLOPs: 635560544 (635.56 M)
Allocated GPU memory prior to inference: 15445504 (14.73 MB)
Allocated GPU memory after to inference: 15445504 (14.73 MB)
Max allocated GPU memory during inference: 3901647360 (3.63 GB)
Warming up with batch_size=32:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=32:  20%|██        | 2/10 [00:00<00:00, 13.82it/s]Warming up with batch_size=32:  40%|████      | 4/10 [00:00<00:00, 13.66it/s]Warming up with batch_size=32:  60%|██████    | 6/10 [00:00<00:00, 13.71it/s]Warming up with batch_size=32:  80%|████████  | 8/10 [00:00<00:00, 13.70it/s]Warming up with batch_size=32: 100%|██████████| 10/10 [00:00<00:00, 13.92it/s]Warming up with batch_size=32: 100%|██████████| 10/10 [00:00<00:00, 13.80it/s]
Measuring inference with batch_size=32:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=32:   2%|▏         | 2/100 [00:00<00:07, 13.52it/s]Measuring inference with batch_size=32:   4%|▍         | 4/100 [00:00<00:06, 13.82it/s]Measuring inference with batch_size=32:   6%|▌         | 6/100 [00:00<00:06, 14.03it/s]Measuring inference with batch_size=32:   8%|▊         | 8/100 [00:00<00:06, 13.52it/s]Measuring inference with batch_size=32:  10%|█         | 10/100 [00:00<00:06, 14.01it/s]Measuring inference with batch_size=32:  12%|█▏        | 12/100 [00:00<00:06, 13.53it/s]Measuring inference with batch_size=32:  14%|█▍        | 14/100 [00:01<00:06, 13.92it/s]Measuring inference with batch_size=32:  16%|█▌        | 16/100 [00:01<00:06, 13.86it/s]Measuring inference with batch_size=32:  18%|█▊        | 18/100 [00:01<00:05, 14.17it/s]Measuring inference with batch_size=32:  20%|██        | 20/100 [00:01<00:05, 14.06it/s]Measuring inference with batch_size=32:  22%|██▏       | 22/100 [00:01<00:05, 14.01it/s]Measuring inference with batch_size=32:  24%|██▍       | 24/100 [00:01<00:05, 14.18it/s]Measuring inference with batch_size=32:  26%|██▌       | 26/100 [00:01<00:05, 14.35it/s]Measuring inference with batch_size=32:  28%|██▊       | 28/100 [00:01<00:04, 14.45it/s]Measuring inference with batch_size=32:  30%|███       | 30/100 [00:02<00:04, 14.63it/s]Measuring inference with batch_size=32:  32%|███▏      | 32/100 [00:02<00:04, 14.74it/s]Measuring inference with batch_size=32:  34%|███▍      | 34/100 [00:02<00:04, 14.84it/s]Measuring inference with batch_size=32:  36%|███▌      | 36/100 [00:02<00:04, 14.82it/s]Measuring inference with batch_size=32:  38%|███▊      | 38/100 [00:02<00:04, 14.90it/s]Measuring inference with batch_size=32:  40%|████      | 40/100 [00:02<00:04, 14.98it/s]Measuring inference with batch_size=32:  42%|████▏     | 42/100 [00:02<00:03, 14.95it/s]Measuring inference with batch_size=32:  44%|████▍     | 44/100 [00:03<00:03, 14.93it/s]Measuring inference with batch_size=32:  46%|████▌     | 46/100 [00:03<00:03, 14.98it/s]Measuring inference with batch_size=32:  48%|████▊     | 48/100 [00:03<00:03, 15.02it/s]Measuring inference with batch_size=32:  50%|█████     | 50/100 [00:03<00:03, 15.04it/s]Measuring inference with batch_size=32:  52%|█████▏    | 52/100 [00:03<00:03, 14.89it/s]Measuring inference with batch_size=32:  54%|█████▍    | 54/100 [00:03<00:03, 14.78it/s]Measuring inference with batch_size=32:  56%|█████▌    | 56/100 [00:03<00:02, 14.82it/s]Measuring inference with batch_size=32:  58%|█████▊    | 58/100 [00:03<00:02, 14.84it/s]Measuring inference with batch_size=32:  60%|██████    | 60/100 [00:04<00:02, 14.90it/s]Measuring inference with batch_size=32:  62%|██████▏   | 62/100 [00:04<00:02, 14.97it/s]Measuring inference with batch_size=32:  64%|██████▍   | 64/100 [00:04<00:02, 15.03it/s]Measuring inference with batch_size=32:  66%|██████▌   | 66/100 [00:04<00:02, 14.99it/s]Measuring inference with batch_size=32:  68%|██████▊   | 68/100 [00:04<00:02, 15.04it/s]Measuring inference with batch_size=32:  70%|███████   | 70/100 [00:04<00:01, 15.02it/s]Measuring inference with batch_size=32:  72%|███████▏  | 72/100 [00:04<00:01, 15.07it/s]Measuring inference with batch_size=32:  74%|███████▍  | 74/100 [00:05<00:01, 15.06it/s]Measuring inference with batch_size=32:  76%|███████▌  | 76/100 [00:05<00:01, 15.08it/s]Measuring inference with batch_size=32:  78%|███████▊  | 78/100 [00:05<00:01, 15.10it/s]Measuring inference with batch_size=32:  80%|████████  | 80/100 [00:05<00:01, 15.06it/s]Measuring inference with batch_size=32:  82%|████████▏ | 82/100 [00:05<00:01, 15.01it/s]Measuring inference with batch_size=32:  84%|████████▍ | 84/100 [00:05<00:01, 15.05it/s]Measuring inference with batch_size=32:  86%|████████▌ | 86/100 [00:05<00:00, 15.11it/s]Measuring inference with batch_size=32:  88%|████████▊ | 88/100 [00:05<00:00, 15.07it/s]Measuring inference with batch_size=32:  90%|█████████ | 90/100 [00:06<00:00, 15.11it/s]Measuring inference with batch_size=32:  92%|█████████▏| 92/100 [00:06<00:00, 15.13it/s]Measuring inference with batch_size=32:  94%|█████████▍| 94/100 [00:06<00:00, 15.18it/s]Measuring inference with batch_size=32:  96%|█████████▌| 96/100 [00:06<00:00, 15.08it/s]Measuring inference with batch_size=32:  98%|█████████▊| 98/100 [00:06<00:00, 15.07it/s]Measuring inference with batch_size=32: 100%|██████████| 100/100 [00:06<00:00, 15.11it/s]Measuring inference with batch_size=32: 100%|██████████| 100/100 [00:06<00:00, 14.74it/s]
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "308.974 \xB5s +/- 55.106 \xB5s [250.340 \xB5s, 579.596 \xB5\
        s]"
      batches_per_second: 3.32 K +/- 470.30 [1.73 K, 3.99 K]
    metrics:
      batches_per_second_max: 3994.575238095238
      batches_per_second_mean: 3317.932391278709
      batches_per_second_min: 1725.3410119292473
      batches_per_second_std: 470.3012271094976
      seconds_per_batch_max: 0.0005795955657958984
      seconds_per_batch_mean: 0.0003089737892150879
      seconds_per_batch_min: 0.0002503395080566406
      seconds_per_batch_std: 5.5105649155641576e-05
  gpu_to_cpu:
    human_readable:
      batch_latency: "182.137 \xB5s +/- 31.532 \xB5s [130.177 \xB5s, 391.006 \xB5\
        s]"
      batches_per_second: 5.62 K +/- 782.18 [2.56 K, 7.68 K]
    metrics:
      batches_per_second_max: 7681.875457875458
      batches_per_second_mean: 5617.162223438067
      batches_per_second_min: 2557.5024390243902
      batches_per_second_std: 782.1822078844726
      seconds_per_batch_max: 0.0003910064697265625
      seconds_per_batch_mean: 0.00018213748931884767
      seconds_per_batch_min: 0.00013017654418945312
      seconds_per_batch_std: 3.1531933241509136e-05
  on_device_inference:
    human_readable:
      batch_latency: 66.883 ms +/- 3.275 ms [64.252 ms, 83.596 ms]
      batches_per_second: 14.98 +/- 0.65 [11.96, 15.56]
    metrics:
      batches_per_second_max: 15.563684399965862
      batches_per_second_mean: 14.983097028454624
      batches_per_second_min: 11.962364349376115
      batches_per_second_std: 0.6457060034677086
      seconds_per_batch_max: 0.08359551429748535
      seconds_per_batch_mean: 0.06688252449035645
      seconds_per_batch_min: 0.06425213813781738
      seconds_per_batch_std: 0.0032751897786892957
  total:
    human_readable:
      batch_latency: 67.374 ms +/- 3.305 ms [64.680 ms, 84.231 ms]
      batches_per_second: 14.87 +/- 0.64 [11.87, 15.46]
    metrics:
      batches_per_second_max: 15.46064897581546
      batches_per_second_mean: 14.873998132422585
      batches_per_second_min: 11.872060505191174
      batches_per_second_std: 0.6423193085913249
      seconds_per_batch_max: 0.08423137664794922
      seconds_per_batch_mean: 0.06737363576889038
      seconds_per_batch_min: 0.06468033790588379
      seconds_per_batch_std: 0.0033049328649365664

Inference energy: 16.967809404643376 J (4.713280390178715e-06 kWh)
Energy (batch_size=1):
  joules: 16.967809404643376
  kWh: 4.713280390178715e-06

Warming up with batch_size=32:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=32:  10%|█         | 1/10 [00:00<00:07,  1.28it/s]Warming up with batch_size=32:  20%|██        | 2/10 [00:01<00:06,  1.28it/s]Warming up with batch_size=32:  30%|███       | 3/10 [00:02<00:05,  1.28it/s]Warming up with batch_size=32:  40%|████      | 4/10 [00:03<00:04,  1.28it/s]Warming up with batch_size=32:  50%|█████     | 5/10 [00:03<00:03,  1.28it/s]Warming up with batch_size=32:  60%|██████    | 6/10 [00:04<00:03,  1.28it/s]Warming up with batch_size=32:  70%|███████   | 7/10 [00:05<00:02,  1.28it/s]Warming up with batch_size=32:  80%|████████  | 8/10 [00:06<00:01,  1.27it/s]Warming up with batch_size=32:  90%|█████████ | 9/10 [00:07<00:00,  1.28it/s]Warming up with batch_size=32: 100%|██████████| 10/10 [00:07<00:00,  1.28it/s]Warming up with batch_size=32: 100%|██████████| 10/10 [00:07<00:00,  1.28it/s]
Measuring inference with batch_size=32:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=32:   1%|          | 1/100 [00:00<01:17,  1.28it/s]Measuring inference with batch_size=32:   2%|▏         | 2/100 [00:01<01:16,  1.28it/s]Measuring inference with batch_size=32:   3%|▎         | 3/100 [00:02<01:15,  1.28it/s]Measuring inference with batch_size=32:   4%|▍         | 4/100 [00:03<01:15,  1.27it/s]Measuring inference with batch_size=32:   5%|▌         | 5/100 [00:03<01:14,  1.27it/s]Measuring inference with batch_size=32:   6%|▌         | 6/100 [00:04<01:13,  1.28it/s]Measuring inference with batch_size=32:   7%|▋         | 7/100 [00:05<01:12,  1.28it/s]Measuring inference with batch_size=32:   8%|▊         | 8/100 [00:06<01:12,  1.28it/s]Measuring inference with batch_size=32:   9%|▉         | 9/100 [00:07<01:11,  1.28it/s]Measuring inference with batch_size=32:  10%|█         | 10/100 [00:07<01:10,  1.28it/s]Measuring inference with batch_size=32:  11%|█         | 11/100 [00:08<01:09,  1.28it/s]Measuring inference with batch_size=32:  12%|█▏        | 12/100 [00:09<01:09,  1.28it/s]Measuring inference with batch_size=32:  13%|█▎        | 13/100 [00:10<01:08,  1.28it/s]Measuring inference with batch_size=32:  14%|█▍        | 14/100 [00:10<01:07,  1.28it/s]Measuring inference with batch_size=32:  15%|█▌        | 15/100 [00:11<01:06,  1.28it/s]Measuring inference with batch_size=32:  16%|█▌        | 16/100 [00:12<01:05,  1.28it/s]Measuring inference with batch_size=32:  17%|█▋        | 17/100 [00:13<01:04,  1.28it/s]Measuring inference with batch_size=32:  18%|█▊        | 18/100 [00:14<01:04,  1.28it/s]Measuring inference with batch_size=32:  19%|█▉        | 19/100 [00:14<01:03,  1.28it/s]Measuring inference with batch_size=32:  20%|██        | 20/100 [00:15<01:02,  1.28it/s]Measuring inference with batch_size=32:  21%|██        | 21/100 [00:16<01:01,  1.28it/s]Measuring inference with batch_size=32:  22%|██▏       | 22/100 [00:17<01:01,  1.28it/s]Measuring inference with batch_size=32:  23%|██▎       | 23/100 [00:18<01:00,  1.28it/s]Measuring inference with batch_size=32:  24%|██▍       | 24/100 [00:18<00:59,  1.28it/s]Measuring inference with batch_size=32:  25%|██▌       | 25/100 [00:19<00:58,  1.28it/s]Measuring inference with batch_size=32:  26%|██▌       | 26/100 [00:20<00:57,  1.28it/s]Measuring inference with batch_size=32:  27%|██▋       | 27/100 [00:21<00:57,  1.28it/s]Measuring inference with batch_size=32:  28%|██▊       | 28/100 [00:21<00:56,  1.28it/s]Measuring inference with batch_size=32:  29%|██▉       | 29/100 [00:22<00:55,  1.28it/s]Measuring inference with batch_size=32:  30%|███       | 30/100 [00:23<00:54,  1.28it/s]Measuring inference with batch_size=32:  31%|███       | 31/100 [00:24<00:54,  1.28it/s]Measuring inference with batch_size=32:  32%|███▏      | 32/100 [00:25<00:53,  1.28it/s]Measuring inference with batch_size=32:  33%|███▎      | 33/100 [00:25<00:52,  1.28it/s]Measuring inference with batch_size=32:  34%|███▍      | 34/100 [00:26<00:51,  1.28it/s]Measuring inference with batch_size=32:  35%|███▌      | 35/100 [00:27<00:50,  1.27it/s]Measuring inference with batch_size=32:  36%|███▌      | 36/100 [00:28<00:50,  1.28it/s]Measuring inference with batch_size=32:  37%|███▋      | 37/100 [00:28<00:49,  1.28it/s]Measuring inference with batch_size=32:  38%|███▊      | 38/100 [00:29<00:48,  1.28it/s]Measuring inference with batch_size=32:  39%|███▉      | 39/100 [00:30<00:47,  1.28it/s]Measuring inference with batch_size=32:  40%|████      | 40/100 [00:31<00:46,  1.28it/s]Measuring inference with batch_size=32:  41%|████      | 41/100 [00:32<00:46,  1.28it/s]Measuring inference with batch_size=32:  42%|████▏     | 42/100 [00:32<00:45,  1.28it/s]Measuring inference with batch_size=32:  43%|████▎     | 43/100 [00:33<00:44,  1.28it/s]Measuring inference with batch_size=32:  44%|████▍     | 44/100 [00:34<00:43,  1.27it/s]Measuring inference with batch_size=32:  45%|████▌     | 45/100 [00:35<00:43,  1.27it/s]Measuring inference with batch_size=32:  46%|████▌     | 46/100 [00:36<00:42,  1.27it/s]Measuring inference with batch_size=32:  47%|████▋     | 47/100 [00:36<00:41,  1.28it/s]Measuring inference with batch_size=32:  48%|████▊     | 48/100 [00:37<00:40,  1.28it/s]Measuring inference with batch_size=32:  49%|████▉     | 49/100 [00:38<00:39,  1.28it/s]Measuring inference with batch_size=32:  50%|█████     | 50/100 [00:39<00:39,  1.28it/s]Measuring inference with batch_size=32:  51%|█████     | 51/100 [00:39<00:38,  1.28it/s]Measuring inference with batch_size=32:  52%|█████▏    | 52/100 [00:40<00:37,  1.28it/s]Measuring inference with batch_size=32:  53%|█████▎    | 53/100 [00:41<00:36,  1.28it/s]Measuring inference with batch_size=32:  54%|█████▍    | 54/100 [00:42<00:36,  1.28it/s]Measuring inference with batch_size=32:  55%|█████▌    | 55/100 [00:43<00:35,  1.28it/s]Measuring inference with batch_size=32:  56%|█████▌    | 56/100 [00:43<00:34,  1.28it/s]Measuring inference with batch_size=32:  57%|█████▋    | 57/100 [00:44<00:33,  1.28it/s]Measuring inference with batch_size=32:  58%|█████▊    | 58/100 [00:45<00:32,  1.28it/s]Measuring inference with batch_size=32:  59%|█████▉    | 59/100 [00:46<00:32,  1.28it/s]Measuring inference with batch_size=32:  60%|██████    | 60/100 [00:47<00:31,  1.28it/s]Measuring inference with batch_size=32:  61%|██████    | 61/100 [00:47<00:30,  1.28it/s]Measuring inference with batch_size=32:  62%|██████▏   | 62/100 [00:48<00:29,  1.28it/s]Measuring inference with batch_size=32:  63%|██████▎   | 63/100 [00:49<00:28,  1.28it/s]Measuring inference with batch_size=32:  64%|██████▍   | 64/100 [00:50<00:28,  1.28it/s]Measuring inference with batch_size=32:  65%|██████▌   | 65/100 [00:50<00:27,  1.27it/s]Measuring inference with batch_size=32:  66%|██████▌   | 66/100 [00:51<00:26,  1.28it/s]Measuring inference with batch_size=32:  67%|██████▋   | 67/100 [00:52<00:25,  1.28it/s]Measuring inference with batch_size=32:  68%|██████▊   | 68/100 [00:53<00:25,  1.28it/s]Measuring inference with batch_size=32:  69%|██████▉   | 69/100 [00:54<00:24,  1.28it/s]Measuring inference with batch_size=32:  70%|███████   | 70/100 [00:54<00:23,  1.28it/s]Measuring inference with batch_size=32:  71%|███████   | 71/100 [00:55<00:22,  1.28it/s]Measuring inference with batch_size=32:  72%|███████▏  | 72/100 [00:56<00:21,  1.28it/s]Measuring inference with batch_size=32:  73%|███████▎  | 73/100 [00:57<00:21,  1.28it/s]Measuring inference with batch_size=32:  74%|███████▍  | 74/100 [00:57<00:20,  1.28it/s]Measuring inference with batch_size=32:  75%|███████▌  | 75/100 [00:58<00:19,  1.28it/s]Measuring inference with batch_size=32:  76%|███████▌  | 76/100 [00:59<00:18,  1.28it/s]Measuring inference with batch_size=32:  77%|███████▋  | 77/100 [01:00<00:18,  1.28it/s]Measuring inference with batch_size=32:  78%|███████▊  | 78/100 [01:01<00:17,  1.28it/s]Measuring inference with batch_size=32:  79%|███████▉  | 79/100 [01:01<00:16,  1.27it/s]Measuring inference with batch_size=32:  80%|████████  | 80/100 [01:02<00:15,  1.28it/s]Measuring inference with batch_size=32:  81%|████████  | 81/100 [01:03<00:14,  1.27it/s]Measuring inference with batch_size=32:  82%|████████▏ | 82/100 [01:04<00:14,  1.28it/s]Measuring inference with batch_size=32:  83%|████████▎ | 83/100 [01:05<00:13,  1.28it/s]Measuring inference with batch_size=32:  84%|████████▍ | 84/100 [01:05<00:12,  1.28it/s]Measuring inference with batch_size=32:  85%|████████▌ | 85/100 [01:06<00:11,  1.28it/s]Measuring inference with batch_size=32:  86%|████████▌ | 86/100 [01:07<00:10,  1.28it/s]Measuring inference with batch_size=32:  87%|████████▋ | 87/100 [01:08<00:10,  1.28it/s]Measuring inference with batch_size=32:  88%|████████▊ | 88/100 [01:08<00:09,  1.28it/s]Measuring inference with batch_size=32:  89%|████████▉ | 89/100 [01:09<00:08,  1.28it/s]Measuring inference with batch_size=32:  90%|█████████ | 90/100 [01:10<00:07,  1.28it/s]Measuring inference with batch_size=32:  91%|█████████ | 91/100 [01:11<00:07,  1.28it/s]Measuring inference with batch_size=32:  92%|█████████▏| 92/100 [01:12<00:06,  1.28it/s]Measuring inference with batch_size=32:  93%|█████████▎| 93/100 [01:12<00:05,  1.28it/s]Measuring inference with batch_size=32:  94%|█████████▍| 94/100 [01:13<00:04,  1.28it/s]Measuring inference with batch_size=32:  95%|█████████▌| 95/100 [01:14<00:03,  1.28it/s]Measuring inference with batch_size=32:  96%|█████████▌| 96/100 [01:15<00:03,  1.28it/s]Measuring inference with batch_size=32:  97%|█████████▋| 97/100 [01:16<00:02,  1.27it/s]Measuring inference with batch_size=32:  98%|█████████▊| 98/100 [01:16<00:01,  1.28it/s]Measuring inference with batch_size=32:  99%|█████████▉| 99/100 [01:17<00:00,  1.27it/s]Measuring inference with batch_size=32: 100%|██████████| 100/100 [01:18<00:00,  1.28it/s]Measuring inference with batch_size=32: 100%|██████████| 100/100 [01:18<00:00,  1.28it/s]
INFO:benchmark:learner.model.forward:
  device: cuda
  energy:
    batch_size_1:
      joules: 16.967809404643376
      kWh: 4.713280390178715e-06
    batch_size_32:
      joules: 17.30524600725174
      kWh: 4.80701277979215e-06
  flops: 635560544
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 20.27 GB
      total: 31.17 GB
      used: 12.78 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  max_inference_memory: 3901647360
  params: 3794322
  post_inference_memory: 15445504
  pre_inference_memory: 15445504
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "308.974 \xB5s +/- 55.106 \xB5s [250.340 \xB5s, 579.596 \xB5\
            s]"
          batches_per_second: 3.32 K +/- 470.30 [1.73 K, 3.99 K]
        metrics:
          batches_per_second_max: 3994.575238095238
          batches_per_second_mean: 3317.932391278709
          batches_per_second_min: 1725.3410119292473
          batches_per_second_std: 470.3012271094976
          seconds_per_batch_max: 0.0005795955657958984
          seconds_per_batch_mean: 0.0003089737892150879
          seconds_per_batch_min: 0.0002503395080566406
          seconds_per_batch_std: 5.5105649155641576e-05
      gpu_to_cpu:
        human_readable:
          batch_latency: "182.137 \xB5s +/- 31.532 \xB5s [130.177 \xB5s, 391.006 \xB5\
            s]"
          batches_per_second: 5.62 K +/- 782.18 [2.56 K, 7.68 K]
        metrics:
          batches_per_second_max: 7681.875457875458
          batches_per_second_mean: 5617.162223438067
          batches_per_second_min: 2557.5024390243902
          batches_per_second_std: 782.1822078844726
          seconds_per_batch_max: 0.0003910064697265625
          seconds_per_batch_mean: 0.00018213748931884767
          seconds_per_batch_min: 0.00013017654418945312
          seconds_per_batch_std: 3.1531933241509136e-05
      on_device_inference:
        human_readable:
          batch_latency: 66.883 ms +/- 3.275 ms [64.252 ms, 83.596 ms]
          batches_per_second: 14.98 +/- 0.65 [11.96, 15.56]
        metrics:
          batches_per_second_max: 15.563684399965862
          batches_per_second_mean: 14.983097028454624
          batches_per_second_min: 11.962364349376115
          batches_per_second_std: 0.6457060034677086
          seconds_per_batch_max: 0.08359551429748535
          seconds_per_batch_mean: 0.06688252449035645
          seconds_per_batch_min: 0.06425213813781738
          seconds_per_batch_std: 0.0032751897786892957
      total:
        human_readable:
          batch_latency: 67.374 ms +/- 3.305 ms [64.680 ms, 84.231 ms]
          batches_per_second: 14.87 +/- 0.64 [11.87, 15.46]
        metrics:
          batches_per_second_max: 15.46064897581546
          batches_per_second_mean: 14.873998132422585
          batches_per_second_min: 11.872060505191174
          batches_per_second_std: 0.6423193085913249
          seconds_per_batch_max: 0.08423137664794922
          seconds_per_batch_mean: 0.06737363576889038
          seconds_per_batch_min: 0.06468033790588379
          seconds_per_batch_std: 0.0033049328649365664
    batch_size_32:
      cpu_to_gpu:
        human_readable:
          batch_latency: "7.496 ms +/- 346.256 \xB5s [5.843 ms, 8.190 ms]"
          batches_per_second: 133.72 +/- 6.93 [122.10, 171.14]
        metrics:
          batches_per_second_max: 171.14019911865515
          batches_per_second_mean: 133.72399510754224
          batches_per_second_min: 122.10491994177583
          batches_per_second_std: 6.926799595236434
          seconds_per_batch_max: 0.008189678192138672
          seconds_per_batch_mean: 0.007495908737182617
          seconds_per_batch_min: 0.005843162536621094
          seconds_per_batch_std: 0.00034625597186550166
      gpu_to_cpu:
        human_readable:
          batch_latency: 77.217 ms +/- 1.359 ms [75.861 ms, 81.959 ms]
          batches_per_second: 12.95 +/- 0.22 [12.20, 13.18]
        metrics:
          batches_per_second_max: 13.181925037556649
          batches_per_second_mean: 12.954440725554328
          batches_per_second_min: 12.201221197285323
          batches_per_second_std: 0.22128341536673046
          seconds_per_batch_max: 0.08195900917053223
          seconds_per_batch_mean: 0.07721682071685791
          seconds_per_batch_min: 0.07586145401000977
          seconds_per_batch_std: 0.001359149835294149
      on_device_inference:
        human_readable:
          batch_latency: 697.646 ms +/- 2.288 ms [690.757 ms, 703.397 ms]
          batches_per_second: 1.43 +/- 0.00 [1.42, 1.45]
        metrics:
          batches_per_second_max: 1.4476880261683263
          batches_per_second_mean: 1.433406379108901
          batches_per_second_min: 1.4216717023776193
          batches_per_second_std: 0.004703847406319835
          seconds_per_batch_max: 0.703397274017334
          seconds_per_batch_mean: 0.6976463747024536
          seconds_per_batch_min: 0.6907565593719482
          seconds_per_batch_std: 0.0022883341194992134
      total:
        human_readable:
          batch_latency: 782.359 ms +/- 2.163 ms [778.248 ms, 787.554 ms]
          batches_per_second: 1.28 +/- 0.00 [1.27, 1.28]
        metrics:
          batches_per_second_max: 1.2849369372681292
          batches_per_second_mean: 1.2781951860160934
          batches_per_second_min: 1.2697549284202565
          batches_per_second_std: 0.0035325322676253075
          seconds_per_batch_max: 0.7875535488128662
          seconds_per_batch_mean: 0.7823591041564941
          seconds_per_batch_min: 0.7782483100891113
          seconds_per_batch_std: 0.0021629521059849466

INFO:benchmark:==== Benchmarking X3DLearner (s) ====
INFO:benchmark:== Benchmarking learner.infer ==
ERROR:pytorch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:pytorch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
Timing results (batch_size=32):
  cpu_to_gpu:
    human_readable:
      batch_latency: "7.496 ms +/- 346.256 \xB5s [5.843 ms, 8.190 ms]"
      batches_per_second: 133.72 +/- 6.93 [122.10, 171.14]
    metrics:
      batches_per_second_max: 171.14019911865515
      batches_per_second_mean: 133.72399510754224
      batches_per_second_min: 122.10491994177583
      batches_per_second_std: 6.926799595236434
      seconds_per_batch_max: 0.008189678192138672
      seconds_per_batch_mean: 0.007495908737182617
      seconds_per_batch_min: 0.005843162536621094
      seconds_per_batch_std: 0.00034625597186550166
  gpu_to_cpu:
    human_readable:
      batch_latency: 77.217 ms +/- 1.359 ms [75.861 ms, 81.959 ms]
      batches_per_second: 12.95 +/- 0.22 [12.20, 13.18]
    metrics:
      batches_per_second_max: 13.181925037556649
      batches_per_second_mean: 12.954440725554328
      batches_per_second_min: 12.201221197285323
      batches_per_second_std: 0.22128341536673046
      seconds_per_batch_max: 0.08195900917053223
      seconds_per_batch_mean: 0.07721682071685791
      seconds_per_batch_min: 0.07586145401000977
      seconds_per_batch_std: 0.001359149835294149
  on_device_inference:
    human_readable:
      batch_latency: 697.646 ms +/- 2.288 ms [690.757 ms, 703.397 ms]
      batches_per_second: 1.43 +/- 0.00 [1.42, 1.45]
    metrics:
      batches_per_second_max: 1.4476880261683263
      batches_per_second_mean: 1.433406379108901
      batches_per_second_min: 1.4216717023776193
      batches_per_second_std: 0.004703847406319835
      seconds_per_batch_max: 0.703397274017334
      seconds_per_batch_mean: 0.6976463747024536
      seconds_per_batch_min: 0.6907565593719482
      seconds_per_batch_std: 0.0022883341194992134
  total:
    human_readable:
      batch_latency: 782.359 ms +/- 2.163 ms [778.248 ms, 787.554 ms]
      batches_per_second: 1.28 +/- 0.00 [1.27, 1.28]
    metrics:
      batches_per_second_max: 1.2849369372681292
      batches_per_second_mean: 1.2781951860160934
      batches_per_second_min: 1.2697549284202565
      batches_per_second_std: 0.0035325322676253075
      seconds_per_batch_max: 0.7875535488128662
      seconds_per_batch_mean: 0.7823591041564941
      seconds_per_batch_min: 0.7782483100891113
      seconds_per_batch_std: 0.0021629521059849466

Inference energy: 17.30524600725174 J (4.80701277979215e-06 kWh)
Energy (batch_size=32):
  joules: 17.30524600725174
  kWh: 4.80701277979215e-06

Machine info:
  cpu:
    architecture: aarch64
    cores:
      physical: 8
      total: 8
    frequency: 2.27 GHz
    model: ARMv8 Processor rev 0 (v8l)
  gpus: null
  memory:
    available: 20.24 GB
    total: 31.17 GB
    used: 12.81 GB
  system:
    node: xavier
    release: 4.9.140-tegra
    system: Linux

Model device: cuda:0
Allocated GPU memory prior to inference: 30891008 (29.46 MB)
Allocated GPU memory after to inference: 30891008 (29.46 MB)
Max allocated GPU memory during inference: 9586036736 (8.93 GB)
Warming up with batch_size=16:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=16:  10%|█         | 1/10 [00:00<00:00,  9.50it/s]Warming up with batch_size=16:  20%|██        | 2/10 [00:00<00:00,  9.55it/s]Warming up with batch_size=16:  30%|███       | 3/10 [00:00<00:00,  9.47it/s]Warming up with batch_size=16:  40%|████      | 4/10 [00:00<00:00,  9.43it/s]Warming up with batch_size=16:  50%|█████     | 5/10 [00:00<00:00,  9.32it/s]Warming up with batch_size=16:  60%|██████    | 6/10 [00:00<00:00,  9.31it/s]Warming up with batch_size=16:  70%|███████   | 7/10 [00:00<00:00,  9.38it/s]Warming up with batch_size=16:  80%|████████  | 8/10 [00:00<00:00,  9.43it/s]Warming up with batch_size=16:  90%|█████████ | 9/10 [00:00<00:00,  9.44it/s]Warming up with batch_size=16: 100%|██████████| 10/10 [00:01<00:00,  9.33it/s]Warming up with batch_size=16: 100%|██████████| 10/10 [00:01<00:00,  9.37it/s]
Measuring inference with batch_size=16:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=16:   1%|          | 1/100 [00:00<00:10,  9.35it/s]Measuring inference with batch_size=16:   2%|▏         | 2/100 [00:00<00:10,  9.50it/s]Measuring inference with batch_size=16:   3%|▎         | 3/100 [00:00<00:10,  9.55it/s]Measuring inference with batch_size=16:   4%|▍         | 4/100 [00:00<00:10,  9.57it/s]Measuring inference with batch_size=16:   5%|▌         | 5/100 [00:00<00:10,  9.48it/s]Measuring inference with batch_size=16:   6%|▌         | 6/100 [00:00<00:09,  9.53it/s]Measuring inference with batch_size=16:   7%|▋         | 7/100 [00:00<00:09,  9.57it/s]Measuring inference with batch_size=16:   8%|▊         | 8/100 [00:00<00:09,  9.55it/s]Measuring inference with batch_size=16:   9%|▉         | 9/100 [00:00<00:09,  9.47it/s]Measuring inference with batch_size=16:  10%|█         | 10/100 [00:01<00:09,  9.49it/s]Measuring inference with batch_size=16:  11%|█         | 11/100 [00:01<00:09,  9.52it/s]Measuring inference with batch_size=16:  12%|█▏        | 12/100 [00:01<00:09,  9.54it/s]Measuring inference with batch_size=16:  13%|█▎        | 13/100 [00:01<00:09,  9.45it/s]Measuring inference with batch_size=16:  14%|█▍        | 14/100 [00:01<00:09,  9.39it/s]Measuring inference with batch_size=16:  15%|█▌        | 15/100 [00:01<00:09,  9.44it/s]Measuring inference with batch_size=16:  16%|█▌        | 16/100 [00:01<00:08,  9.47it/s]Measuring inference with batch_size=16:  17%|█▋        | 17/100 [00:01<00:08,  9.49it/s]Measuring inference with batch_size=16:  18%|█▊        | 18/100 [00:01<00:08,  9.40it/s]Measuring inference with batch_size=16:  19%|█▉        | 19/100 [00:02<00:08,  9.35it/s]Measuring inference with batch_size=16:  20%|██        | 20/100 [00:02<00:08,  9.41it/s]Measuring inference with batch_size=16:  21%|██        | 21/100 [00:02<00:08,  9.47it/s]Measuring inference with batch_size=16:  22%|██▏       | 22/100 [00:02<00:08,  9.50it/s]Measuring inference with batch_size=16:  23%|██▎       | 23/100 [00:02<00:08,  9.49it/s]Measuring inference with batch_size=16:  24%|██▍       | 24/100 [00:02<00:08,  9.48it/s]Measuring inference with batch_size=16:  25%|██▌       | 25/100 [00:02<00:07,  9.49it/s]Measuring inference with batch_size=16:  26%|██▌       | 26/100 [00:02<00:07,  9.53it/s]Measuring inference with batch_size=16:  27%|██▋       | 27/100 [00:02<00:07,  9.54it/s]Measuring inference with batch_size=16:  28%|██▊       | 28/100 [00:02<00:07,  9.44it/s]Measuring inference with batch_size=16:  29%|██▉       | 29/100 [00:03<00:07,  9.47it/s]Measuring inference with batch_size=16:  30%|███       | 30/100 [00:03<00:07,  9.51it/s]Measuring inference with batch_size=16:  31%|███       | 31/100 [00:03<00:07,  9.53it/s]Measuring inference with batch_size=16:  32%|███▏      | 32/100 [00:03<00:07,  9.53it/s]Measuring inference with batch_size=16:  33%|███▎      | 33/100 [00:03<00:07,  9.45it/s]Measuring inference with batch_size=16:  34%|███▍      | 34/100 [00:03<00:06,  9.50it/s]Measuring inference with batch_size=16:  35%|███▌      | 35/100 [00:03<00:06,  9.53it/s]Measuring inference with batch_size=16:  36%|███▌      | 36/100 [00:03<00:06,  9.55it/s]Measuring inference with batch_size=16:  37%|███▋      | 37/100 [00:03<00:06,  9.56it/s]Measuring inference with batch_size=16:  38%|███▊      | 38/100 [00:04<00:06,  9.51it/s]Measuring inference with batch_size=16:  39%|███▉      | 39/100 [00:04<00:06,  9.52it/s]Measuring inference with batch_size=16:  40%|████      | 40/100 [00:04<00:06,  9.49it/s]Measuring inference with batch_size=16:  41%|████      | 41/100 [00:04<00:06,  9.53it/s]Measuring inference with batch_size=16:  42%|████▏     | 42/100 [00:04<00:06,  9.55it/s]Measuring inference with batch_size=16:  43%|████▎     | 43/100 [00:04<00:05,  9.54it/s]Measuring inference with batch_size=16:  44%|████▍     | 44/100 [00:04<00:05,  9.52it/s]Measuring inference with batch_size=16:  45%|████▌     | 45/100 [00:04<00:05,  9.54it/s]Measuring inference with batch_size=16:  46%|████▌     | 46/100 [00:04<00:05,  9.54it/s]Measuring inference with batch_size=16:  47%|████▋     | 47/100 [00:04<00:05,  9.38it/s]Measuring inference with batch_size=16:  48%|████▊     | 48/100 [00:05<00:05,  9.44it/s]Measuring inference with batch_size=16:  49%|████▉     | 49/100 [00:05<00:05,  9.47it/s]Measuring inference with batch_size=16:  50%|█████     | 50/100 [00:05<00:05,  9.41it/s]Measuring inference with batch_size=16:  51%|█████     | 51/100 [00:05<00:05,  9.47it/s]Measuring inference with batch_size=16:  52%|█████▏    | 52/100 [00:05<00:05,  9.36it/s]Measuring inference with batch_size=16:  53%|█████▎    | 53/100 [00:05<00:04,  9.44it/s]Measuring inference with batch_size=16:  54%|█████▍    | 54/100 [00:05<00:04,  9.50it/s]Measuring inference with batch_size=16:  55%|█████▌    | 55/100 [00:05<00:04,  9.54it/s]Measuring inference with batch_size=16:  56%|█████▌    | 56/100 [00:05<00:04,  9.55it/s]Measuring inference with batch_size=16:  57%|█████▋    | 57/100 [00:06<00:04,  9.47it/s]Measuring inference with batch_size=16:  58%|█████▊    | 58/100 [00:06<00:04,  9.43it/s]Measuring inference with batch_size=16:  59%|█████▉    | 59/100 [00:06<00:04,  9.37it/s]Measuring inference with batch_size=16:  60%|██████    | 60/100 [00:06<00:04,  9.32it/s]Measuring inference with batch_size=16:  61%|██████    | 61/100 [00:06<00:04,  9.32it/s]Measuring inference with batch_size=16:  62%|██████▏   | 62/100 [00:06<00:04,  9.37it/s]Measuring inference with batch_size=16:  63%|██████▎   | 63/100 [00:06<00:03,  9.43it/s]Measuring inference with batch_size=16:  64%|██████▍   | 64/100 [00:06<00:03,  9.37it/s]Measuring inference with batch_size=16:  65%|██████▌   | 65/100 [00:06<00:03,  9.42it/s]Measuring inference with batch_size=16:  66%|██████▌   | 66/100 [00:06<00:03,  9.37it/s]Measuring inference with batch_size=16:  67%|██████▋   | 67/100 [00:07<00:03,  9.41it/s]Measuring inference with batch_size=16:  68%|██████▊   | 68/100 [00:07<00:03,  9.42it/s]Measuring inference with batch_size=16:  69%|██████▉   | 69/100 [00:07<00:03,  9.45it/s]Measuring inference with batch_size=16:  70%|███████   | 70/100 [00:07<00:03,  9.46it/s]Measuring inference with batch_size=16:  71%|███████   | 71/100 [00:07<00:03,  9.41it/s]Measuring inference with batch_size=16:  72%|███████▏  | 72/100 [00:07<00:02,  9.48it/s]Measuring inference with batch_size=16:  73%|███████▎  | 73/100 [00:07<00:02,  9.49it/s]Measuring inference with batch_size=16:  74%|███████▍  | 74/100 [00:07<00:02,  9.50it/s]Measuring inference with batch_size=16:  75%|███████▌  | 75/100 [00:07<00:02,  9.51it/s]Measuring inference with batch_size=16:  76%|███████▌  | 76/100 [00:08<00:02,  9.44it/s]Measuring inference with batch_size=16:  77%|███████▋  | 77/100 [00:08<00:02,  9.39it/s]Measuring inference with batch_size=16:  78%|███████▊  | 78/100 [00:08<00:02,  9.43it/s]Measuring inference with batch_size=16:  79%|███████▉  | 79/100 [00:08<00:02,  9.37it/s]Measuring inference with batch_size=16:  80%|████████  | 80/100 [00:08<00:02,  9.33it/s]Measuring inference with batch_size=16:  81%|████████  | 81/100 [00:08<00:02,  9.31it/s]Measuring inference with batch_size=16:  82%|████████▏ | 82/100 [00:08<00:01,  9.33it/s]Measuring inference with batch_size=16:  83%|████████▎ | 83/100 [00:08<00:01,  9.39it/s]Measuring inference with batch_size=16:  84%|████████▍ | 84/100 [00:08<00:01,  9.31it/s]Measuring inference with batch_size=16:  85%|████████▌ | 85/100 [00:08<00:01,  9.27it/s]Measuring inference with batch_size=16:  86%|████████▌ | 86/100 [00:09<00:01,  9.36it/s]Measuring inference with batch_size=16:  87%|████████▋ | 87/100 [00:09<00:01,  9.42it/s]Measuring inference with batch_size=16:  88%|████████▊ | 88/100 [00:09<00:01,  9.46it/s]Measuring inference with batch_size=16:  89%|████████▉ | 89/100 [00:09<00:01,  9.51it/s]Measuring inference with batch_size=16:  90%|█████████ | 90/100 [00:09<00:01,  9.42it/s]Measuring inference with batch_size=16:  91%|█████████ | 91/100 [00:09<00:00,  9.46it/s]Measuring inference with batch_size=16:  92%|█████████▏| 92/100 [00:09<00:00,  9.41it/s]Measuring inference with batch_size=16:  93%|█████████▎| 93/100 [00:09<00:00,  9.45it/s]Measuring inference with batch_size=16:  94%|█████████▍| 94/100 [00:09<00:00,  9.39it/s]Measuring inference with batch_size=16:  95%|█████████▌| 95/100 [00:10<00:00,  9.34it/s]Measuring inference with batch_size=16:  96%|█████████▌| 96/100 [00:10<00:00,  9.30it/s]Measuring inference with batch_size=16:  97%|█████████▋| 97/100 [00:10<00:00,  9.36it/s]Measuring inference with batch_size=16:  98%|█████████▊| 98/100 [00:10<00:00,  9.40it/s]Measuring inference with batch_size=16:  99%|█████████▉| 99/100 [00:10<00:00,  9.31it/s]Measuring inference with batch_size=16: 100%|██████████| 100/100 [00:10<00:00,  9.32it/s]Measuring inference with batch_size=16: 100%|██████████| 100/100 [00:10<00:00,  9.44it/s]
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "7.951 \xB5s +/- 2.756 \xB5s [5.007 \xB5s, 16.212 \xB5s]"
      batches_per_second: 138.14 K +/- 37.29 K [61.68 K, 199.73 K]
    metrics:
      batches_per_second_max: 199728.7619047619
      batches_per_second_mean: 138138.409482417
      batches_per_second_min: 61680.94117647059
      batches_per_second_std: 37293.761235721664
      seconds_per_batch_max: 1.621246337890625e-05
      seconds_per_batch_mean: 7.95125961303711e-06
      seconds_per_batch_min: 5.0067901611328125e-06
      seconds_per_batch_std: 2.755849612269401e-06
  gpu_to_cpu:
    human_readable:
      batch_latency: "296.717 \xB5s +/- 108.747 \xB5s [199.795 \xB5s, 1.177 ms]"
      batches_per_second: 3.62 K +/- 825.99 [849.91, 5.01 K]
    metrics:
      batches_per_second_max: 5005.136038186158
      batches_per_second_mean: 3616.332129052905
      batches_per_second_min: 849.9096251266463
      batches_per_second_std: 825.9905759166558
      seconds_per_batch_max: 0.001176595687866211
      seconds_per_batch_mean: 0.00029671669006347656
      seconds_per_batch_min: 0.00019979476928710938
      seconds_per_batch_std: 0.00010874658720716445
  on_device_inference:
    human_readable:
      batch_latency: 104.730 ms +/- 1.777 ms [102.515 ms, 109.627 ms]
      batches_per_second: 9.55 +/- 0.16 [9.12, 9.75]
    metrics:
      batches_per_second_max: 9.754694426226457
      batches_per_second_mean: 9.551083095033094
      batches_per_second_min: 9.121879397225358
      batches_per_second_std: 0.16030084322876073
      seconds_per_batch_max: 0.10962653160095215
      seconds_per_batch_mean: 0.10472999572753906
      seconds_per_batch_min: 0.10251474380493164
      seconds_per_batch_std: 0.0017774113835844951
  total:
    human_readable:
      batch_latency: 105.035 ms +/- 1.792 ms [102.808 ms, 109.990 ms]
      batches_per_second: 9.52 +/- 0.16 [9.09, 9.73]
    metrics:
      batches_per_second_max: 9.726914746084239
      batches_per_second_mean: 9.523407049011245
      batches_per_second_min: 9.091705991117045
      batches_per_second_std: 0.1606736881890027
      seconds_per_batch_max: 0.10999035835266113
      seconds_per_batch_mean: 0.10503466367721558
      seconds_per_batch_min: 0.10280752182006836
      seconds_per_batch_std: 0.0017917494487345274

Inference energy: 60.93153206486702 J (1.692542557357417e-05 kWh)
Energy (batch_size=1):
  joules: 60.93153206486702
  kWh: 1.692542557357417e-05

Warming up with batch_size=16:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=16:  10%|█         | 1/10 [00:02<00:18,  2.03s/it]Warming up with batch_size=16:  20%|██        | 2/10 [00:04<00:16,  2.04s/it]Warming up with batch_size=16:  30%|███       | 3/10 [00:06<00:14,  2.03s/it]Warming up with batch_size=16:  40%|████      | 4/10 [00:08<00:12,  2.03s/it]Warming up with batch_size=16:  50%|█████     | 5/10 [00:10<00:10,  2.03s/it]Warming up with batch_size=16:  60%|██████    | 6/10 [00:12<00:08,  2.04s/it]Warming up with batch_size=16:  70%|███████   | 7/10 [00:14<00:06,  2.04s/it]Warming up with batch_size=16:  80%|████████  | 8/10 [00:16<00:04,  2.04s/it]Warming up with batch_size=16:  90%|█████████ | 9/10 [00:18<00:02,  2.04s/it]Warming up with batch_size=16: 100%|██████████| 10/10 [00:20<00:00,  2.04s/it]Warming up with batch_size=16: 100%|██████████| 10/10 [00:20<00:00,  2.04s/it]
Measuring inference with batch_size=16:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=16:   1%|          | 1/100 [00:02<03:21,  2.04s/it]Measuring inference with batch_size=16:   2%|▏         | 2/100 [00:04<03:19,  2.04s/it]Measuring inference with batch_size=16:   3%|▎         | 3/100 [00:06<03:17,  2.04s/it]Measuring inference with batch_size=16:   4%|▍         | 4/100 [00:08<03:15,  2.04s/it]Measuring inference with batch_size=16:   5%|▌         | 5/100 [00:10<03:13,  2.04s/it]Measuring inference with batch_size=16:   6%|▌         | 6/100 [00:12<03:11,  2.04s/it]Measuring inference with batch_size=16:   7%|▋         | 7/100 [00:14<03:09,  2.04s/it]Measuring inference with batch_size=16:   8%|▊         | 8/100 [00:16<03:07,  2.04s/it]Measuring inference with batch_size=16:   9%|▉         | 9/100 [00:18<03:05,  2.04s/it]Measuring inference with batch_size=16:  10%|█         | 10/100 [00:20<03:03,  2.03s/it]Measuring inference with batch_size=16:  11%|█         | 11/100 [00:22<03:00,  2.03s/it]Measuring inference with batch_size=16:  12%|█▏        | 12/100 [00:24<02:58,  2.03s/it]Measuring inference with batch_size=16:  13%|█▎        | 13/100 [00:26<02:56,  2.03s/it]Measuring inference with batch_size=16:  14%|█▍        | 14/100 [00:28<02:54,  2.03s/it]Measuring inference with batch_size=16:  15%|█▌        | 15/100 [00:30<02:52,  2.03s/it]Measuring inference with batch_size=16:  16%|█▌        | 16/100 [00:32<02:50,  2.03s/it]Measuring inference with batch_size=16:  17%|█▋        | 17/100 [00:34<02:48,  2.03s/it]Measuring inference with batch_size=16:  18%|█▊        | 18/100 [00:36<02:46,  2.03s/it]Measuring inference with batch_size=16:  19%|█▉        | 19/100 [00:38<02:44,  2.03s/it]Measuring inference with batch_size=16:  20%|██        | 20/100 [00:40<02:42,  2.03s/it]Measuring inference with batch_size=16:  21%|██        | 21/100 [00:42<02:40,  2.03s/it]Measuring inference with batch_size=16:  22%|██▏       | 22/100 [00:44<02:38,  2.03s/it]Measuring inference with batch_size=16:  23%|██▎       | 23/100 [00:46<02:36,  2.03s/it]Measuring inference with batch_size=16:  24%|██▍       | 24/100 [00:48<02:34,  2.03s/it]Measuring inference with batch_size=16:  25%|██▌       | 25/100 [00:50<02:32,  2.04s/it]Measuring inference with batch_size=16:  26%|██▌       | 26/100 [00:52<02:30,  2.03s/it]Measuring inference with batch_size=16:  27%|██▋       | 27/100 [00:54<02:28,  2.03s/it]Measuring inference with batch_size=16:  28%|██▊       | 28/100 [00:56<02:26,  2.03s/it]Measuring inference with batch_size=16:  29%|██▉       | 29/100 [00:59<02:24,  2.04s/it]Measuring inference with batch_size=16:  30%|███       | 30/100 [01:01<02:22,  2.04s/it]Measuring inference with batch_size=16:  31%|███       | 31/100 [01:03<02:20,  2.04s/it]Measuring inference with batch_size=16:  32%|███▏      | 32/100 [01:05<02:18,  2.04s/it]Measuring inference with batch_size=16:  33%|███▎      | 33/100 [01:07<02:16,  2.04s/it]Measuring inference with batch_size=16:  34%|███▍      | 34/100 [01:09<02:14,  2.04s/it]Measuring inference with batch_size=16:  35%|███▌      | 35/100 [01:11<02:12,  2.04s/it]Measuring inference with batch_size=16:  36%|███▌      | 36/100 [01:13<02:10,  2.04s/it]Measuring inference with batch_size=16:  37%|███▋      | 37/100 [01:15<02:08,  2.04s/it]Measuring inference with batch_size=16:  38%|███▊      | 38/100 [01:17<02:05,  2.03s/it]Measuring inference with batch_size=16:  39%|███▉      | 39/100 [01:19<02:03,  2.03s/it]Measuring inference with batch_size=16:  40%|████      | 40/100 [01:21<02:01,  2.03s/it]Measuring inference with batch_size=16:  41%|████      | 41/100 [01:23<01:59,  2.03s/it]Measuring inference with batch_size=16:  42%|████▏     | 42/100 [01:25<01:57,  2.03s/it]Measuring inference with batch_size=16:  43%|████▎     | 43/100 [01:27<01:55,  2.03s/it]Measuring inference with batch_size=16:  44%|████▍     | 44/100 [01:29<01:53,  2.03s/it]Measuring inference with batch_size=16:  45%|████▌     | 45/100 [01:31<01:52,  2.04s/it]Measuring inference with batch_size=16:  46%|████▌     | 46/100 [01:33<01:50,  2.04s/it]Measuring inference with batch_size=16:  47%|████▋     | 47/100 [01:35<01:47,  2.03s/it]Measuring inference with batch_size=16:  48%|████▊     | 48/100 [01:37<01:45,  2.04s/it]Measuring inference with batch_size=16:  49%|████▉     | 49/100 [01:39<01:43,  2.04s/it]Measuring inference with batch_size=16:  50%|█████     | 50/100 [01:41<01:41,  2.04s/it]Measuring inference with batch_size=16:  51%|█████     | 51/100 [01:43<01:39,  2.04s/it]Measuring inference with batch_size=16:  52%|█████▏    | 52/100 [01:45<01:37,  2.04s/it]Measuring inference with batch_size=16:  53%|█████▎    | 53/100 [01:47<01:35,  2.04s/it]Measuring inference with batch_size=16:  54%|█████▍    | 54/100 [01:49<01:33,  2.04s/it]Measuring inference with batch_size=16:  55%|█████▌    | 55/100 [01:51<01:31,  2.03s/it]Measuring inference with batch_size=16:  56%|█████▌    | 56/100 [01:53<01:29,  2.03s/it]Measuring inference with batch_size=16:  57%|█████▋    | 57/100 [01:55<01:27,  2.03s/it]Measuring inference with batch_size=16:  58%|█████▊    | 58/100 [01:58<01:25,  2.03s/it]Measuring inference with batch_size=16:  59%|█████▉    | 59/100 [02:00<01:23,  2.03s/it]Measuring inference with batch_size=16:  60%|██████    | 60/100 [02:02<01:21,  2.04s/it]Measuring inference with batch_size=16:  61%|██████    | 61/100 [02:04<01:19,  2.04s/it]Measuring inference with batch_size=16:  62%|██████▏   | 62/100 [02:06<01:17,  2.04s/it]Measuring inference with batch_size=16:  63%|██████▎   | 63/100 [02:08<01:15,  2.04s/it]Measuring inference with batch_size=16:  64%|██████▍   | 64/100 [02:10<01:13,  2.03s/it]Measuring inference with batch_size=16:  65%|██████▌   | 65/100 [02:12<01:11,  2.03s/it]Measuring inference with batch_size=16:  66%|██████▌   | 66/100 [02:14<01:09,  2.03s/it]Measuring inference with batch_size=16:  67%|██████▋   | 67/100 [02:16<01:07,  2.03s/it]Measuring inference with batch_size=16:  68%|██████▊   | 68/100 [02:18<01:05,  2.03s/it]Measuring inference with batch_size=16:  69%|██████▉   | 69/100 [02:20<01:02,  2.03s/it]Measuring inference with batch_size=16:  70%|███████   | 70/100 [02:22<01:00,  2.03s/it]Measuring inference with batch_size=16:  71%|███████   | 71/100 [02:24<00:58,  2.03s/it]Measuring inference with batch_size=16:  72%|███████▏  | 72/100 [02:26<00:56,  2.03s/it]Measuring inference with batch_size=16:  73%|███████▎  | 73/100 [02:28<00:54,  2.03s/it]Measuring inference with batch_size=16:  74%|███████▍  | 74/100 [02:30<00:52,  2.04s/it]Measuring inference with batch_size=16:  75%|███████▌  | 75/100 [02:32<00:50,  2.04s/it]Measuring inference with batch_size=16:  76%|███████▌  | 76/100 [02:34<00:50,  2.09s/it]Measuring inference with batch_size=16:  77%|███████▋  | 77/100 [02:36<00:47,  2.08s/it]Measuring inference with batch_size=16:  78%|███████▊  | 78/100 [02:38<00:45,  2.07s/it]Measuring inference with batch_size=16:  79%|███████▉  | 79/100 [02:40<00:43,  2.06s/it]Measuring inference with batch_size=16:  80%|████████  | 80/100 [02:42<00:41,  2.05s/it]Measuring inference with batch_size=16:  81%|████████  | 81/100 [02:45<00:38,  2.05s/it]Measuring inference with batch_size=16:  82%|████████▏ | 82/100 [02:47<00:36,  2.05s/it]Measuring inference with batch_size=16:  83%|████████▎ | 83/100 [02:49<00:34,  2.04s/it]Measuring inference with batch_size=16:  84%|████████▍ | 84/100 [02:51<00:33,  2.07s/it]Measuring inference with batch_size=16:  85%|████████▌ | 85/100 [02:53<00:32,  2.16s/it]Measuring inference with batch_size=16:  86%|████████▌ | 86/100 [02:55<00:30,  2.15s/it]Measuring inference with batch_size=16:  87%|████████▋ | 87/100 [02:57<00:27,  2.12s/it]Measuring inference with batch_size=16:  88%|████████▊ | 88/100 [02:59<00:25,  2.10s/it]Measuring inference with batch_size=16:  89%|████████▉ | 89/100 [03:01<00:22,  2.08s/it]Measuring inference with batch_size=16:  90%|█████████ | 90/100 [03:03<00:20,  2.07s/it]Measuring inference with batch_size=16:  91%|█████████ | 91/100 [03:05<00:18,  2.06s/it]Measuring inference with batch_size=16:  92%|█████████▏| 92/100 [03:07<00:16,  2.05s/it]Measuring inference with batch_size=16:  93%|█████████▎| 93/100 [03:10<00:14,  2.05s/it]Measuring inference with batch_size=16:  94%|█████████▍| 94/100 [03:12<00:12,  2.04s/it]Measuring inference with batch_size=16:  95%|█████████▌| 95/100 [03:14<00:10,  2.04s/it]Measuring inference with batch_size=16:  96%|█████████▌| 96/100 [03:16<00:08,  2.04s/it]Measuring inference with batch_size=16:  97%|█████████▋| 97/100 [03:18<00:06,  2.04s/it]Measuring inference with batch_size=16:  98%|█████████▊| 98/100 [03:20<00:04,  2.04s/it]Measuring inference with batch_size=16:  99%|█████████▉| 99/100 [03:22<00:02,  2.04s/it]Measuring inference with batch_size=16: 100%|██████████| 100/100 [03:24<00:00,  2.11s/it]Measuring inference with batch_size=16: 100%|██████████| 100/100 [03:24<00:00,  2.04s/it]
INFO:benchmark:learner.infer:
  device: cuda
  energy:
    batch_size_1:
      joules: 60.93153206486702
      kWh: 1.692542557357417e-05
    batch_size_16:
      joules: 60.302153694995226
      kWh: 1.6750598248609784e-05
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 20.24 GB
      total: 31.17 GB
      used: 12.81 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  max_inference_memory: 9586036736
  post_inference_memory: 30891008
  pre_inference_memory: 30891008
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "7.951 \xB5s +/- 2.756 \xB5s [5.007 \xB5s, 16.212 \xB5s]"
          batches_per_second: 138.14 K +/- 37.29 K [61.68 K, 199.73 K]
        metrics:
          batches_per_second_max: 199728.7619047619
          batches_per_second_mean: 138138.409482417
          batches_per_second_min: 61680.94117647059
          batches_per_second_std: 37293.761235721664
          seconds_per_batch_max: 1.621246337890625e-05
          seconds_per_batch_mean: 7.95125961303711e-06
          seconds_per_batch_min: 5.0067901611328125e-06
          seconds_per_batch_std: 2.755849612269401e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: "296.717 \xB5s +/- 108.747 \xB5s [199.795 \xB5s, 1.177 ms]"
          batches_per_second: 3.62 K +/- 825.99 [849.91, 5.01 K]
        metrics:
          batches_per_second_max: 5005.136038186158
          batches_per_second_mean: 3616.332129052905
          batches_per_second_min: 849.9096251266463
          batches_per_second_std: 825.9905759166558
          seconds_per_batch_max: 0.001176595687866211
          seconds_per_batch_mean: 0.00029671669006347656
          seconds_per_batch_min: 0.00019979476928710938
          seconds_per_batch_std: 0.00010874658720716445
      on_device_inference:
        human_readable:
          batch_latency: 104.730 ms +/- 1.777 ms [102.515 ms, 109.627 ms]
          batches_per_second: 9.55 +/- 0.16 [9.12, 9.75]
        metrics:
          batches_per_second_max: 9.754694426226457
          batches_per_second_mean: 9.551083095033094
          batches_per_second_min: 9.121879397225358
          batches_per_second_std: 0.16030084322876073
          seconds_per_batch_max: 0.10962653160095215
          seconds_per_batch_mean: 0.10472999572753906
          seconds_per_batch_min: 0.10251474380493164
          seconds_per_batch_std: 0.0017774113835844951
      total:
        human_readable:
          batch_latency: 105.035 ms +/- 1.792 ms [102.808 ms, 109.990 ms]
          batches_per_second: 9.52 +/- 0.16 [9.09, 9.73]
        metrics:
          batches_per_second_max: 9.726914746084239
          batches_per_second_mean: 9.523407049011245
          batches_per_second_min: 9.091705991117045
          batches_per_second_std: 0.1606736881890027
          seconds_per_batch_max: 0.10999035835266113
          seconds_per_batch_mean: 0.10503466367721558
          seconds_per_batch_min: 0.10280752182006836
          seconds_per_batch_std: 0.0017917494487345274
    batch_size_16:
      cpu_to_gpu:
        human_readable:
          batch_latency: "9.136 \xB5s +/- 2.436 \xB5s [6.199 \xB5s, 23.365 \xB5s]"
          batches_per_second: 113.99 K +/- 18.62 K [42.80 K, 161.32 K]
        metrics:
          batches_per_second_max: 161319.38461538462
          batches_per_second_mean: 113994.76393356902
          batches_per_second_min: 42799.02040816326
          batches_per_second_std: 18621.92000533826
          seconds_per_batch_max: 2.3365020751953125e-05
          seconds_per_batch_mean: 9.136199951171875e-06
          seconds_per_batch_min: 6.198883056640625e-06
          seconds_per_batch_std: 2.4355785284216683e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: "2.224 ms +/- 154.221 \xB5s [1.580 ms, 2.985 ms]"
          batches_per_second: 451.79 +/- 32.92 [335.06, 632.91]
        metrics:
          batches_per_second_max: 632.9114229666516
          batches_per_second_mean: 451.79044753874695
          batches_per_second_min: 335.06183096341266
          batches_per_second_std: 32.92153553540077
          seconds_per_batch_max: 0.0029845237731933594
          seconds_per_batch_mean: 0.0022243404388427733
          seconds_per_batch_min: 0.0015799999237060547
          seconds_per_batch_std: 0.0001542212171016348
      on_device_inference:
        human_readable:
          batch_latency: 2.042 s +/- 45.242 ms [2.019 s, 2.372 s]
          batches_per_second: 0.49 +/- 0.01 [0.42, 0.50]
        metrics:
          batches_per_second_max: 0.4953827859754235
          batches_per_second_mean: 0.4900339176863096
          batches_per_second_min: 0.4216501349705389
          batches_per_second_std: 0.00970365536309823
          seconds_per_batch_max: 2.3716344833374023
          seconds_per_batch_mean: 2.041570188999176
          seconds_per_batch_min: 2.0186409950256348
          seconds_per_batch_std: 0.045242298030737406
      total:
        human_readable:
          batch_latency: 2.044 s +/- 45.223 ms [2.021 s, 2.374 s]
          batches_per_second: 0.49 +/- 0.01 [0.42, 0.49]
        metrics:
          batches_per_second_max: 0.4948364479972138
          batches_per_second_mean: 0.489497784131661
          batches_per_second_min: 0.4212559548057462
          batches_per_second_std: 0.009679649233840848
          seconds_per_batch_max: 2.3738536834716797
          seconds_per_batch_mean: 2.04380366563797
          seconds_per_batch_min: 2.020869731903076
          seconds_per_batch_std: 0.04522296573668035

INFO:benchmark:== Benchmarking model directly ==
Timing results (batch_size=16):
  cpu_to_gpu:
    human_readable:
      batch_latency: "9.136 \xB5s +/- 2.436 \xB5s [6.199 \xB5s, 23.365 \xB5s]"
      batches_per_second: 113.99 K +/- 18.62 K [42.80 K, 161.32 K]
    metrics:
      batches_per_second_max: 161319.38461538462
      batches_per_second_mean: 113994.76393356902
      batches_per_second_min: 42799.02040816326
      batches_per_second_std: 18621.92000533826
      seconds_per_batch_max: 2.3365020751953125e-05
      seconds_per_batch_mean: 9.136199951171875e-06
      seconds_per_batch_min: 6.198883056640625e-06
      seconds_per_batch_std: 2.4355785284216683e-06
  gpu_to_cpu:
    human_readable:
      batch_latency: "2.224 ms +/- 154.221 \xB5s [1.580 ms, 2.985 ms]"
      batches_per_second: 451.79 +/- 32.92 [335.06, 632.91]
    metrics:
      batches_per_second_max: 632.9114229666516
      batches_per_second_mean: 451.79044753874695
      batches_per_second_min: 335.06183096341266
      batches_per_second_std: 32.92153553540077
      seconds_per_batch_max: 0.0029845237731933594
      seconds_per_batch_mean: 0.0022243404388427733
      seconds_per_batch_min: 0.0015799999237060547
      seconds_per_batch_std: 0.0001542212171016348
  on_device_inference:
    human_readable:
      batch_latency: 2.042 s +/- 45.242 ms [2.019 s, 2.372 s]
      batches_per_second: 0.49 +/- 0.01 [0.42, 0.50]
    metrics:
      batches_per_second_max: 0.4953827859754235
      batches_per_second_mean: 0.4900339176863096
      batches_per_second_min: 0.4216501349705389
      batches_per_second_std: 0.00970365536309823
      seconds_per_batch_max: 2.3716344833374023
      seconds_per_batch_mean: 2.041570188999176
      seconds_per_batch_min: 2.0186409950256348
      seconds_per_batch_std: 0.045242298030737406
  total:
    human_readable:
      batch_latency: 2.044 s +/- 45.223 ms [2.021 s, 2.374 s]
      batches_per_second: 0.49 +/- 0.01 [0.42, 0.49]
    metrics:
      batches_per_second_max: 0.4948364479972138
      batches_per_second_mean: 0.489497784131661
      batches_per_second_min: 0.4212559548057462
      batches_per_second_std: 0.009679649233840848
      seconds_per_batch_max: 2.3738536834716797
      seconds_per_batch_mean: 2.04380366563797
      seconds_per_batch_min: 2.020869731903076
      seconds_per_batch_std: 0.04522296573668035

Inference energy: 60.302153694995226 J (1.6750598248609784e-05 kWh)
Energy (batch_size=16):
  joules: 60.302153694995226
  kWh: 1.6750598248609784e-05

Machine info:
  cpu:
    architecture: aarch64
    cores:
      physical: 8
      total: 8
    frequency: 2.27 GHz
    model: ARMv8 Processor rev 0 (v8l)
  gpus: null
  memory:
    available: 11.22 GB
    total: 31.17 GB
    used: 21.81 GB
  system:
    node: xavier
    release: 4.9.140-tegra
    system: Linux

Model device: cuda:0
Model parameters: 3794322 (3.79 M)
Model FLOPs: 2061365744 (2.06 G)
Allocated GPU memory prior to inference: 30891008 (29.46 MB)
Allocated GPU memory after to inference: 30891008 (29.46 MB)
Max allocated GPU memory during inference: 9580759040 (8.92 GB)
Warming up with batch_size=16:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=16:  10%|█         | 1/10 [00:00<00:01,  8.08it/s]Warming up with batch_size=16:  20%|██        | 2/10 [00:00<00:00,  8.52it/s]Warming up with batch_size=16:  30%|███       | 3/10 [00:00<00:00,  9.07it/s]Warming up with batch_size=16:  40%|████      | 4/10 [00:00<00:00,  8.66it/s]Warming up with batch_size=16:  50%|█████     | 5/10 [00:00<00:00,  8.59it/s]Warming up with batch_size=16:  60%|██████    | 6/10 [00:00<00:00,  8.72it/s]Warming up with batch_size=16:  80%|████████  | 8/10 [00:00<00:00,  9.38it/s]Warming up with batch_size=16: 100%|██████████| 10/10 [00:01<00:00,  9.64it/s]Warming up with batch_size=16: 100%|██████████| 10/10 [00:01<00:00,  9.18it/s]
Measuring inference with batch_size=16:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=16:   1%|          | 1/100 [00:00<00:10,  9.55it/s]Measuring inference with batch_size=16:   2%|▏         | 2/100 [00:00<00:10,  8.96it/s]Measuring inference with batch_size=16:   4%|▍         | 4/100 [00:00<00:09,  9.67it/s]Measuring inference with batch_size=16:   6%|▌         | 6/100 [00:00<00:09,  9.82it/s]Measuring inference with batch_size=16:   7%|▋         | 7/100 [00:00<00:09,  9.79it/s]Measuring inference with batch_size=16:   9%|▉         | 9/100 [00:00<00:09,  9.96it/s]Measuring inference with batch_size=16:  11%|█         | 11/100 [00:01<00:08, 10.04it/s]Measuring inference with batch_size=16:  13%|█▎        | 13/100 [00:01<00:08, 10.09it/s]Measuring inference with batch_size=16:  15%|█▌        | 15/100 [00:01<00:08, 10.10it/s]Measuring inference with batch_size=16:  17%|█▋        | 17/100 [00:01<00:08, 10.13it/s]Measuring inference with batch_size=16:  19%|█▉        | 19/100 [00:01<00:07, 10.15it/s]Measuring inference with batch_size=16:  21%|██        | 21/100 [00:02<00:07, 10.16it/s]Measuring inference with batch_size=16:  23%|██▎       | 23/100 [00:02<00:07, 10.17it/s]Measuring inference with batch_size=16:  25%|██▌       | 25/100 [00:02<00:07, 10.19it/s]Measuring inference with batch_size=16:  27%|██▋       | 27/100 [00:02<00:07, 10.20it/s]Measuring inference with batch_size=16:  29%|██▉       | 29/100 [00:02<00:06, 10.21it/s]Measuring inference with batch_size=16:  31%|███       | 31/100 [00:03<00:06, 10.22it/s]Measuring inference with batch_size=16:  33%|███▎      | 33/100 [00:03<00:06, 10.21it/s]Measuring inference with batch_size=16:  35%|███▌      | 35/100 [00:03<00:06, 10.20it/s]Measuring inference with batch_size=16:  37%|███▋      | 37/100 [00:03<00:06, 10.20it/s]Measuring inference with batch_size=16:  39%|███▉      | 39/100 [00:03<00:05, 10.20it/s]Measuring inference with batch_size=16:  41%|████      | 41/100 [00:04<00:05, 10.20it/s]Measuring inference with batch_size=16:  43%|████▎     | 43/100 [00:04<00:05, 10.20it/s]Measuring inference with batch_size=16:  45%|████▌     | 45/100 [00:04<00:05, 10.20it/s]Measuring inference with batch_size=16:  47%|████▋     | 47/100 [00:04<00:05, 10.20it/s]Measuring inference with batch_size=16:  49%|████▉     | 49/100 [00:04<00:04, 10.21it/s]Measuring inference with batch_size=16:  51%|█████     | 51/100 [00:05<00:04, 10.20it/s]Measuring inference with batch_size=16:  53%|█████▎    | 53/100 [00:05<00:04, 10.21it/s]Measuring inference with batch_size=16:  55%|█████▌    | 55/100 [00:05<00:04, 10.21it/s]Measuring inference with batch_size=16:  57%|█████▋    | 57/100 [00:05<00:04, 10.21it/s]Measuring inference with batch_size=16:  59%|█████▉    | 59/100 [00:05<00:04, 10.21it/s]Measuring inference with batch_size=16:  61%|██████    | 61/100 [00:06<00:03, 10.21it/s]Measuring inference with batch_size=16:  63%|██████▎   | 63/100 [00:06<00:03, 10.22it/s]Measuring inference with batch_size=16:  65%|██████▌   | 65/100 [00:06<00:03, 10.21it/s]Measuring inference with batch_size=16:  67%|██████▋   | 67/100 [00:06<00:03, 10.21it/s]Measuring inference with batch_size=16:  69%|██████▉   | 69/100 [00:06<00:03, 10.21it/s]Measuring inference with batch_size=16:  71%|███████   | 71/100 [00:06<00:02, 10.19it/s]Measuring inference with batch_size=16:  73%|███████▎  | 73/100 [00:07<00:02, 10.19it/s]Measuring inference with batch_size=16:  75%|███████▌  | 75/100 [00:07<00:02, 10.19it/s]Measuring inference with batch_size=16:  77%|███████▋  | 77/100 [00:07<00:02, 10.19it/s]Measuring inference with batch_size=16:  79%|███████▉  | 79/100 [00:07<00:02, 10.20it/s]Measuring inference with batch_size=16:  81%|████████  | 81/100 [00:07<00:01, 10.19it/s]Measuring inference with batch_size=16:  83%|████████▎ | 83/100 [00:08<00:01, 10.19it/s]Measuring inference with batch_size=16:  85%|████████▌ | 85/100 [00:08<00:01, 10.19it/s]Measuring inference with batch_size=16:  87%|████████▋ | 87/100 [00:08<00:01, 10.19it/s]Measuring inference with batch_size=16:  89%|████████▉ | 89/100 [00:08<00:01, 10.21it/s]Measuring inference with batch_size=16:  91%|█████████ | 91/100 [00:08<00:00, 10.21it/s]Measuring inference with batch_size=16:  93%|█████████▎| 93/100 [00:09<00:00, 10.21it/s]Measuring inference with batch_size=16:  95%|█████████▌| 95/100 [00:09<00:00, 10.21it/s]Measuring inference with batch_size=16:  97%|█████████▋| 97/100 [00:09<00:00, 10.20it/s]Measuring inference with batch_size=16:  99%|█████████▉| 99/100 [00:09<00:00, 10.20it/s]Measuring inference with batch_size=16: 100%|██████████| 100/100 [00:09<00:00, 10.16it/s]
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "766.654 \xB5s +/- 123.199 \xB5s [604.630 \xB5s, 1.100 ms]"
      batches_per_second: 1.34 K +/- 193.48 [909.43, 1.65 K]
    metrics:
      batches_per_second_max: 1653.9053627760252
      batches_per_second_mean: 1335.0896306561285
      batches_per_second_min: 909.4327840416305
      batches_per_second_std: 193.47685816405405
      seconds_per_batch_max: 0.0010995864868164062
      seconds_per_batch_mean: 0.0007666540145874023
      seconds_per_batch_min: 0.0006046295166015625
      seconds_per_batch_std: 0.0001231989299579503
  gpu_to_cpu:
    human_readable:
      batch_latency: "2.183 ms +/- 593.973 \xB5s [212.431 \xB5s, 4.913 ms]"
      batches_per_second: 562.62 +/- 610.28 [203.53, 4.71 K]
    metrics:
      batches_per_second_max: 4707.4118967452305
      batches_per_second_mean: 562.6232762872197
      batches_per_second_min: 203.527950310559
      batches_per_second_std: 610.2797430636498
      seconds_per_batch_max: 0.004913330078125
      seconds_per_batch_mean: 0.002182927131652832
      seconds_per_batch_min: 0.0002124309539794922
      seconds_per_batch_std: 0.000593973121509146
  on_device_inference:
    human_readable:
      batch_latency: 94.894 ms +/- 2.022 ms [92.649 ms, 112.726 ms]
      batches_per_second: 10.54 +/- 0.20 [8.87, 10.79]
    metrics:
      batches_per_second_max: 10.793398850743312
      batches_per_second_mean: 10.54220566109679
      batches_per_second_min: 8.871088776369167
      batches_per_second_std: 0.1964535434674774
      seconds_per_batch_max: 0.11272573471069336
      seconds_per_batch_mean: 0.0948944091796875
      seconds_per_batch_min: 0.09264922142028809
      seconds_per_batch_std: 0.0020219160101698557
  total:
    human_readable:
      batch_latency: 97.844 ms +/- 2.013 ms [96.776 ms, 115.382 ms]
      batches_per_second: 10.22 +/- 0.18 [8.67, 10.33]
    metrics:
      batches_per_second_max: 10.333164986068237
      batches_per_second_mean: 10.224131972622601
      batches_per_second_min: 8.666866413057628
      batches_per_second_std: 0.18402712128880663
      seconds_per_batch_max: 0.11538195610046387
      seconds_per_batch_mean: 0.09784399032592773
      seconds_per_batch_min: 0.09677577018737793
      seconds_per_batch_std: 0.0020125209977427

Inference energy: 53.82634023605983 J (1.4951761176683285e-05 kWh)
Energy (batch_size=1):
  joules: 53.82634023605983
  kWh: 1.4951761176683285e-05

Warming up with batch_size=16:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=16:  10%|█         | 1/10 [00:01<00:17,  1.97s/it]Warming up with batch_size=16:  20%|██        | 2/10 [00:03<00:15,  1.97s/it]Warming up with batch_size=16:  30%|███       | 3/10 [00:05<00:13,  1.97s/it]Warming up with batch_size=16:  40%|████      | 4/10 [00:07<00:11,  1.97s/it]Warming up with batch_size=16:  50%|█████     | 5/10 [00:09<00:09,  1.97s/it]Warming up with batch_size=16:  60%|██████    | 6/10 [00:11<00:07,  1.98s/it]Warming up with batch_size=16:  70%|███████   | 7/10 [00:13<00:05,  1.98s/it]Warming up with batch_size=16:  80%|████████  | 8/10 [00:15<00:03,  1.98s/it]Warming up with batch_size=16:  90%|█████████ | 9/10 [00:17<00:01,  1.98s/it]Warming up with batch_size=16: 100%|██████████| 10/10 [00:19<00:00,  1.98s/it]Warming up with batch_size=16: 100%|██████████| 10/10 [00:19<00:00,  1.98s/it]
Measuring inference with batch_size=16:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=16:   1%|          | 1/100 [00:01<03:14,  1.97s/it]Measuring inference with batch_size=16:   2%|▏         | 2/100 [00:03<03:13,  1.97s/it]Measuring inference with batch_size=16:   3%|▎         | 3/100 [00:05<03:11,  1.97s/it]Measuring inference with batch_size=16:   4%|▍         | 4/100 [00:07<03:09,  1.97s/it]Measuring inference with batch_size=16:   5%|▌         | 5/100 [00:09<03:07,  1.97s/it]Measuring inference with batch_size=16:   6%|▌         | 6/100 [00:11<03:05,  1.97s/it]Measuring inference with batch_size=16:   7%|▋         | 7/100 [00:13<03:03,  1.97s/it]Measuring inference with batch_size=16:   8%|▊         | 8/100 [00:15<03:01,  1.97s/it]Measuring inference with batch_size=16:   9%|▉         | 9/100 [00:17<02:59,  1.98s/it]Measuring inference with batch_size=16:  10%|█         | 10/100 [00:19<02:57,  1.97s/it]Measuring inference with batch_size=16:  11%|█         | 11/100 [00:21<02:55,  1.97s/it]Measuring inference with batch_size=16:  12%|█▏        | 12/100 [00:23<02:53,  1.97s/it]Measuring inference with batch_size=16:  13%|█▎        | 13/100 [00:25<02:51,  1.97s/it]Measuring inference with batch_size=16:  14%|█▍        | 14/100 [00:27<02:49,  1.97s/it]Measuring inference with batch_size=16:  15%|█▌        | 15/100 [00:29<02:47,  1.97s/it]Measuring inference with batch_size=16:  16%|█▌        | 16/100 [00:31<02:45,  1.97s/it]Measuring inference with batch_size=16:  17%|█▋        | 17/100 [00:33<02:44,  1.98s/it]Measuring inference with batch_size=16:  18%|█▊        | 18/100 [00:35<02:42,  1.98s/it]Measuring inference with batch_size=16:  19%|█▉        | 19/100 [00:37<02:39,  1.97s/it]Measuring inference with batch_size=16:  20%|██        | 20/100 [00:39<02:37,  1.97s/it]Measuring inference with batch_size=16:  21%|██        | 21/100 [00:41<02:36,  1.98s/it]Measuring inference with batch_size=16:  22%|██▏       | 22/100 [00:43<02:34,  1.98s/it]Measuring inference with batch_size=16:  23%|██▎       | 23/100 [00:45<02:32,  1.98s/it]Measuring inference with batch_size=16:  24%|██▍       | 24/100 [00:47<02:30,  1.98s/it]Measuring inference with batch_size=16:  25%|██▌       | 25/100 [00:49<02:28,  1.98s/it]Measuring inference with batch_size=16:  26%|██▌       | 26/100 [00:51<02:26,  1.98s/it]Measuring inference with batch_size=16:  27%|██▋       | 27/100 [00:53<02:24,  1.98s/it]Measuring inference with batch_size=16:  28%|██▊       | 28/100 [00:55<02:22,  1.98s/it]Measuring inference with batch_size=16:  29%|██▉       | 29/100 [00:57<02:20,  1.98s/it]Measuring inference with batch_size=16:  30%|███       | 30/100 [00:59<02:18,  1.98s/it]Measuring inference with batch_size=16:  31%|███       | 31/100 [01:01<02:16,  1.98s/it]Measuring inference with batch_size=16:  32%|███▏      | 32/100 [01:03<02:14,  1.97s/it]Measuring inference with batch_size=16:  33%|███▎      | 33/100 [01:05<02:12,  1.97s/it]Measuring inference with batch_size=16:  34%|███▍      | 34/100 [01:07<02:10,  1.98s/it]Measuring inference with batch_size=16:  35%|███▌      | 35/100 [01:09<02:08,  1.97s/it]Measuring inference with batch_size=16:  36%|███▌      | 36/100 [01:11<02:06,  1.98s/it]Measuring inference with batch_size=16:  37%|███▋      | 37/100 [01:13<02:04,  1.97s/it]Measuring inference with batch_size=16:  38%|███▊      | 38/100 [01:15<02:02,  1.97s/it]Measuring inference with batch_size=16:  39%|███▉      | 39/100 [01:17<02:00,  1.97s/it]Measuring inference with batch_size=16:  40%|████      | 40/100 [01:18<01:58,  1.97s/it]Measuring inference with batch_size=16:  41%|████      | 41/100 [01:20<01:56,  1.97s/it]Measuring inference with batch_size=16:  42%|████▏     | 42/100 [01:22<01:54,  1.97s/it]Measuring inference with batch_size=16:  43%|████▎     | 43/100 [01:24<01:52,  1.97s/it]Measuring inference with batch_size=16:  44%|████▍     | 44/100 [01:26<01:50,  1.97s/it]Measuring inference with batch_size=16:  45%|████▌     | 45/100 [01:28<01:48,  1.98s/it]Measuring inference with batch_size=16:  46%|████▌     | 46/100 [01:30<01:46,  1.98s/it]Measuring inference with batch_size=16:  47%|████▋     | 47/100 [01:32<01:44,  1.98s/it]Measuring inference with batch_size=16:  48%|████▊     | 48/100 [01:34<01:42,  1.98s/it]Measuring inference with batch_size=16:  49%|████▉     | 49/100 [01:36<01:40,  1.97s/it]Measuring inference with batch_size=16:  50%|█████     | 50/100 [01:38<01:38,  1.97s/it]Measuring inference with batch_size=16:  51%|█████     | 51/100 [01:40<01:36,  1.98s/it]Measuring inference with batch_size=16:  52%|█████▏    | 52/100 [01:42<01:34,  1.98s/it]Measuring inference with batch_size=16:  53%|█████▎    | 53/100 [01:44<01:32,  1.98s/it]Measuring inference with batch_size=16:  54%|█████▍    | 54/100 [01:46<01:30,  1.97s/it]Measuring inference with batch_size=16:  55%|█████▌    | 55/100 [01:48<01:28,  1.97s/it]Measuring inference with batch_size=16:  56%|█████▌    | 56/100 [01:50<01:26,  1.98s/it]Measuring inference with batch_size=16:  57%|█████▋    | 57/100 [01:52<01:24,  1.97s/it]Measuring inference with batch_size=16:  58%|█████▊    | 58/100 [01:54<01:22,  1.98s/it]Measuring inference with batch_size=16:  59%|█████▉    | 59/100 [01:56<01:21,  1.98s/it]Measuring inference with batch_size=16:  60%|██████    | 60/100 [01:58<01:19,  1.98s/it]Measuring inference with batch_size=16:  61%|██████    | 61/100 [02:00<01:17,  1.98s/it]Measuring inference with batch_size=16:  62%|██████▏   | 62/100 [02:02<01:15,  1.98s/it]Measuring inference with batch_size=16:  63%|██████▎   | 63/100 [02:04<01:13,  1.98s/it]Measuring inference with batch_size=16:  64%|██████▍   | 64/100 [02:06<01:11,  1.98s/it]Measuring inference with batch_size=16:  65%|██████▌   | 65/100 [02:08<01:09,  1.97s/it]Measuring inference with batch_size=16:  66%|██████▌   | 66/100 [02:10<01:07,  1.97s/it]Measuring inference with batch_size=16:  67%|██████▋   | 67/100 [02:12<01:05,  1.97s/it]Measuring inference with batch_size=16:  68%|██████▊   | 68/100 [02:14<01:03,  1.98s/it]Measuring inference with batch_size=16:  69%|██████▉   | 69/100 [02:16<01:01,  1.98s/it]Measuring inference with batch_size=16:  70%|███████   | 70/100 [02:18<00:59,  1.97s/it]Measuring inference with batch_size=16:  71%|███████   | 71/100 [02:20<00:57,  1.97s/it]Measuring inference with batch_size=16:  72%|███████▏  | 72/100 [02:22<00:55,  1.97s/it]Measuring inference with batch_size=16:  73%|███████▎  | 73/100 [02:24<00:53,  1.97s/it]Measuring inference with batch_size=16:  74%|███████▍  | 74/100 [02:26<00:51,  1.98s/it]Measuring inference with batch_size=16:  75%|███████▌  | 75/100 [02:28<00:49,  1.98s/it]Measuring inference with batch_size=16:  76%|███████▌  | 76/100 [02:30<00:47,  1.98s/it]Measuring inference with batch_size=16:  77%|███████▋  | 77/100 [02:32<00:45,  1.98s/it]Measuring inference with batch_size=16:  78%|███████▊  | 78/100 [02:34<00:43,  1.98s/it]Measuring inference with batch_size=16:  79%|███████▉  | 79/100 [02:36<00:41,  1.98s/it]Measuring inference with batch_size=16:  80%|████████  | 80/100 [02:37<00:39,  1.97s/it]Measuring inference with batch_size=16:  81%|████████  | 81/100 [02:39<00:37,  1.98s/it]Measuring inference with batch_size=16:  82%|████████▏ | 82/100 [02:41<00:35,  1.97s/it]Measuring inference with batch_size=16:  83%|████████▎ | 83/100 [02:43<00:33,  1.98s/it]Measuring inference with batch_size=16:  84%|████████▍ | 84/100 [02:45<00:31,  1.98s/it]Measuring inference with batch_size=16:  85%|████████▌ | 85/100 [02:47<00:29,  1.97s/it]Measuring inference with batch_size=16:  86%|████████▌ | 86/100 [02:49<00:27,  1.97s/it]Measuring inference with batch_size=16:  87%|████████▋ | 87/100 [02:51<00:25,  1.97s/it]Measuring inference with batch_size=16:  88%|████████▊ | 88/100 [02:53<00:23,  1.97s/it]Measuring inference with batch_size=16:  89%|████████▉ | 89/100 [02:55<00:21,  1.97s/it]Measuring inference with batch_size=16:  90%|█████████ | 90/100 [02:57<00:19,  1.98s/it]Measuring inference with batch_size=16:  91%|█████████ | 91/100 [02:59<00:17,  1.98s/it]Measuring inference with batch_size=16:  92%|█████████▏| 92/100 [03:01<00:15,  1.98s/it]Measuring inference with batch_size=16:  93%|█████████▎| 93/100 [03:03<00:13,  1.98s/it]Measuring inference with batch_size=16:  94%|█████████▍| 94/100 [03:05<00:11,  1.98s/it]Measuring inference with batch_size=16:  95%|█████████▌| 95/100 [03:07<00:09,  1.98s/it]Measuring inference with batch_size=16:  96%|█████████▌| 96/100 [03:09<00:07,  1.98s/it]Measuring inference with batch_size=16:  97%|█████████▋| 97/100 [03:11<00:05,  1.98s/it]Measuring inference with batch_size=16:  98%|█████████▊| 98/100 [03:13<00:03,  1.98s/it]Measuring inference with batch_size=16:  99%|█████████▉| 99/100 [03:15<00:01,  1.98s/it]Measuring inference with batch_size=16: 100%|██████████| 100/100 [03:17<00:00,  1.98s/it]Measuring inference with batch_size=16: 100%|██████████| 100/100 [03:17<00:00,  1.98s/it]
INFO:benchmark:learner.model.forward:
  device: cuda
  energy:
    batch_size_1:
      joules: 53.82634023605983
      kWh: 1.4951761176683285e-05
    batch_size_16:
      joules: 53.84570485862095
      kWh: 1.4957140238505818e-05
  flops: 2061365744
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 11.22 GB
      total: 31.17 GB
      used: 21.81 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  max_inference_memory: 9580759040
  params: 3794322
  post_inference_memory: 30891008
  pre_inference_memory: 30891008
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "766.654 \xB5s +/- 123.199 \xB5s [604.630 \xB5s, 1.100 ms]"
          batches_per_second: 1.34 K +/- 193.48 [909.43, 1.65 K]
        metrics:
          batches_per_second_max: 1653.9053627760252
          batches_per_second_mean: 1335.0896306561285
          batches_per_second_min: 909.4327840416305
          batches_per_second_std: 193.47685816405405
          seconds_per_batch_max: 0.0010995864868164062
          seconds_per_batch_mean: 0.0007666540145874023
          seconds_per_batch_min: 0.0006046295166015625
          seconds_per_batch_std: 0.0001231989299579503
      gpu_to_cpu:
        human_readable:
          batch_latency: "2.183 ms +/- 593.973 \xB5s [212.431 \xB5s, 4.913 ms]"
          batches_per_second: 562.62 +/- 610.28 [203.53, 4.71 K]
        metrics:
          batches_per_second_max: 4707.4118967452305
          batches_per_second_mean: 562.6232762872197
          batches_per_second_min: 203.527950310559
          batches_per_second_std: 610.2797430636498
          seconds_per_batch_max: 0.004913330078125
          seconds_per_batch_mean: 0.002182927131652832
          seconds_per_batch_min: 0.0002124309539794922
          seconds_per_batch_std: 0.000593973121509146
      on_device_inference:
        human_readable:
          batch_latency: 94.894 ms +/- 2.022 ms [92.649 ms, 112.726 ms]
          batches_per_second: 10.54 +/- 0.20 [8.87, 10.79]
        metrics:
          batches_per_second_max: 10.793398850743312
          batches_per_second_mean: 10.54220566109679
          batches_per_second_min: 8.871088776369167
          batches_per_second_std: 0.1964535434674774
          seconds_per_batch_max: 0.11272573471069336
          seconds_per_batch_mean: 0.0948944091796875
          seconds_per_batch_min: 0.09264922142028809
          seconds_per_batch_std: 0.0020219160101698557
      total:
        human_readable:
          batch_latency: 97.844 ms +/- 2.013 ms [96.776 ms, 115.382 ms]
          batches_per_second: 10.22 +/- 0.18 [8.67, 10.33]
        metrics:
          batches_per_second_max: 10.333164986068237
          batches_per_second_mean: 10.224131972622601
          batches_per_second_min: 8.666866413057628
          batches_per_second_std: 0.18402712128880663
          seconds_per_batch_max: 0.11538195610046387
          seconds_per_batch_mean: 0.09784399032592773
          seconds_per_batch_min: 0.09677577018737793
          seconds_per_batch_std: 0.0020125209977427
    batch_size_16:
      cpu_to_gpu:
        human_readable:
          batch_latency: "10.575 ms +/- 307.665 \xB5s [9.647 ms, 11.338 ms]"
          batches_per_second: 94.64 +/- 2.77 [88.20, 103.66]
        metrics:
          batches_per_second_max: 103.65519968366944
          batches_per_second_mean: 94.64294212093345
          batches_per_second_min: 88.19715703591555
          batches_per_second_std: 2.773012920635653
          seconds_per_batch_max: 0.011338233947753906
          seconds_per_batch_mean: 0.010575034618377686
          seconds_per_batch_min: 0.009647369384765625
          seconds_per_batch_std: 0.00030766511771005475
      gpu_to_cpu:
        human_readable:
          batch_latency: 782.399 ms +/- 2.318 ms [776.550 ms, 787.682 ms]
          batches_per_second: 1.28 +/- 0.00 [1.27, 1.29]
        metrics:
          batches_per_second_max: 1.2877466006445022
          batches_per_second_mean: 1.2781313228287754
          batches_per_second_min: 1.2695477722776773
          batches_per_second_std: 0.003788297527674491
          seconds_per_batch_max: 0.787682056427002
          seconds_per_batch_mean: 0.7823990893363952
          seconds_per_batch_min: 0.77655029296875
          seconds_per_batch_std: 0.0023184812868317743
      on_device_inference:
        human_readable:
          batch_latency: 1.181 s +/- 2.058 ms [1.176 s, 1.188 s]
          batches_per_second: 0.85 +/- 0.00 [0.84, 0.85]
        metrics:
          batches_per_second_max: 0.8505105836687442
          batches_per_second_mean: 0.8467905867625143
          batches_per_second_min: 0.8414192192781371
          batches_per_second_std: 0.0014734022869013269
          seconds_per_batch_max: 1.1884682178497314
          seconds_per_batch_mean: 1.1809330987930298
          seconds_per_batch_min: 1.1757643222808838
          seconds_per_batch_std: 0.0020575987580710928
      total:
        human_readable:
          batch_latency: 1.974 s +/- 3.571 ms [1.965 s, 1.987 s]
          batches_per_second: 0.51 +/- 0.00 [0.50, 0.51]
        metrics:
          batches_per_second_max: 0.5088496294547502
          batches_per_second_mean: 0.5066110809605965
          batches_per_second_min: 0.50331468819971
          batches_per_second_std: 0.0009161030352739605
          seconds_per_batch_max: 1.9868285655975342
          seconds_per_batch_mean: 1.9739072227478027
          seconds_per_batch_min: 1.965217113494873
          seconds_per_batch_std: 0.003571481554974931

INFO:benchmark:==== Benchmarking X3DLearner (m) ====
INFO:benchmark:== Benchmarking learner.infer ==
ERROR:pytorch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:pytorch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
Timing results (batch_size=16):
  cpu_to_gpu:
    human_readable:
      batch_latency: "10.575 ms +/- 307.665 \xB5s [9.647 ms, 11.338 ms]"
      batches_per_second: 94.64 +/- 2.77 [88.20, 103.66]
    metrics:
      batches_per_second_max: 103.65519968366944
      batches_per_second_mean: 94.64294212093345
      batches_per_second_min: 88.19715703591555
      batches_per_second_std: 2.773012920635653
      seconds_per_batch_max: 0.011338233947753906
      seconds_per_batch_mean: 0.010575034618377686
      seconds_per_batch_min: 0.009647369384765625
      seconds_per_batch_std: 0.00030766511771005475
  gpu_to_cpu:
    human_readable:
      batch_latency: 782.399 ms +/- 2.318 ms [776.550 ms, 787.682 ms]
      batches_per_second: 1.28 +/- 0.00 [1.27, 1.29]
    metrics:
      batches_per_second_max: 1.2877466006445022
      batches_per_second_mean: 1.2781313228287754
      batches_per_second_min: 1.2695477722776773
      batches_per_second_std: 0.003788297527674491
      seconds_per_batch_max: 0.787682056427002
      seconds_per_batch_mean: 0.7823990893363952
      seconds_per_batch_min: 0.77655029296875
      seconds_per_batch_std: 0.0023184812868317743
  on_device_inference:
    human_readable:
      batch_latency: 1.181 s +/- 2.058 ms [1.176 s, 1.188 s]
      batches_per_second: 0.85 +/- 0.00 [0.84, 0.85]
    metrics:
      batches_per_second_max: 0.8505105836687442
      batches_per_second_mean: 0.8467905867625143
      batches_per_second_min: 0.8414192192781371
      batches_per_second_std: 0.0014734022869013269
      seconds_per_batch_max: 1.1884682178497314
      seconds_per_batch_mean: 1.1809330987930298
      seconds_per_batch_min: 1.1757643222808838
      seconds_per_batch_std: 0.0020575987580710928
  total:
    human_readable:
      batch_latency: 1.974 s +/- 3.571 ms [1.965 s, 1.987 s]
      batches_per_second: 0.51 +/- 0.00 [0.50, 0.51]
    metrics:
      batches_per_second_max: 0.5088496294547502
      batches_per_second_mean: 0.5066110809605965
      batches_per_second_min: 0.50331468819971
      batches_per_second_std: 0.0009161030352739605
      seconds_per_batch_max: 1.9868285655975342
      seconds_per_batch_mean: 1.9739072227478027
      seconds_per_batch_min: 1.965217113494873
      seconds_per_batch_std: 0.003571481554974931

Inference energy: 53.84570485862095 J (1.4957140238505818e-05 kWh)
Energy (batch_size=16):
  joules: 53.84570485862095
  kWh: 1.4957140238505818e-05

Machine info:
  cpu:
    architecture: aarch64
    cores:
      physical: 8
      total: 8
    frequency: 2.27 GHz
    model: ARMv8 Processor rev 0 (v8l)
  gpus: null
  memory:
    available: 11.24 GB
    total: 31.17 GB
    used: 21.81 GB
  system:
    node: xavier
    release: 4.9.140-tegra
    system: Linux

Model device: cuda:0
Allocated GPU memory prior to inference: 46336512 (44.19 MB)
Allocated GPU memory after to inference: 46336512 (44.19 MB)
Max allocated GPU memory during inference: 7583963136 (7.06 GB)
Warming up with batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=8:  10%|█         | 1/10 [00:00<00:02,  4.48it/s]Warming up with batch_size=8:  20%|██        | 2/10 [00:00<00:01,  4.51it/s]Warming up with batch_size=8:  30%|███       | 3/10 [00:00<00:01,  4.50it/s]Warming up with batch_size=8:  40%|████      | 4/10 [00:00<00:01,  4.49it/s]Warming up with batch_size=8:  50%|█████     | 5/10 [00:01<00:01,  4.49it/s]Warming up with batch_size=8:  60%|██████    | 6/10 [00:01<00:00,  4.48it/s]Warming up with batch_size=8:  70%|███████   | 7/10 [00:01<00:00,  4.45it/s]Warming up with batch_size=8:  80%|████████  | 8/10 [00:01<00:00,  4.47it/s]Warming up with batch_size=8:  90%|█████████ | 9/10 [00:02<00:00,  4.46it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:02<00:00,  4.47it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:02<00:00,  4.47it/s]
Measuring inference with batch_size=8:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=8:   1%|          | 1/100 [00:00<00:21,  4.51it/s]Measuring inference with batch_size=8:   2%|▏         | 2/100 [00:00<00:21,  4.50it/s]Measuring inference with batch_size=8:   3%|▎         | 3/100 [00:00<00:21,  4.51it/s]Measuring inference with batch_size=8:   4%|▍         | 4/100 [00:00<00:21,  4.50it/s]Measuring inference with batch_size=8:   5%|▌         | 5/100 [00:01<00:21,  4.50it/s]Measuring inference with batch_size=8:   6%|▌         | 6/100 [00:01<00:20,  4.51it/s]Measuring inference with batch_size=8:   7%|▋         | 7/100 [00:01<00:20,  4.50it/s]Measuring inference with batch_size=8:   8%|▊         | 8/100 [00:01<00:20,  4.52it/s]Measuring inference with batch_size=8:   9%|▉         | 9/100 [00:01<00:20,  4.52it/s]Measuring inference with batch_size=8:  10%|█         | 10/100 [00:02<00:19,  4.52it/s]Measuring inference with batch_size=8:  11%|█         | 11/100 [00:02<00:19,  4.50it/s]Measuring inference with batch_size=8:  12%|█▏        | 12/100 [00:02<00:19,  4.50it/s]Measuring inference with batch_size=8:  13%|█▎        | 13/100 [00:02<00:19,  4.51it/s]Measuring inference with batch_size=8:  14%|█▍        | 14/100 [00:03<00:19,  4.51it/s]Measuring inference with batch_size=8:  15%|█▌        | 15/100 [00:03<00:18,  4.48it/s]Measuring inference with batch_size=8:  16%|█▌        | 16/100 [00:03<00:18,  4.48it/s]Measuring inference with batch_size=8:  17%|█▋        | 17/100 [00:03<00:18,  4.48it/s]Measuring inference with batch_size=8:  18%|█▊        | 18/100 [00:04<00:18,  4.49it/s]Measuring inference with batch_size=8:  19%|█▉        | 19/100 [00:04<00:17,  4.50it/s]Measuring inference with batch_size=8:  20%|██        | 20/100 [00:04<00:17,  4.50it/s]Measuring inference with batch_size=8:  21%|██        | 21/100 [00:04<00:17,  4.49it/s]Measuring inference with batch_size=8:  22%|██▏       | 22/100 [00:04<00:17,  4.50it/s]Measuring inference with batch_size=8:  23%|██▎       | 23/100 [00:05<00:17,  4.50it/s]Measuring inference with batch_size=8:  24%|██▍       | 24/100 [00:05<00:16,  4.51it/s]Measuring inference with batch_size=8:  25%|██▌       | 25/100 [00:05<00:16,  4.50it/s]Measuring inference with batch_size=8:  26%|██▌       | 26/100 [00:05<00:16,  4.50it/s]Measuring inference with batch_size=8:  27%|██▋       | 27/100 [00:06<00:16,  4.48it/s]Measuring inference with batch_size=8:  28%|██▊       | 28/100 [00:06<00:16,  4.49it/s]Measuring inference with batch_size=8:  29%|██▉       | 29/100 [00:06<00:15,  4.49it/s]Measuring inference with batch_size=8:  30%|███       | 30/100 [00:06<00:15,  4.50it/s]Measuring inference with batch_size=8:  31%|███       | 31/100 [00:06<00:15,  4.50it/s]Measuring inference with batch_size=8:  32%|███▏      | 32/100 [00:07<00:15,  4.50it/s]Measuring inference with batch_size=8:  33%|███▎      | 33/100 [00:07<00:14,  4.51it/s]Measuring inference with batch_size=8:  34%|███▍      | 34/100 [00:07<00:14,  4.51it/s]Measuring inference with batch_size=8:  35%|███▌      | 35/100 [00:07<00:14,  4.51it/s]Measuring inference with batch_size=8:  36%|███▌      | 36/100 [00:08<00:14,  4.49it/s]Measuring inference with batch_size=8:  37%|███▋      | 37/100 [00:08<00:14,  4.48it/s]Measuring inference with batch_size=8:  38%|███▊      | 38/100 [00:08<00:13,  4.50it/s]Measuring inference with batch_size=8:  39%|███▉      | 39/100 [00:08<00:13,  4.49it/s]Measuring inference with batch_size=8:  40%|████      | 40/100 [00:08<00:13,  4.50it/s]Measuring inference with batch_size=8:  41%|████      | 41/100 [00:09<00:13,  4.49it/s]Measuring inference with batch_size=8:  42%|████▏     | 42/100 [00:09<00:12,  4.47it/s]Measuring inference with batch_size=8:  43%|████▎     | 43/100 [00:09<00:12,  4.48it/s]Measuring inference with batch_size=8:  44%|████▍     | 44/100 [00:09<00:12,  4.49it/s]Measuring inference with batch_size=8:  45%|████▌     | 45/100 [00:10<00:12,  4.46it/s]Measuring inference with batch_size=8:  46%|████▌     | 46/100 [00:10<00:12,  4.47it/s]Measuring inference with batch_size=8:  47%|████▋     | 47/100 [00:10<00:11,  4.48it/s]Measuring inference with batch_size=8:  48%|████▊     | 48/100 [00:10<00:11,  4.49it/s]Measuring inference with batch_size=8:  49%|████▉     | 49/100 [00:10<00:11,  4.49it/s]Measuring inference with batch_size=8:  50%|█████     | 50/100 [00:11<00:11,  4.47it/s]Measuring inference with batch_size=8:  51%|█████     | 51/100 [00:11<00:10,  4.48it/s]Measuring inference with batch_size=8:  52%|█████▏    | 52/100 [00:11<00:10,  4.47it/s]Measuring inference with batch_size=8:  53%|█████▎    | 53/100 [00:11<00:10,  4.49it/s]Measuring inference with batch_size=8:  54%|█████▍    | 54/100 [00:12<00:10,  4.47it/s]Measuring inference with batch_size=8:  55%|█████▌    | 55/100 [00:12<00:10,  4.47it/s]Measuring inference with batch_size=8:  56%|█████▌    | 56/100 [00:12<00:09,  4.49it/s]Measuring inference with batch_size=8:  57%|█████▋    | 57/100 [00:12<00:09,  4.50it/s]Measuring inference with batch_size=8:  58%|█████▊    | 58/100 [00:12<00:09,  4.50it/s]Measuring inference with batch_size=8:  59%|█████▉    | 59/100 [00:13<00:09,  4.49it/s]Measuring inference with batch_size=8:  60%|██████    | 60/100 [00:13<00:08,  4.49it/s]Measuring inference with batch_size=8:  61%|██████    | 61/100 [00:13<00:08,  4.48it/s]Measuring inference with batch_size=8:  62%|██████▏   | 62/100 [00:13<00:08,  4.49it/s]Measuring inference with batch_size=8:  63%|██████▎   | 63/100 [00:14<00:08,  4.50it/s]Measuring inference with batch_size=8:  64%|██████▍   | 64/100 [00:14<00:08,  4.48it/s]Measuring inference with batch_size=8:  65%|██████▌   | 65/100 [00:14<00:07,  4.47it/s]Measuring inference with batch_size=8:  66%|██████▌   | 66/100 [00:14<00:07,  4.47it/s]Measuring inference with batch_size=8:  67%|██████▋   | 67/100 [00:14<00:07,  4.45it/s]Measuring inference with batch_size=8:  68%|██████▊   | 68/100 [00:15<00:07,  4.46it/s]Measuring inference with batch_size=8:  69%|██████▉   | 69/100 [00:15<00:06,  4.47it/s]Measuring inference with batch_size=8:  70%|███████   | 70/100 [00:15<00:06,  4.48it/s]Measuring inference with batch_size=8:  71%|███████   | 71/100 [00:15<00:06,  4.46it/s]Measuring inference with batch_size=8:  72%|███████▏  | 72/100 [00:16<00:06,  4.48it/s]Measuring inference with batch_size=8:  73%|███████▎  | 73/100 [00:16<00:06,  4.48it/s]Measuring inference with batch_size=8:  74%|███████▍  | 74/100 [00:16<00:05,  4.49it/s]Measuring inference with batch_size=8:  75%|███████▌  | 75/100 [00:16<00:05,  4.48it/s]Measuring inference with batch_size=8:  76%|███████▌  | 76/100 [00:16<00:05,  4.46it/s]Measuring inference with batch_size=8:  77%|███████▋  | 77/100 [00:17<00:05,  4.45it/s]Measuring inference with batch_size=8:  78%|███████▊  | 78/100 [00:17<00:04,  4.46it/s]Measuring inference with batch_size=8:  79%|███████▉  | 79/100 [00:17<00:04,  4.46it/s]Measuring inference with batch_size=8:  80%|████████  | 80/100 [00:17<00:04,  4.47it/s]Measuring inference with batch_size=8:  81%|████████  | 81/100 [00:18<00:04,  4.47it/s]Measuring inference with batch_size=8:  82%|████████▏ | 82/100 [00:18<00:04,  4.47it/s]Measuring inference with batch_size=8:  83%|████████▎ | 83/100 [00:18<00:03,  4.49it/s]Measuring inference with batch_size=8:  84%|████████▍ | 84/100 [00:18<00:03,  4.48it/s]Measuring inference with batch_size=8:  85%|████████▌ | 85/100 [00:18<00:03,  4.46it/s]Measuring inference with batch_size=8:  86%|████████▌ | 86/100 [00:19<00:03,  4.48it/s]Measuring inference with batch_size=8:  87%|████████▋ | 87/100 [00:19<00:02,  4.46it/s]Measuring inference with batch_size=8:  88%|████████▊ | 88/100 [00:19<00:02,  4.45it/s]Measuring inference with batch_size=8:  89%|████████▉ | 89/100 [00:19<00:02,  4.45it/s]Measuring inference with batch_size=8:  90%|█████████ | 90/100 [00:20<00:02,  4.45it/s]Measuring inference with batch_size=8:  91%|█████████ | 91/100 [00:20<00:02,  4.45it/s]Measuring inference with batch_size=8:  92%|█████████▏| 92/100 [00:20<00:01,  4.45it/s]Measuring inference with batch_size=8:  93%|█████████▎| 93/100 [00:20<00:01,  4.45it/s]Measuring inference with batch_size=8:  94%|█████████▍| 94/100 [00:20<00:01,  4.47it/s]Measuring inference with batch_size=8:  95%|█████████▌| 95/100 [00:21<00:01,  4.48it/s]Measuring inference with batch_size=8:  96%|█████████▌| 96/100 [00:21<00:00,  4.49it/s]Measuring inference with batch_size=8:  97%|█████████▋| 97/100 [00:21<00:00,  4.49it/s]Measuring inference with batch_size=8:  98%|█████████▊| 98/100 [00:21<00:00,  4.50it/s]Measuring inference with batch_size=8:  99%|█████████▉| 99/100 [00:22<00:00,  4.49it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [00:22<00:00,  4.47it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [00:22<00:00,  4.48it/s]
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "9.413 \xB5s +/- 7.610 \xB5s [5.484 \xB5s, 82.493 \xB5s]"
      batches_per_second: 120.68 K +/- 30.79 K [12.12 K, 182.36 K]
    metrics:
      batches_per_second_max: 182361.04347826086
      batches_per_second_mean: 120676.46028690696
      batches_per_second_min: 12122.265895953757
      batches_per_second_std: 30794.52315958648
      seconds_per_batch_max: 8.249282836914062e-05
      seconds_per_batch_mean: 9.412765502929687e-06
      seconds_per_batch_min: 5.4836273193359375e-06
      seconds_per_batch_std: 7.610183609412239e-06
  gpu_to_cpu:
    human_readable:
      batch_latency: "425.181 \xB5s +/- 113.317 \xB5s [244.856 \xB5s, 814.915 \xB5\
        s]"
      batches_per_second: 2.50 K +/- 601.67 [1.23 K, 4.08 K]
    metrics:
      batches_per_second_max: 4084.035053554041
      batches_per_second_mean: 2503.7984552791145
      batches_per_second_min: 1227.1222937390287
      batches_per_second_std: 601.666281045037
      seconds_per_batch_max: 0.0008149147033691406
      seconds_per_batch_mean: 0.00042518138885498045
      seconds_per_batch_min: 0.0002448558807373047
      seconds_per_batch_std: 0.00011331670744417096
  on_device_inference:
    human_readable:
      batch_latency: 221.587 ms +/- 1.723 ms [218.705 ms, 225.623 ms]
      batches_per_second: 4.51 +/- 0.03 [4.43, 4.57]
    metrics:
      batches_per_second_max: 4.5723604817091585
      batches_per_second_mean: 4.513176211906698
      batches_per_second_min: 4.432164999001409
      batches_per_second_std: 0.03491296530153913
      seconds_per_batch_max: 0.22562336921691895
      seconds_per_batch_mean: 0.22158677101135255
      seconds_per_batch_min: 0.218705415725708
      seconds_per_batch_std: 0.0017227250198446357
  total:
    human_readable:
      batch_latency: 222.021 ms +/- 1.743 ms [218.982 ms, 226.100 ms]
      batches_per_second: 4.50 +/- 0.04 [4.42, 4.57]
    metrics:
      batches_per_second_max: 4.566580799975612
      batches_per_second_mean: 4.504347177179691
      batches_per_second_min: 4.422827050892568
      batches_per_second_std: 0.03518333976892499
      seconds_per_batch_max: 0.22609972953796387
      seconds_per_batch_mean: 0.22202136516571044
      seconds_per_batch_min: 0.21898221969604492
      seconds_per_batch_std: 0.0017426191751606648

Inference energy: 31.831809226338066 J (8.842169229538351e-06 kWh)
Energy (batch_size=1):
  joules: 31.831809226338066
  kWh: 8.842169229538351e-06

Warming up with batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=8:  10%|█         | 1/10 [00:01<00:11,  1.26s/it]Warming up with batch_size=8:  20%|██        | 2/10 [00:02<00:10,  1.26s/it]Warming up with batch_size=8:  30%|███       | 3/10 [00:03<00:08,  1.26s/it]Warming up with batch_size=8:  40%|████      | 4/10 [00:05<00:07,  1.25s/it]Warming up with batch_size=8:  50%|█████     | 5/10 [00:06<00:06,  1.25s/it]Warming up with batch_size=8:  60%|██████    | 6/10 [00:07<00:05,  1.25s/it]Warming up with batch_size=8:  70%|███████   | 7/10 [00:08<00:03,  1.25s/it]Warming up with batch_size=8:  80%|████████  | 8/10 [00:10<00:02,  1.25s/it]Warming up with batch_size=8:  90%|█████████ | 9/10 [00:11<00:01,  1.25s/it]Warming up with batch_size=8: 100%|██████████| 10/10 [00:12<00:00,  1.25s/it]Warming up with batch_size=8: 100%|██████████| 10/10 [00:12<00:00,  1.25s/it]
Measuring inference with batch_size=8:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=8:   1%|          | 1/100 [00:01<02:03,  1.25s/it]Measuring inference with batch_size=8:   2%|▏         | 2/100 [00:02<02:02,  1.25s/it]Measuring inference with batch_size=8:   3%|▎         | 3/100 [00:03<02:01,  1.26s/it]Measuring inference with batch_size=8:   4%|▍         | 4/100 [00:05<02:00,  1.25s/it]Measuring inference with batch_size=8:   5%|▌         | 5/100 [00:06<01:59,  1.25s/it]Measuring inference with batch_size=8:   6%|▌         | 6/100 [00:07<01:57,  1.25s/it]Measuring inference with batch_size=8:   7%|▋         | 7/100 [00:08<01:56,  1.25s/it]Measuring inference with batch_size=8:   8%|▊         | 8/100 [00:10<01:54,  1.25s/it]Measuring inference with batch_size=8:   9%|▉         | 9/100 [00:11<01:53,  1.25s/it]Measuring inference with batch_size=8:  10%|█         | 10/100 [00:12<01:52,  1.25s/it]Measuring inference with batch_size=8:  11%|█         | 11/100 [00:13<01:51,  1.25s/it]Measuring inference with batch_size=8:  12%|█▏        | 12/100 [00:15<01:49,  1.25s/it]Measuring inference with batch_size=8:  13%|█▎        | 13/100 [00:16<01:48,  1.25s/it]Measuring inference with batch_size=8:  14%|█▍        | 14/100 [00:17<01:47,  1.25s/it]Measuring inference with batch_size=8:  15%|█▌        | 15/100 [00:18<01:46,  1.25s/it]Measuring inference with batch_size=8:  16%|█▌        | 16/100 [00:20<01:46,  1.27s/it]Measuring inference with batch_size=8:  17%|█▋        | 17/100 [00:21<01:47,  1.30s/it]Measuring inference with batch_size=8:  18%|█▊        | 18/100 [00:22<01:51,  1.36s/it]Measuring inference with batch_size=8:  19%|█▉        | 19/100 [00:24<01:48,  1.34s/it]Measuring inference with batch_size=8:  20%|██        | 20/100 [00:25<01:46,  1.34s/it]Measuring inference with batch_size=8:  21%|██        | 21/100 [00:26<01:47,  1.36s/it]Measuring inference with batch_size=8:  22%|██▏       | 22/100 [00:28<01:47,  1.38s/it]Measuring inference with batch_size=8:  23%|██▎       | 23/100 [00:29<01:48,  1.40s/it]Measuring inference with batch_size=8:  24%|██▍       | 24/100 [00:31<01:45,  1.38s/it]Measuring inference with batch_size=8:  25%|██▌       | 25/100 [00:32<01:41,  1.35s/it]Measuring inference with batch_size=8:  26%|██▌       | 26/100 [00:33<01:38,  1.33s/it]Measuring inference with batch_size=8:  27%|██▋       | 27/100 [00:35<01:35,  1.31s/it]Measuring inference with batch_size=8:  28%|██▊       | 28/100 [00:36<01:34,  1.31s/it]Measuring inference with batch_size=8:  29%|██▉       | 29/100 [00:37<01:32,  1.30s/it]Measuring inference with batch_size=8:  30%|███       | 30/100 [00:38<01:30,  1.30s/it]Measuring inference with batch_size=8:  31%|███       | 31/100 [00:40<01:30,  1.31s/it]Measuring inference with batch_size=8:  32%|███▏      | 32/100 [00:41<01:32,  1.36s/it]Measuring inference with batch_size=8:  33%|███▎      | 33/100 [00:43<01:32,  1.37s/it]Measuring inference with batch_size=8:  34%|███▍      | 34/100 [00:44<01:31,  1.39s/it]Measuring inference with batch_size=8:  35%|███▌      | 35/100 [00:45<01:30,  1.39s/it]Measuring inference with batch_size=8:  36%|███▌      | 36/100 [00:47<01:30,  1.42s/it]Measuring inference with batch_size=8:  37%|███▋      | 37/100 [00:48<01:26,  1.38s/it]Measuring inference with batch_size=8:  38%|███▊      | 38/100 [00:49<01:23,  1.35s/it]Measuring inference with batch_size=8:  39%|███▉      | 39/100 [00:51<01:20,  1.33s/it]Measuring inference with batch_size=8:  40%|████      | 40/100 [00:52<01:18,  1.31s/it]Measuring inference with batch_size=8:  41%|████      | 41/100 [00:54<01:25,  1.46s/it]Measuring inference with batch_size=8:  42%|████▏     | 42/100 [00:55<01:23,  1.44s/it]Measuring inference with batch_size=8:  43%|████▎     | 43/100 [00:56<01:18,  1.39s/it]Measuring inference with batch_size=8:  44%|████▍     | 44/100 [00:58<01:16,  1.36s/it]Measuring inference with batch_size=8:  45%|████▌     | 45/100 [00:59<01:13,  1.33s/it]Measuring inference with batch_size=8:  46%|████▌     | 46/100 [01:00<01:11,  1.32s/it]Measuring inference with batch_size=8:  47%|████▋     | 47/100 [01:02<01:09,  1.31s/it]Measuring inference with batch_size=8:  48%|████▊     | 48/100 [01:03<01:07,  1.30s/it]Measuring inference with batch_size=8:  49%|████▉     | 49/100 [01:04<01:05,  1.29s/it]Measuring inference with batch_size=8:  50%|█████     | 50/100 [01:05<01:03,  1.28s/it]Measuring inference with batch_size=8:  51%|█████     | 51/100 [01:07<01:02,  1.27s/it]Measuring inference with batch_size=8:  52%|█████▏    | 52/100 [01:08<01:00,  1.27s/it]Measuring inference with batch_size=8:  53%|█████▎    | 53/100 [01:09<00:59,  1.27s/it]Measuring inference with batch_size=8:  54%|█████▍    | 54/100 [01:10<00:58,  1.26s/it]Measuring inference with batch_size=8:  55%|█████▌    | 55/100 [01:12<00:56,  1.26s/it]Measuring inference with batch_size=8:  56%|█████▌    | 56/100 [01:13<00:55,  1.26s/it]Measuring inference with batch_size=8:  57%|█████▋    | 57/100 [01:14<00:54,  1.26s/it]Measuring inference with batch_size=8:  58%|█████▊    | 58/100 [01:16<00:52,  1.26s/it]Measuring inference with batch_size=8:  59%|█████▉    | 59/100 [01:17<00:52,  1.27s/it]Measuring inference with batch_size=8:  60%|██████    | 60/100 [01:18<00:51,  1.28s/it]Measuring inference with batch_size=8:  61%|██████    | 61/100 [01:19<00:49,  1.28s/it]Measuring inference with batch_size=8:  62%|██████▏   | 62/100 [01:21<00:48,  1.27s/it]Measuring inference with batch_size=8:  63%|██████▎   | 63/100 [01:22<00:46,  1.27s/it]Measuring inference with batch_size=8:  64%|██████▍   | 64/100 [01:23<00:45,  1.27s/it]Measuring inference with batch_size=8:  65%|██████▌   | 65/100 [01:24<00:44,  1.27s/it]Measuring inference with batch_size=8:  66%|██████▌   | 66/100 [01:26<00:43,  1.27s/it]Measuring inference with batch_size=8:  67%|██████▋   | 67/100 [01:27<00:41,  1.27s/it]Measuring inference with batch_size=8:  68%|██████▊   | 68/100 [01:28<00:40,  1.27s/it]Measuring inference with batch_size=8:  69%|██████▉   | 69/100 [01:30<00:39,  1.28s/it]Measuring inference with batch_size=8:  70%|███████   | 70/100 [01:31<00:38,  1.30s/it]Measuring inference with batch_size=8:  71%|███████   | 71/100 [01:32<00:37,  1.29s/it]Measuring inference with batch_size=8:  72%|███████▏  | 72/100 [01:33<00:35,  1.29s/it]Measuring inference with batch_size=8:  73%|███████▎  | 73/100 [01:35<00:34,  1.29s/it]Measuring inference with batch_size=8:  74%|███████▍  | 74/100 [01:36<00:33,  1.29s/it]Measuring inference with batch_size=8:  75%|███████▌  | 75/100 [01:37<00:32,  1.30s/it]Measuring inference with batch_size=8:  76%|███████▌  | 76/100 [01:39<00:31,  1.29s/it]Measuring inference with batch_size=8:  77%|███████▋  | 77/100 [01:40<00:29,  1.29s/it]Measuring inference with batch_size=8:  78%|███████▊  | 78/100 [01:41<00:28,  1.28s/it]Measuring inference with batch_size=8:  79%|███████▉  | 79/100 [01:42<00:26,  1.28s/it]Measuring inference with batch_size=8:  80%|████████  | 80/100 [01:44<00:26,  1.32s/it]Measuring inference with batch_size=8:  81%|████████  | 81/100 [01:45<00:25,  1.33s/it]Measuring inference with batch_size=8:  82%|████████▏ | 82/100 [01:46<00:23,  1.31s/it]Measuring inference with batch_size=8:  83%|████████▎ | 83/100 [01:48<00:22,  1.30s/it]Measuring inference with batch_size=8:  84%|████████▍ | 84/100 [01:49<00:20,  1.29s/it]Measuring inference with batch_size=8:  85%|████████▌ | 85/100 [01:50<00:19,  1.29s/it]Measuring inference with batch_size=8:  86%|████████▌ | 86/100 [01:52<00:17,  1.28s/it]Measuring inference with batch_size=8:  87%|████████▋ | 87/100 [01:53<00:16,  1.27s/it]Measuring inference with batch_size=8:  88%|████████▊ | 88/100 [01:54<00:15,  1.27s/it]Measuring inference with batch_size=8:  89%|████████▉ | 89/100 [01:55<00:14,  1.27s/it]Measuring inference with batch_size=8:  90%|█████████ | 90/100 [01:57<00:12,  1.27s/it]Measuring inference with batch_size=8:  91%|█████████ | 91/100 [01:58<00:11,  1.27s/it]Measuring inference with batch_size=8:  92%|█████████▏| 92/100 [01:59<00:10,  1.27s/it]Measuring inference with batch_size=8:  93%|█████████▎| 93/100 [02:00<00:08,  1.27s/it]Measuring inference with batch_size=8:  94%|█████████▍| 94/100 [02:02<00:07,  1.27s/it]Measuring inference with batch_size=8:  95%|█████████▌| 95/100 [02:03<00:06,  1.29s/it]Measuring inference with batch_size=8:  96%|█████████▌| 96/100 [02:04<00:05,  1.29s/it]Measuring inference with batch_size=8:  97%|█████████▋| 97/100 [02:06<00:03,  1.27s/it]Measuring inference with batch_size=8:  98%|█████████▊| 98/100 [02:07<00:02,  1.27s/it]Measuring inference with batch_size=8:  99%|█████████▉| 99/100 [02:08<00:01,  1.27s/it]Measuring inference with batch_size=8: 100%|██████████| 100/100 [02:09<00:00,  1.26s/it]Measuring inference with batch_size=8: 100%|██████████| 100/100 [02:09<00:00,  1.30s/it]
INFO:benchmark:learner.infer:
  device: cuda
  energy:
    batch_size_1:
      joules: 31.831809226338066
      kWh: 8.842169229538351e-06
    batch_size_8:
      joules: 34.65610598104795
      kWh: 9.626696105846653e-06
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 11.24 GB
      total: 31.17 GB
      used: 21.81 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  max_inference_memory: 7583963136
  post_inference_memory: 46336512
  pre_inference_memory: 46336512
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "9.413 \xB5s +/- 7.610 \xB5s [5.484 \xB5s, 82.493 \xB5s]"
          batches_per_second: 120.68 K +/- 30.79 K [12.12 K, 182.36 K]
        metrics:
          batches_per_second_max: 182361.04347826086
          batches_per_second_mean: 120676.46028690696
          batches_per_second_min: 12122.265895953757
          batches_per_second_std: 30794.52315958648
          seconds_per_batch_max: 8.249282836914062e-05
          seconds_per_batch_mean: 9.412765502929687e-06
          seconds_per_batch_min: 5.4836273193359375e-06
          seconds_per_batch_std: 7.610183609412239e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: "425.181 \xB5s +/- 113.317 \xB5s [244.856 \xB5s, 814.915\
            \ \xB5s]"
          batches_per_second: 2.50 K +/- 601.67 [1.23 K, 4.08 K]
        metrics:
          batches_per_second_max: 4084.035053554041
          batches_per_second_mean: 2503.7984552791145
          batches_per_second_min: 1227.1222937390287
          batches_per_second_std: 601.666281045037
          seconds_per_batch_max: 0.0008149147033691406
          seconds_per_batch_mean: 0.00042518138885498045
          seconds_per_batch_min: 0.0002448558807373047
          seconds_per_batch_std: 0.00011331670744417096
      on_device_inference:
        human_readable:
          batch_latency: 221.587 ms +/- 1.723 ms [218.705 ms, 225.623 ms]
          batches_per_second: 4.51 +/- 0.03 [4.43, 4.57]
        metrics:
          batches_per_second_max: 4.5723604817091585
          batches_per_second_mean: 4.513176211906698
          batches_per_second_min: 4.432164999001409
          batches_per_second_std: 0.03491296530153913
          seconds_per_batch_max: 0.22562336921691895
          seconds_per_batch_mean: 0.22158677101135255
          seconds_per_batch_min: 0.218705415725708
          seconds_per_batch_std: 0.0017227250198446357
      total:
        human_readable:
          batch_latency: 222.021 ms +/- 1.743 ms [218.982 ms, 226.100 ms]
          batches_per_second: 4.50 +/- 0.04 [4.42, 4.57]
        metrics:
          batches_per_second_max: 4.566580799975612
          batches_per_second_mean: 4.504347177179691
          batches_per_second_min: 4.422827050892568
          batches_per_second_std: 0.03518333976892499
          seconds_per_batch_max: 0.22609972953796387
          seconds_per_batch_mean: 0.22202136516571044
          seconds_per_batch_min: 0.21898221969604492
          seconds_per_batch_std: 0.0017426191751606648
    batch_size_8:
      cpu_to_gpu:
        human_readable:
          batch_latency: "13.182 \xB5s +/- 7.958 \xB5s [6.199 \xB5s, 55.790 \xB5s]"
          batches_per_second: 89.11 K +/- 26.38 K [17.92 K, 161.32 K]
        metrics:
          batches_per_second_max: 161319.38461538462
          batches_per_second_mean: 89106.77582915612
          batches_per_second_min: 17924.37606837607
          batches_per_second_std: 26375.10201532289
          seconds_per_batch_max: 5.5789947509765625e-05
          seconds_per_batch_mean: 1.318216323852539e-05
          seconds_per_batch_min: 6.198883056640625e-06
          seconds_per_batch_std: 7.95777467159418e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: "1.340 ms +/- 822.317 \xB5s [730.276 \xB5s, 8.231 ms]"
          batches_per_second: 837.47 +/- 209.35 [121.49, 1.37 K]
        metrics:
          batches_per_second_max: 1369.345086516487
          batches_per_second_mean: 837.4666135007114
          batches_per_second_min: 121.49303362975408
          batches_per_second_std: 209.34834383782177
          seconds_per_batch_max: 0.008230924606323242
          seconds_per_batch_mean: 0.0013399219512939453
          seconds_per_batch_min: 0.0007302761077880859
          seconds_per_batch_std: 0.0008223174798295148
      on_device_inference:
        human_readable:
          batch_latency: 1.292 s +/- 56.587 ms [1.242 s, 1.514 s]
          batches_per_second: 0.78 +/- 0.03 [0.66, 0.81]
        metrics:
          batches_per_second_max: 0.8050376271191646
          batches_per_second_mean: 0.7756198015917452
          batches_per_second_min: 0.6603949959102249
          batches_per_second_std: 0.03130606498582129
          seconds_per_batch_max: 1.5142452716827393
          seconds_per_batch_mean: 1.2915725326538086
          seconds_per_batch_min: 1.242177963256836
          seconds_per_batch_std: 0.05658674272773507
      total:
        human_readable:
          batch_latency: 1.293 s +/- 56.921 ms [1.243 s, 1.516 s]
          batches_per_second: 0.77 +/- 0.03 [0.66, 0.80]
        metrics:
          batches_per_second_max: 0.8042500595378026
          batches_per_second_mean: 0.7748207210966307
          batches_per_second_min: 0.6597292632110097
          batches_per_second_std: 0.03140974517747435
          seconds_per_batch_max: 1.5157732963562012
          seconds_per_batch_mean: 1.292925636768341
          seconds_per_batch_min: 1.243394374847412
          seconds_per_batch_std: 0.05692123982731073

INFO:benchmark:== Benchmarking model directly ==
Timing results (batch_size=8):
  cpu_to_gpu:
    human_readable:
      batch_latency: "13.182 \xB5s +/- 7.958 \xB5s [6.199 \xB5s, 55.790 \xB5s]"
      batches_per_second: 89.11 K +/- 26.38 K [17.92 K, 161.32 K]
    metrics:
      batches_per_second_max: 161319.38461538462
      batches_per_second_mean: 89106.77582915612
      batches_per_second_min: 17924.37606837607
      batches_per_second_std: 26375.10201532289
      seconds_per_batch_max: 5.5789947509765625e-05
      seconds_per_batch_mean: 1.318216323852539e-05
      seconds_per_batch_min: 6.198883056640625e-06
      seconds_per_batch_std: 7.95777467159418e-06
  gpu_to_cpu:
    human_readable:
      batch_latency: "1.340 ms +/- 822.317 \xB5s [730.276 \xB5s, 8.231 ms]"
      batches_per_second: 837.47 +/- 209.35 [121.49, 1.37 K]
    metrics:
      batches_per_second_max: 1369.345086516487
      batches_per_second_mean: 837.4666135007114
      batches_per_second_min: 121.49303362975408
      batches_per_second_std: 209.34834383782177
      seconds_per_batch_max: 0.008230924606323242
      seconds_per_batch_mean: 0.0013399219512939453
      seconds_per_batch_min: 0.0007302761077880859
      seconds_per_batch_std: 0.0008223174798295148
  on_device_inference:
    human_readable:
      batch_latency: 1.292 s +/- 56.587 ms [1.242 s, 1.514 s]
      batches_per_second: 0.78 +/- 0.03 [0.66, 0.81]
    metrics:
      batches_per_second_max: 0.8050376271191646
      batches_per_second_mean: 0.7756198015917452
      batches_per_second_min: 0.6603949959102249
      batches_per_second_std: 0.03130606498582129
      seconds_per_batch_max: 1.5142452716827393
      seconds_per_batch_mean: 1.2915725326538086
      seconds_per_batch_min: 1.242177963256836
      seconds_per_batch_std: 0.05658674272773507
  total:
    human_readable:
      batch_latency: 1.293 s +/- 56.921 ms [1.243 s, 1.516 s]
      batches_per_second: 0.77 +/- 0.03 [0.66, 0.80]
    metrics:
      batches_per_second_max: 0.8042500595378026
      batches_per_second_mean: 0.7748207210966307
      batches_per_second_min: 0.6597292632110097
      batches_per_second_std: 0.03140974517747435
      seconds_per_batch_max: 1.5157732963562012
      seconds_per_batch_mean: 1.292925636768341
      seconds_per_batch_min: 1.243394374847412
      seconds_per_batch_std: 0.05692123982731073

Inference energy: 34.65610598104795 J (9.626696105846653e-06 kWh)
Energy (batch_size=8):
  joules: 34.65610598104795
  kWh: 9.626696105846653e-06

Machine info:
  cpu:
    architecture: aarch64
    cores:
      physical: 8
      total: 8
    frequency: 2.27 GHz
    model: ARMv8 Processor rev 0 (v8l)
  gpus: null
  memory:
    available: 11.01 GB
    total: 31.17 GB
    used: 22.04 GB
  system:
    node: xavier
    release: 4.9.140-tegra
    system: Linux

Model device: cuda:0
Model parameters: 3794322 (3.79 M)
Model FLOPs: 4970008352 (4.97 G)
Allocated GPU memory prior to inference: 46336512 (44.19 MB)
Allocated GPU memory after to inference: 46336512 (44.19 MB)
Max allocated GPU memory during inference: 7583962624 (7.06 GB)
Warming up with batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=8:  10%|█         | 1/10 [00:00<00:01,  4.67it/s]Warming up with batch_size=8:  20%|██        | 2/10 [00:00<00:01,  4.24it/s]Warming up with batch_size=8:  30%|███       | 3/10 [00:00<00:01,  4.03it/s]Warming up with batch_size=8:  40%|████      | 4/10 [00:00<00:01,  3.97it/s]Warming up with batch_size=8:  50%|█████     | 5/10 [00:01<00:01,  4.03it/s]Warming up with batch_size=8:  60%|██████    | 6/10 [00:01<00:00,  4.17it/s]Warming up with batch_size=8:  70%|███████   | 7/10 [00:01<00:00,  4.24it/s]Warming up with batch_size=8:  80%|████████  | 8/10 [00:01<00:00,  4.34it/s]Warming up with batch_size=8:  90%|█████████ | 9/10 [00:02<00:00,  4.38it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:02<00:00,  4.41it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:02<00:00,  4.26it/s]
Measuring inference with batch_size=8:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=8:   1%|          | 1/100 [00:00<00:22,  4.47it/s]Measuring inference with batch_size=8:   2%|▏         | 2/100 [00:00<00:22,  4.39it/s]Measuring inference with batch_size=8:   3%|▎         | 3/100 [00:00<00:22,  4.36it/s]Measuring inference with batch_size=8:   4%|▍         | 4/100 [00:00<00:22,  4.24it/s]Measuring inference with batch_size=8:   5%|▌         | 5/100 [00:01<00:23,  4.09it/s]Measuring inference with batch_size=8:   6%|▌         | 6/100 [00:01<00:22,  4.13it/s]Measuring inference with batch_size=8:   7%|▋         | 7/100 [00:01<00:21,  4.26it/s]Measuring inference with batch_size=8:   8%|▊         | 8/100 [00:01<00:21,  4.32it/s]Measuring inference with batch_size=8:   9%|▉         | 9/100 [00:02<00:20,  4.38it/s]Measuring inference with batch_size=8:  10%|█         | 10/100 [00:02<00:20,  4.44it/s]Measuring inference with batch_size=8:  11%|█         | 11/100 [00:02<00:19,  4.48it/s]Measuring inference with batch_size=8:  12%|█▏        | 12/100 [00:02<00:19,  4.46it/s]Measuring inference with batch_size=8:  13%|█▎        | 13/100 [00:02<00:19,  4.45it/s]Measuring inference with batch_size=8:  14%|█▍        | 14/100 [00:03<00:19,  4.44it/s]Measuring inference with batch_size=8:  15%|█▌        | 15/100 [00:03<00:19,  4.45it/s]Measuring inference with batch_size=8:  16%|█▌        | 16/100 [00:03<00:18,  4.46it/s]Measuring inference with batch_size=8:  17%|█▋        | 17/100 [00:03<00:18,  4.39it/s]Measuring inference with batch_size=8:  18%|█▊        | 18/100 [00:04<00:18,  4.38it/s]Measuring inference with batch_size=8:  19%|█▉        | 19/100 [00:04<00:18,  4.29it/s]Measuring inference with batch_size=8:  20%|██        | 20/100 [00:04<00:18,  4.37it/s]Measuring inference with batch_size=8:  21%|██        | 21/100 [00:04<00:17,  4.42it/s]Measuring inference with batch_size=8:  22%|██▏       | 22/100 [00:05<00:17,  4.41it/s]Measuring inference with batch_size=8:  23%|██▎       | 23/100 [00:05<00:17,  4.48it/s]Measuring inference with batch_size=8:  24%|██▍       | 24/100 [00:05<00:16,  4.53it/s]Measuring inference with batch_size=8:  25%|██▌       | 25/100 [00:05<00:16,  4.50it/s]Measuring inference with batch_size=8:  26%|██▌       | 26/100 [00:05<00:16,  4.52it/s]Measuring inference with batch_size=8:  27%|██▋       | 27/100 [00:06<00:16,  4.52it/s]Measuring inference with batch_size=8:  28%|██▊       | 28/100 [00:06<00:15,  4.54it/s]Measuring inference with batch_size=8:  29%|██▉       | 29/100 [00:06<00:15,  4.57it/s]Measuring inference with batch_size=8:  30%|███       | 30/100 [00:06<00:15,  4.57it/s]Measuring inference with batch_size=8:  31%|███       | 31/100 [00:06<00:14,  4.61it/s]Measuring inference with batch_size=8:  32%|███▏      | 32/100 [00:07<00:14,  4.61it/s]Measuring inference with batch_size=8:  33%|███▎      | 33/100 [00:07<00:14,  4.61it/s]Measuring inference with batch_size=8:  34%|███▍      | 34/100 [00:07<00:14,  4.60it/s]Measuring inference with batch_size=8:  35%|███▌      | 35/100 [00:07<00:14,  4.62it/s]Measuring inference with batch_size=8:  36%|███▌      | 36/100 [00:08<00:13,  4.62it/s]Measuring inference with batch_size=8:  37%|███▋      | 37/100 [00:08<00:13,  4.61it/s]Measuring inference with batch_size=8:  38%|███▊      | 38/100 [00:08<00:13,  4.63it/s]Measuring inference with batch_size=8:  39%|███▉      | 39/100 [00:08<00:13,  4.59it/s]Measuring inference with batch_size=8:  40%|████      | 40/100 [00:08<00:13,  4.61it/s]Measuring inference with batch_size=8:  41%|████      | 41/100 [00:09<00:12,  4.59it/s]Measuring inference with batch_size=8:  42%|████▏     | 42/100 [00:09<00:13,  4.45it/s]Measuring inference with batch_size=8:  43%|████▎     | 43/100 [00:09<00:13,  4.36it/s]Measuring inference with batch_size=8:  44%|████▍     | 44/100 [00:09<00:13,  4.17it/s]Measuring inference with batch_size=8:  45%|████▌     | 45/100 [00:10<00:13,  4.07it/s]Measuring inference with batch_size=8:  46%|████▌     | 46/100 [00:10<00:13,  4.06it/s]Measuring inference with batch_size=8:  47%|████▋     | 47/100 [00:10<00:12,  4.22it/s]Measuring inference with batch_size=8:  48%|████▊     | 48/100 [00:10<00:12,  4.29it/s]Measuring inference with batch_size=8:  49%|████▉     | 49/100 [00:11<00:11,  4.30it/s]Measuring inference with batch_size=8:  50%|█████     | 50/100 [00:11<00:11,  4.35it/s]Measuring inference with batch_size=8:  51%|█████     | 51/100 [00:11<00:11,  4.42it/s]Measuring inference with batch_size=8:  52%|█████▏    | 52/100 [00:11<00:10,  4.47it/s]Measuring inference with batch_size=8:  53%|█████▎    | 53/100 [00:11<00:10,  4.50it/s]Measuring inference with batch_size=8:  54%|█████▍    | 54/100 [00:12<00:10,  4.49it/s]Measuring inference with batch_size=8:  55%|█████▌    | 55/100 [00:12<00:10,  4.47it/s]Measuring inference with batch_size=8:  56%|█████▌    | 56/100 [00:12<00:09,  4.43it/s]Measuring inference with batch_size=8:  57%|█████▋    | 57/100 [00:12<00:09,  4.43it/s]Measuring inference with batch_size=8:  58%|█████▊    | 58/100 [00:13<00:09,  4.34it/s]Measuring inference with batch_size=8:  59%|█████▉    | 59/100 [00:13<00:09,  4.37it/s]Measuring inference with batch_size=8:  60%|██████    | 60/100 [00:13<00:09,  4.43it/s]Measuring inference with batch_size=8:  61%|██████    | 61/100 [00:13<00:08,  4.48it/s]Measuring inference with batch_size=8:  62%|██████▏   | 62/100 [00:13<00:08,  4.52it/s]Measuring inference with batch_size=8:  63%|██████▎   | 63/100 [00:14<00:08,  4.52it/s]Measuring inference with batch_size=8:  64%|██████▍   | 64/100 [00:14<00:08,  4.43it/s]Measuring inference with batch_size=8:  65%|██████▌   | 65/100 [00:14<00:07,  4.44it/s]Measuring inference with batch_size=8:  66%|██████▌   | 66/100 [00:14<00:07,  4.36it/s]Measuring inference with batch_size=8:  67%|██████▋   | 67/100 [00:15<00:07,  4.39it/s]Measuring inference with batch_size=8:  68%|██████▊   | 68/100 [00:15<00:07,  4.47it/s]Measuring inference with batch_size=8:  69%|██████▉   | 69/100 [00:15<00:06,  4.51it/s]Measuring inference with batch_size=8:  70%|███████   | 70/100 [00:15<00:06,  4.53it/s]Measuring inference with batch_size=8:  71%|███████   | 71/100 [00:16<00:06,  4.55it/s]Measuring inference with batch_size=8:  72%|███████▏  | 72/100 [00:16<00:06,  4.52it/s]Measuring inference with batch_size=8:  73%|███████▎  | 73/100 [00:16<00:06,  4.50it/s]Measuring inference with batch_size=8:  74%|███████▍  | 74/100 [00:16<00:05,  4.47it/s]Measuring inference with batch_size=8:  75%|███████▌  | 75/100 [00:16<00:05,  4.43it/s]Measuring inference with batch_size=8:  76%|███████▌  | 76/100 [00:17<00:05,  4.44it/s]Measuring inference with batch_size=8:  77%|███████▋  | 77/100 [00:17<00:05,  4.36it/s]Measuring inference with batch_size=8:  78%|███████▊  | 78/100 [00:17<00:05,  4.31it/s]Measuring inference with batch_size=8:  79%|███████▉  | 79/100 [00:17<00:05,  4.14it/s]Measuring inference with batch_size=8:  80%|████████  | 80/100 [00:18<00:04,  4.13it/s]Measuring inference with batch_size=8:  81%|████████  | 81/100 [00:18<00:04,  4.27it/s]Measuring inference with batch_size=8:  82%|████████▏ | 82/100 [00:19<00:08,  2.02it/s]Measuring inference with batch_size=8:  83%|████████▎ | 83/100 [00:19<00:06,  2.44it/s]Measuring inference with batch_size=8:  84%|████████▍ | 84/100 [00:19<00:05,  2.81it/s]Measuring inference with batch_size=8:  85%|████████▌ | 85/100 [00:20<00:04,  3.16it/s]Measuring inference with batch_size=8:  86%|████████▌ | 86/100 [00:20<00:04,  3.41it/s]Measuring inference with batch_size=8:  87%|████████▋ | 87/100 [00:20<00:03,  3.61it/s]Measuring inference with batch_size=8:  88%|████████▊ | 88/100 [00:20<00:03,  3.85it/s]Measuring inference with batch_size=8:  89%|████████▉ | 89/100 [00:21<00:02,  3.96it/s]Measuring inference with batch_size=8:  90%|█████████ | 90/100 [00:21<00:02,  4.12it/s]Measuring inference with batch_size=8:  91%|█████████ | 91/100 [00:21<00:02,  4.27it/s]Measuring inference with batch_size=8:  92%|█████████▏| 92/100 [00:21<00:01,  4.35it/s]Measuring inference with batch_size=8:  93%|█████████▎| 93/100 [00:21<00:01,  4.18it/s]Measuring inference with batch_size=8:  94%|█████████▍| 94/100 [00:22<00:01,  4.26it/s]Measuring inference with batch_size=8:  95%|█████████▌| 95/100 [00:22<00:01,  4.32it/s]Measuring inference with batch_size=8:  96%|█████████▌| 96/100 [00:22<00:00,  4.41it/s]Measuring inference with batch_size=8:  97%|█████████▋| 97/100 [00:22<00:00,  4.45it/s]Measuring inference with batch_size=8:  98%|█████████▊| 98/100 [00:23<00:00,  4.50it/s]Measuring inference with batch_size=8:  99%|█████████▉| 99/100 [00:23<00:00,  4.28it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [00:23<00:00,  4.30it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [00:23<00:00,  4.25it/s]
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "1.648 ms +/- 269.360 \xB5s [1.316 ms, 2.473 ms]"
      batches_per_second: 620.85 +/- 86.23 [404.43, 759.84]
    metrics:
      batches_per_second_max: 759.8376811594203
      batches_per_second_mean: 620.8481715331895
      batches_per_second_min: 404.4261884099894
      batches_per_second_std: 86.22540364932433
      seconds_per_batch_max: 0.0024726390838623047
      seconds_per_batch_mean: 0.001647624969482422
      seconds_per_batch_min: 0.001316070556640625
      seconds_per_batch_std: 0.0002693599389591195
  gpu_to_cpu:
    human_readable:
      batch_latency: 8.220 ms +/- 1.585 ms [1.866 ms, 13.010 ms]
      batches_per_second: 129.24 +/- 48.81 [76.86, 535.81]
    metrics:
      batches_per_second_max: 535.807869187532
      batches_per_second_mean: 129.23768020505335
      batches_per_second_min: 76.8638029614426
      batches_per_second_std: 48.8133920012411
      seconds_per_batch_max: 0.013010025024414062
      seconds_per_batch_mean: 0.008219962120056152
      seconds_per_batch_min: 0.0018663406372070312
      seconds_per_batch_std: 0.0015845745937246403
  on_device_inference:
    human_readable:
      batch_latency: 215.887 ms +/- 11.814 ms [202.308 ms, 252.178 ms]
      batches_per_second: 4.65 +/- 0.24 [3.97, 4.94]
    metrics:
      batches_per_second_max: 4.9429539138899425
      batches_per_second_mean: 4.6450565173715175
      batches_per_second_min: 3.9654462032706417
      batches_per_second_std: 0.23794583666596733
      seconds_per_batch_max: 0.252178430557251
      seconds_per_batch_mean: 0.21588655948638916
      seconds_per_batch_min: 0.20230817794799805
      seconds_per_batch_std: 0.01181369305434546
  total:
    human_readable:
      batch_latency: 225.754 ms +/- 12.191 ms [212.189 ms, 263.896 ms]
      batches_per_second: 4.44 +/- 0.22 [3.79, 4.71]
    metrics:
      batches_per_second_max: 4.712775257138876
      batches_per_second_mean: 4.441658788348057
      batches_per_second_min: 3.789375159799035
      batches_per_second_std: 0.2237965786354358
      seconds_per_batch_max: 0.26389575004577637
      seconds_per_batch_mean: 0.22575414657592774
      seconds_per_batch_min: 0.2121891975402832
      seconds_per_batch_std: 0.012190755552306773

Inference energy: 40.28407266672452 J (1.1190020185201256e-05 kWh)
Energy (batch_size=1):
  joules: 40.28407266672452
  kWh: 1.1190020185201256e-05

Warming up with batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=8:  10%|█         | 1/10 [00:01<00:11,  1.25s/it]Warming up with batch_size=8:  20%|██        | 2/10 [00:02<00:09,  1.25s/it]Warming up with batch_size=8:  30%|███       | 3/10 [00:03<00:08,  1.28s/it]Warming up with batch_size=8:  40%|████      | 4/10 [00:05<00:07,  1.32s/it]Warming up with batch_size=8:  50%|█████     | 5/10 [00:06<00:06,  1.34s/it]Warming up with batch_size=8:  60%|██████    | 6/10 [00:07<00:05,  1.32s/it]Warming up with batch_size=8:  70%|███████   | 7/10 [00:09<00:04,  1.36s/it]Warming up with batch_size=8:  80%|████████  | 8/10 [00:10<00:02,  1.34s/it]Warming up with batch_size=8:  90%|█████████ | 9/10 [00:11<00:01,  1.34s/it]Warming up with batch_size=8: 100%|██████████| 10/10 [00:13<00:00,  1.33s/it]Warming up with batch_size=8: 100%|██████████| 10/10 [00:13<00:00,  1.33s/it]
Measuring inference with batch_size=8:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=8:   1%|          | 1/100 [00:01<02:14,  1.35s/it]Measuring inference with batch_size=8:   2%|▏         | 2/100 [00:02<02:12,  1.35s/it]Measuring inference with batch_size=8:   3%|▎         | 3/100 [00:04<02:10,  1.34s/it]Measuring inference with batch_size=8:   4%|▍         | 4/100 [00:05<02:08,  1.34s/it]Measuring inference with batch_size=8:   5%|▌         | 5/100 [00:06<02:05,  1.33s/it]Measuring inference with batch_size=8:   6%|▌         | 6/100 [00:07<02:01,  1.29s/it]Measuring inference with batch_size=8:   7%|▋         | 7/100 [00:09<01:58,  1.28s/it]Measuring inference with batch_size=8:   8%|▊         | 8/100 [00:10<01:56,  1.26s/it]Measuring inference with batch_size=8:   9%|▉         | 9/100 [00:11<01:54,  1.25s/it]Measuring inference with batch_size=8:  10%|█         | 10/100 [00:12<01:52,  1.25s/it]Measuring inference with batch_size=8:  11%|█         | 11/100 [00:14<01:50,  1.25s/it]Measuring inference with batch_size=8:  12%|█▏        | 12/100 [00:15<01:49,  1.24s/it]Measuring inference with batch_size=8:  13%|█▎        | 13/100 [00:16<01:48,  1.24s/it]Measuring inference with batch_size=8:  14%|█▍        | 14/100 [00:17<01:47,  1.25s/it]Measuring inference with batch_size=8:  15%|█▌        | 15/100 [00:19<01:50,  1.30s/it]Measuring inference with batch_size=8:  16%|█▌        | 16/100 [00:20<01:52,  1.34s/it]Measuring inference with batch_size=8:  17%|█▋        | 17/100 [00:21<01:50,  1.33s/it]Measuring inference with batch_size=8:  18%|█▊        | 18/100 [00:23<01:53,  1.38s/it]Measuring inference with batch_size=8:  19%|█▉        | 19/100 [00:24<01:51,  1.38s/it]Measuring inference with batch_size=8:  20%|██        | 20/100 [00:26<01:46,  1.33s/it]Measuring inference with batch_size=8:  21%|██        | 21/100 [00:27<01:42,  1.30s/it]Measuring inference with batch_size=8:  22%|██▏       | 22/100 [00:28<01:39,  1.28s/it]Measuring inference with batch_size=8:  23%|██▎       | 23/100 [00:29<01:36,  1.26s/it]Measuring inference with batch_size=8:  24%|██▍       | 24/100 [00:30<01:34,  1.25s/it]Measuring inference with batch_size=8:  25%|██▌       | 25/100 [00:32<01:32,  1.24s/it]Measuring inference with batch_size=8:  26%|██▌       | 26/100 [00:33<01:31,  1.23s/it]Measuring inference with batch_size=8:  27%|██▋       | 27/100 [00:34<01:29,  1.23s/it]Measuring inference with batch_size=8:  28%|██▊       | 28/100 [00:35<01:28,  1.23s/it]Measuring inference with batch_size=8:  29%|██▉       | 29/100 [00:37<01:27,  1.23s/it]Measuring inference with batch_size=8:  30%|███       | 30/100 [00:38<01:25,  1.22s/it]Measuring inference with batch_size=8:  31%|███       | 31/100 [00:39<01:24,  1.22s/it]Measuring inference with batch_size=8:  32%|███▏      | 32/100 [00:40<01:23,  1.22s/it]Measuring inference with batch_size=8:  33%|███▎      | 33/100 [00:41<01:21,  1.22s/it]Measuring inference with batch_size=8:  34%|███▍      | 34/100 [00:43<01:20,  1.22s/it]Measuring inference with batch_size=8:  35%|███▌      | 35/100 [00:44<01:19,  1.22s/it]Measuring inference with batch_size=8:  36%|███▌      | 36/100 [00:45<01:18,  1.22s/it]Measuring inference with batch_size=8:  37%|███▋      | 37/100 [00:46<01:16,  1.22s/it]Measuring inference with batch_size=8:  38%|███▊      | 38/100 [00:48<01:15,  1.22s/it]Measuring inference with batch_size=8:  39%|███▉      | 39/100 [00:49<01:14,  1.22s/it]Measuring inference with batch_size=8:  40%|████      | 40/100 [00:50<01:13,  1.22s/it]Measuring inference with batch_size=8:  41%|████      | 41/100 [00:51<01:12,  1.22s/it]Measuring inference with batch_size=8:  42%|████▏     | 42/100 [00:52<01:10,  1.22s/it]Measuring inference with batch_size=8:  43%|████▎     | 43/100 [00:54<01:09,  1.22s/it]Measuring inference with batch_size=8:  44%|████▍     | 44/100 [00:55<01:08,  1.22s/it]Measuring inference with batch_size=8:  45%|████▌     | 45/100 [00:56<01:07,  1.22s/it]Measuring inference with batch_size=8:  46%|████▌     | 46/100 [00:57<01:05,  1.22s/it]Measuring inference with batch_size=8:  47%|████▋     | 47/100 [00:59<01:04,  1.22s/it]Measuring inference with batch_size=8:  48%|████▊     | 48/100 [01:00<01:03,  1.22s/it]Measuring inference with batch_size=8:  49%|████▉     | 49/100 [01:01<01:02,  1.22s/it]Measuring inference with batch_size=8:  50%|█████     | 50/100 [01:02<01:01,  1.22s/it]Measuring inference with batch_size=8:  51%|█████     | 51/100 [01:03<00:59,  1.22s/it]Measuring inference with batch_size=8:  52%|█████▏    | 52/100 [01:05<00:58,  1.22s/it]Measuring inference with batch_size=8:  53%|█████▎    | 53/100 [01:06<00:57,  1.22s/it]Measuring inference with batch_size=8:  54%|█████▍    | 54/100 [01:07<00:56,  1.22s/it]Measuring inference with batch_size=8:  55%|█████▌    | 55/100 [01:08<00:54,  1.22s/it]Measuring inference with batch_size=8:  56%|█████▌    | 56/100 [01:10<00:53,  1.22s/it]Measuring inference with batch_size=8:  57%|█████▋    | 57/100 [01:11<00:52,  1.22s/it]Measuring inference with batch_size=8:  58%|█████▊    | 58/100 [01:12<00:51,  1.22s/it]Measuring inference with batch_size=8:  59%|█████▉    | 59/100 [01:13<00:49,  1.22s/it]Measuring inference with batch_size=8:  60%|██████    | 60/100 [01:14<00:48,  1.22s/it]Measuring inference with batch_size=8:  61%|██████    | 61/100 [01:16<00:47,  1.22s/it]Measuring inference with batch_size=8:  62%|██████▏   | 62/100 [01:17<00:46,  1.22s/it]Measuring inference with batch_size=8:  63%|██████▎   | 63/100 [01:18<00:45,  1.22s/it]Measuring inference with batch_size=8:  64%|██████▍   | 64/100 [01:19<00:43,  1.22s/it]Measuring inference with batch_size=8:  65%|██████▌   | 65/100 [01:20<00:42,  1.22s/it]Measuring inference with batch_size=8:  66%|██████▌   | 66/100 [01:22<00:41,  1.22s/it]Measuring inference with batch_size=8:  67%|██████▋   | 67/100 [01:23<00:40,  1.22s/it]Measuring inference with batch_size=8:  68%|██████▊   | 68/100 [01:24<00:39,  1.22s/it]Measuring inference with batch_size=8:  69%|██████▉   | 69/100 [01:25<00:37,  1.22s/it]Measuring inference with batch_size=8:  70%|███████   | 70/100 [01:27<00:36,  1.22s/it]Measuring inference with batch_size=8:  71%|███████   | 71/100 [01:28<00:35,  1.22s/it]Measuring inference with batch_size=8:  72%|███████▏  | 72/100 [01:29<00:34,  1.22s/it]Measuring inference with batch_size=8:  73%|███████▎  | 73/100 [01:30<00:32,  1.22s/it]Measuring inference with batch_size=8:  74%|███████▍  | 74/100 [01:31<00:31,  1.22s/it]Measuring inference with batch_size=8:  75%|███████▌  | 75/100 [01:33<00:30,  1.22s/it]Measuring inference with batch_size=8:  76%|███████▌  | 76/100 [01:34<00:29,  1.22s/it]Measuring inference with batch_size=8:  77%|███████▋  | 77/100 [01:35<00:28,  1.22s/it]Measuring inference with batch_size=8:  78%|███████▊  | 78/100 [01:36<00:26,  1.22s/it]Measuring inference with batch_size=8:  79%|███████▉  | 79/100 [01:38<00:25,  1.22s/it]Measuring inference with batch_size=8:  80%|████████  | 80/100 [01:39<00:24,  1.22s/it]Measuring inference with batch_size=8:  81%|████████  | 81/100 [01:40<00:23,  1.22s/it]Measuring inference with batch_size=8:  82%|████████▏ | 82/100 [01:41<00:21,  1.22s/it]Measuring inference with batch_size=8:  83%|████████▎ | 83/100 [01:42<00:20,  1.22s/it]Measuring inference with batch_size=8:  84%|████████▍ | 84/100 [01:44<00:19,  1.22s/it]Measuring inference with batch_size=8:  85%|████████▌ | 85/100 [01:45<00:18,  1.22s/it]Measuring inference with batch_size=8:  86%|████████▌ | 86/100 [01:46<00:17,  1.22s/it]Measuring inference with batch_size=8:  87%|████████▋ | 87/100 [01:47<00:15,  1.22s/it]Measuring inference with batch_size=8:  88%|████████▊ | 88/100 [01:49<00:14,  1.22s/it]Measuring inference with batch_size=8:  89%|████████▉ | 89/100 [01:50<00:13,  1.22s/it]Measuring inference with batch_size=8:  90%|█████████ | 90/100 [01:51<00:12,  1.22s/it]Measuring inference with batch_size=8:  91%|█████████ | 91/100 [01:52<00:10,  1.22s/it]Measuring inference with batch_size=8:  92%|█████████▏| 92/100 [01:53<00:09,  1.22s/it]Measuring inference with batch_size=8:  93%|█████████▎| 93/100 [01:55<00:08,  1.22s/it]Measuring inference with batch_size=8:  94%|█████████▍| 94/100 [01:56<00:07,  1.22s/it]Measuring inference with batch_size=8:  95%|█████████▌| 95/100 [01:57<00:06,  1.22s/it]Measuring inference with batch_size=8:  96%|█████████▌| 96/100 [01:58<00:04,  1.22s/it]Measuring inference with batch_size=8:  97%|█████████▋| 97/100 [02:00<00:03,  1.22s/it]Measuring inference with batch_size=8:  98%|█████████▊| 98/100 [02:01<00:02,  1.22s/it]Measuring inference with batch_size=8:  99%|█████████▉| 99/100 [02:02<00:01,  1.22s/it]Measuring inference with batch_size=8: 100%|██████████| 100/100 [02:03<00:00,  1.22s/it]Measuring inference with batch_size=8: 100%|██████████| 100/100 [02:03<00:00,  1.24s/it]
INFO:benchmark:learner.model.forward:
  device: cuda
  energy:
    batch_size_1:
      joules: 40.28407266672452
      kWh: 1.1190020185201256e-05
    batch_size_8:
      joules: 33.43725726960499
      kWh: 9.28812701933472e-06
  flops: 4970008352
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 11.01 GB
      total: 31.17 GB
      used: 22.04 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  max_inference_memory: 7583962624
  params: 3794322
  post_inference_memory: 46336512
  pre_inference_memory: 46336512
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "1.648 ms +/- 269.360 \xB5s [1.316 ms, 2.473 ms]"
          batches_per_second: 620.85 +/- 86.23 [404.43, 759.84]
        metrics:
          batches_per_second_max: 759.8376811594203
          batches_per_second_mean: 620.8481715331895
          batches_per_second_min: 404.4261884099894
          batches_per_second_std: 86.22540364932433
          seconds_per_batch_max: 0.0024726390838623047
          seconds_per_batch_mean: 0.001647624969482422
          seconds_per_batch_min: 0.001316070556640625
          seconds_per_batch_std: 0.0002693599389591195
      gpu_to_cpu:
        human_readable:
          batch_latency: 8.220 ms +/- 1.585 ms [1.866 ms, 13.010 ms]
          batches_per_second: 129.24 +/- 48.81 [76.86, 535.81]
        metrics:
          batches_per_second_max: 535.807869187532
          batches_per_second_mean: 129.23768020505335
          batches_per_second_min: 76.8638029614426
          batches_per_second_std: 48.8133920012411
          seconds_per_batch_max: 0.013010025024414062
          seconds_per_batch_mean: 0.008219962120056152
          seconds_per_batch_min: 0.0018663406372070312
          seconds_per_batch_std: 0.0015845745937246403
      on_device_inference:
        human_readable:
          batch_latency: 215.887 ms +/- 11.814 ms [202.308 ms, 252.178 ms]
          batches_per_second: 4.65 +/- 0.24 [3.97, 4.94]
        metrics:
          batches_per_second_max: 4.9429539138899425
          batches_per_second_mean: 4.6450565173715175
          batches_per_second_min: 3.9654462032706417
          batches_per_second_std: 0.23794583666596733
          seconds_per_batch_max: 0.252178430557251
          seconds_per_batch_mean: 0.21588655948638916
          seconds_per_batch_min: 0.20230817794799805
          seconds_per_batch_std: 0.01181369305434546
      total:
        human_readable:
          batch_latency: 225.754 ms +/- 12.191 ms [212.189 ms, 263.896 ms]
          batches_per_second: 4.44 +/- 0.22 [3.79, 4.71]
        metrics:
          batches_per_second_max: 4.712775257138876
          batches_per_second_mean: 4.441658788348057
          batches_per_second_min: 3.789375159799035
          batches_per_second_std: 0.2237965786354358
          seconds_per_batch_max: 0.26389575004577637
          seconds_per_batch_mean: 0.22575414657592774
          seconds_per_batch_min: 0.2121891975402832
          seconds_per_batch_std: 0.012190755552306773
    batch_size_8:
      cpu_to_gpu:
        human_readable:
          batch_latency: "11.602 ms +/- 593.437 \xB5s [9.545 ms, 13.346 ms]"
          batches_per_second: 86.42 +/- 4.58 [74.93, 104.77]
        metrics:
          batches_per_second_max: 104.77116378987336
          batches_per_second_mean: 86.4222377292654
          batches_per_second_min: 74.92772160491621
          batches_per_second_std: 4.579540961741608
          seconds_per_batch_max: 0.013346195220947266
          seconds_per_batch_mean: 0.011602356433868408
          seconds_per_batch_min: 0.009544610977172852
          seconds_per_batch_std: 0.0005934369574054142
      gpu_to_cpu:
        human_readable:
          batch_latency: 148.677 ms +/- 6.989 ms [145.778 ms, 185.354 ms]
          batches_per_second: 6.74 +/- 0.27 [5.40, 6.86]
        metrics:
          batches_per_second_max: 6.859725990654832
          batches_per_second_mean: 6.73876461647803
          batches_per_second_min: 5.3950750676586505
          batches_per_second_std: 0.27175126517248005
          seconds_per_batch_max: 0.18535423278808594
          seconds_per_batch_mean: 0.14867662906646728
          seconds_per_batch_min: 0.14577841758728027
          seconds_per_batch_std: 0.0069892240745621885
      on_device_inference:
        human_readable:
          batch_latency: 1.076 s +/- 43.247 ms [1.058 s, 1.314 s]
          batches_per_second: 0.93 +/- 0.03 [0.76, 0.95]
        metrics:
          batches_per_second_max: 0.9454594636022235
          batches_per_second_mean: 0.9310963530182599
          batches_per_second_min: 0.7611121303298258
          batches_per_second_std: 0.03295132510886814
          seconds_per_batch_max: 1.3138668537139893
          seconds_per_batch_mean: 1.0755309224128724
          seconds_per_batch_min: 1.0576868057250977
          seconds_per_batch_std: 0.04324722109532692
      total:
        human_readable:
          batch_latency: 1.236 s +/- 49.075 ms [1.215 s, 1.506 s]
          batches_per_second: 0.81 +/- 0.03 [0.66, 0.82]
        metrics:
          batches_per_second_max: 0.8229584447879839
          batches_per_second_mean: 0.8103125823833465
          batches_per_second_min: 0.663976022609642
          batches_per_second_std: 0.028411971383047848
          seconds_per_batch_max: 1.5060784816741943
          seconds_per_batch_mean: 1.235809907913208
          seconds_per_batch_min: 1.2151281833648682
          seconds_per_batch_std: 0.04907450233470005

INFO:benchmark:==== Benchmarking X3DLearner (l) ====
INFO:benchmark:== Benchmarking learner.infer ==
ERROR:pytorch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:pytorch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
Timing results (batch_size=8):
  cpu_to_gpu:
    human_readable:
      batch_latency: "11.602 ms +/- 593.437 \xB5s [9.545 ms, 13.346 ms]"
      batches_per_second: 86.42 +/- 4.58 [74.93, 104.77]
    metrics:
      batches_per_second_max: 104.77116378987336
      batches_per_second_mean: 86.4222377292654
      batches_per_second_min: 74.92772160491621
      batches_per_second_std: 4.579540961741608
      seconds_per_batch_max: 0.013346195220947266
      seconds_per_batch_mean: 0.011602356433868408
      seconds_per_batch_min: 0.009544610977172852
      seconds_per_batch_std: 0.0005934369574054142
  gpu_to_cpu:
    human_readable:
      batch_latency: 148.677 ms +/- 6.989 ms [145.778 ms, 185.354 ms]
      batches_per_second: 6.74 +/- 0.27 [5.40, 6.86]
    metrics:
      batches_per_second_max: 6.859725990654832
      batches_per_second_mean: 6.73876461647803
      batches_per_second_min: 5.3950750676586505
      batches_per_second_std: 0.27175126517248005
      seconds_per_batch_max: 0.18535423278808594
      seconds_per_batch_mean: 0.14867662906646728
      seconds_per_batch_min: 0.14577841758728027
      seconds_per_batch_std: 0.0069892240745621885
  on_device_inference:
    human_readable:
      batch_latency: 1.076 s +/- 43.247 ms [1.058 s, 1.314 s]
      batches_per_second: 0.93 +/- 0.03 [0.76, 0.95]
    metrics:
      batches_per_second_max: 0.9454594636022235
      batches_per_second_mean: 0.9310963530182599
      batches_per_second_min: 0.7611121303298258
      batches_per_second_std: 0.03295132510886814
      seconds_per_batch_max: 1.3138668537139893
      seconds_per_batch_mean: 1.0755309224128724
      seconds_per_batch_min: 1.0576868057250977
      seconds_per_batch_std: 0.04324722109532692
  total:
    human_readable:
      batch_latency: 1.236 s +/- 49.075 ms [1.215 s, 1.506 s]
      batches_per_second: 0.81 +/- 0.03 [0.66, 0.82]
    metrics:
      batches_per_second_max: 0.8229584447879839
      batches_per_second_mean: 0.8103125823833465
      batches_per_second_min: 0.663976022609642
      batches_per_second_std: 0.028411971383047848
      seconds_per_batch_max: 1.5060784816741943
      seconds_per_batch_mean: 1.235809907913208
      seconds_per_batch_min: 1.2151281833648682
      seconds_per_batch_std: 0.04907450233470005

Inference energy: 33.43725726960499 J (9.28812701933472e-06 kWh)
Energy (batch_size=8):
  joules: 33.43725726960499
  kWh: 9.28812701933472e-06

Machine info:
  cpu:
    architecture: aarch64
    cores:
      physical: 8
      total: 8
    frequency: 2.27 GHz
    model: ARMv8 Processor rev 0 (v8l)
  gpus: null
  memory:
    available: 11.14 GB
    total: 31.17 GB
    used: 21.91 GB
  system:
    node: xavier
    release: 4.9.140-tegra
    system: Linux

Model device: cuda:0
Allocated GPU memory prior to inference: 71496192 (68.18 MB)
Allocated GPU memory after to inference: 71496192 (68.18 MB)
Max allocated GPU memory during inference: 6681312768 (6.22 GB)
Warming up with batch_size=2:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=2:  10%|█         | 1/10 [00:00<00:07,  1.26it/s]Warming up with batch_size=2:  20%|██        | 2/10 [00:01<00:06,  1.26it/s]Warming up with batch_size=2:  30%|███       | 3/10 [00:02<00:05,  1.26it/s]Warming up with batch_size=2:  40%|████      | 4/10 [00:03<00:04,  1.26it/s]Warming up with batch_size=2:  50%|█████     | 5/10 [00:03<00:03,  1.26it/s]Warming up with batch_size=2:  60%|██████    | 6/10 [00:04<00:03,  1.26it/s]Warming up with batch_size=2:  70%|███████   | 7/10 [00:05<00:02,  1.26it/s]Warming up with batch_size=2:  80%|████████  | 8/10 [00:06<00:01,  1.26it/s]Warming up with batch_size=2:  90%|█████████ | 9/10 [00:07<00:00,  1.26it/s]Warming up with batch_size=2: 100%|██████████| 10/10 [00:07<00:00,  1.26it/s]Warming up with batch_size=2: 100%|██████████| 10/10 [00:07<00:00,  1.26it/s]
Measuring inference with batch_size=2:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=2:   1%|          | 1/100 [00:00<01:18,  1.26it/s]Measuring inference with batch_size=2:   2%|▏         | 2/100 [00:01<01:18,  1.26it/s]Measuring inference with batch_size=2:   3%|▎         | 3/100 [00:02<01:17,  1.26it/s]Measuring inference with batch_size=2:   4%|▍         | 4/100 [00:03<01:16,  1.26it/s]Measuring inference with batch_size=2:   5%|▌         | 5/100 [00:03<01:15,  1.26it/s]Measuring inference with batch_size=2:   6%|▌         | 6/100 [00:04<01:14,  1.26it/s]Measuring inference with batch_size=2:   7%|▋         | 7/100 [00:05<01:13,  1.26it/s]Measuring inference with batch_size=2:   8%|▊         | 8/100 [00:06<01:13,  1.26it/s]Measuring inference with batch_size=2:   9%|▉         | 9/100 [00:07<01:12,  1.26it/s]Measuring inference with batch_size=2:  10%|█         | 10/100 [00:07<01:11,  1.26it/s]Measuring inference with batch_size=2:  11%|█         | 11/100 [00:08<01:10,  1.26it/s]Measuring inference with batch_size=2:  12%|█▏        | 12/100 [00:09<01:10,  1.26it/s]Measuring inference with batch_size=2:  13%|█▎        | 13/100 [00:10<01:09,  1.26it/s]Measuring inference with batch_size=2:  14%|█▍        | 14/100 [00:11<01:08,  1.26it/s]Measuring inference with batch_size=2:  15%|█▌        | 15/100 [00:11<01:07,  1.26it/s]Measuring inference with batch_size=2:  16%|█▌        | 16/100 [00:12<01:06,  1.26it/s]Measuring inference with batch_size=2:  17%|█▋        | 17/100 [00:13<01:06,  1.25it/s]Measuring inference with batch_size=2:  18%|█▊        | 18/100 [00:14<01:05,  1.25it/s]Measuring inference with batch_size=2:  19%|█▉        | 19/100 [00:15<01:04,  1.25it/s]Measuring inference with batch_size=2:  20%|██        | 20/100 [00:15<01:03,  1.25it/s]Measuring inference with batch_size=2:  21%|██        | 21/100 [00:16<01:03,  1.25it/s]Measuring inference with batch_size=2:  22%|██▏       | 22/100 [00:17<01:02,  1.25it/s]Measuring inference with batch_size=2:  23%|██▎       | 23/100 [00:18<01:01,  1.25it/s]Measuring inference with batch_size=2:  24%|██▍       | 24/100 [00:19<01:00,  1.25it/s]Measuring inference with batch_size=2:  25%|██▌       | 25/100 [00:19<00:59,  1.25it/s]Measuring inference with batch_size=2:  26%|██▌       | 26/100 [00:20<00:59,  1.25it/s]Measuring inference with batch_size=2:  27%|██▋       | 27/100 [00:21<00:58,  1.25it/s]Measuring inference with batch_size=2:  28%|██▊       | 28/100 [00:22<00:57,  1.25it/s]Measuring inference with batch_size=2:  29%|██▉       | 29/100 [00:23<00:56,  1.25it/s]Measuring inference with batch_size=2:  30%|███       | 30/100 [00:23<00:55,  1.25it/s]Measuring inference with batch_size=2:  31%|███       | 31/100 [00:24<00:54,  1.26it/s]Measuring inference with batch_size=2:  32%|███▏      | 32/100 [00:25<00:54,  1.25it/s]Measuring inference with batch_size=2:  33%|███▎      | 33/100 [00:26<00:53,  1.25it/s]Measuring inference with batch_size=2:  34%|███▍      | 34/100 [00:27<00:52,  1.25it/s]Measuring inference with batch_size=2:  35%|███▌      | 35/100 [00:27<00:51,  1.25it/s]Measuring inference with batch_size=2:  36%|███▌      | 36/100 [00:28<00:50,  1.26it/s]Measuring inference with batch_size=2:  37%|███▋      | 37/100 [00:29<00:50,  1.26it/s]Measuring inference with batch_size=2:  38%|███▊      | 38/100 [00:30<00:49,  1.26it/s]Measuring inference with batch_size=2:  39%|███▉      | 39/100 [00:31<00:48,  1.25it/s]Measuring inference with batch_size=2:  40%|████      | 40/100 [00:31<00:47,  1.25it/s]Measuring inference with batch_size=2:  41%|████      | 41/100 [00:32<00:46,  1.26it/s]Measuring inference with batch_size=2:  42%|████▏     | 42/100 [00:33<00:46,  1.26it/s]Measuring inference with batch_size=2:  43%|████▎     | 43/100 [00:34<00:45,  1.26it/s]Measuring inference with batch_size=2:  44%|████▍     | 44/100 [00:35<00:44,  1.26it/s]Measuring inference with batch_size=2:  45%|████▌     | 45/100 [00:35<00:43,  1.26it/s]Measuring inference with batch_size=2:  46%|████▌     | 46/100 [00:36<00:42,  1.26it/s]Measuring inference with batch_size=2:  47%|████▋     | 47/100 [00:37<00:42,  1.26it/s]Measuring inference with batch_size=2:  48%|████▊     | 48/100 [00:38<00:41,  1.26it/s]Measuring inference with batch_size=2:  49%|████▉     | 49/100 [00:39<00:40,  1.26it/s]Measuring inference with batch_size=2:  50%|█████     | 50/100 [00:39<00:39,  1.26it/s]Measuring inference with batch_size=2:  51%|█████     | 51/100 [00:40<00:39,  1.26it/s]Measuring inference with batch_size=2:  52%|█████▏    | 52/100 [00:41<00:38,  1.26it/s]Measuring inference with batch_size=2:  53%|█████▎    | 53/100 [00:42<00:37,  1.26it/s]Measuring inference with batch_size=2:  54%|█████▍    | 54/100 [00:43<00:36,  1.26it/s]Measuring inference with batch_size=2:  55%|█████▌    | 55/100 [00:43<00:35,  1.26it/s]Measuring inference with batch_size=2:  56%|█████▌    | 56/100 [00:44<00:34,  1.26it/s]Measuring inference with batch_size=2:  57%|█████▋    | 57/100 [00:45<00:34,  1.26it/s]Measuring inference with batch_size=2:  58%|█████▊    | 58/100 [00:46<00:33,  1.26it/s]Measuring inference with batch_size=2:  59%|█████▉    | 59/100 [00:46<00:32,  1.26it/s]Measuring inference with batch_size=2:  60%|██████    | 60/100 [00:47<00:31,  1.26it/s]Measuring inference with batch_size=2:  61%|██████    | 61/100 [00:48<00:31,  1.26it/s]Measuring inference with batch_size=2:  62%|██████▏   | 62/100 [00:49<00:30,  1.26it/s]Measuring inference with batch_size=2:  63%|██████▎   | 63/100 [00:50<00:29,  1.26it/s]Measuring inference with batch_size=2:  64%|██████▍   | 64/100 [00:50<00:28,  1.26it/s]Measuring inference with batch_size=2:  65%|██████▌   | 65/100 [00:51<00:27,  1.26it/s]Measuring inference with batch_size=2:  66%|██████▌   | 66/100 [00:52<00:27,  1.26it/s]Measuring inference with batch_size=2:  67%|██████▋   | 67/100 [00:53<00:26,  1.26it/s]Measuring inference with batch_size=2:  68%|██████▊   | 68/100 [00:54<00:25,  1.26it/s]Measuring inference with batch_size=2:  69%|██████▉   | 69/100 [00:54<00:24,  1.26it/s]Measuring inference with batch_size=2:  70%|███████   | 70/100 [00:55<00:23,  1.26it/s]Measuring inference with batch_size=2:  71%|███████   | 71/100 [00:56<00:23,  1.26it/s]Measuring inference with batch_size=2:  72%|███████▏  | 72/100 [00:57<00:22,  1.26it/s]Measuring inference with batch_size=2:  73%|███████▎  | 73/100 [00:58<00:21,  1.25it/s]Measuring inference with batch_size=2:  74%|███████▍  | 74/100 [00:58<00:20,  1.26it/s]Measuring inference with batch_size=2:  75%|███████▌  | 75/100 [00:59<00:19,  1.26it/s]Measuring inference with batch_size=2:  76%|███████▌  | 76/100 [01:00<00:19,  1.26it/s]Measuring inference with batch_size=2:  77%|███████▋  | 77/100 [01:01<00:18,  1.26it/s]Measuring inference with batch_size=2:  78%|███████▊  | 78/100 [01:02<00:17,  1.25it/s]Measuring inference with batch_size=2:  79%|███████▉  | 79/100 [01:02<00:16,  1.25it/s]Measuring inference with batch_size=2:  80%|████████  | 80/100 [01:03<00:15,  1.25it/s]Measuring inference with batch_size=2:  81%|████████  | 81/100 [01:04<00:15,  1.25it/s]Measuring inference with batch_size=2:  82%|████████▏ | 82/100 [01:05<00:14,  1.25it/s]Measuring inference with batch_size=2:  83%|████████▎ | 83/100 [01:06<00:13,  1.25it/s]Measuring inference with batch_size=2:  84%|████████▍ | 84/100 [01:06<00:12,  1.25it/s]Measuring inference with batch_size=2:  85%|████████▌ | 85/100 [01:07<00:11,  1.25it/s]Measuring inference with batch_size=2:  86%|████████▌ | 86/100 [01:08<00:11,  1.26it/s]Measuring inference with batch_size=2:  87%|████████▋ | 87/100 [01:09<00:10,  1.26it/s]Measuring inference with batch_size=2:  88%|████████▊ | 88/100 [01:10<00:09,  1.26it/s]Measuring inference with batch_size=2:  89%|████████▉ | 89/100 [01:10<00:08,  1.26it/s]Measuring inference with batch_size=2:  90%|█████████ | 90/100 [01:11<00:07,  1.25it/s]Measuring inference with batch_size=2:  91%|█████████ | 91/100 [01:12<00:07,  1.25it/s]Measuring inference with batch_size=2:  92%|█████████▏| 92/100 [01:13<00:06,  1.25it/s]Measuring inference with batch_size=2:  93%|█████████▎| 93/100 [01:14<00:05,  1.25it/s]Measuring inference with batch_size=2:  94%|█████████▍| 94/100 [01:14<00:04,  1.25it/s]Measuring inference with batch_size=2:  95%|█████████▌| 95/100 [01:15<00:03,  1.25it/s]Measuring inference with batch_size=2:  96%|█████████▌| 96/100 [01:16<00:03,  1.25it/s]Measuring inference with batch_size=2:  97%|█████████▋| 97/100 [01:17<00:02,  1.25it/s]Measuring inference with batch_size=2:  98%|█████████▊| 98/100 [01:18<00:01,  1.26it/s]Measuring inference with batch_size=2:  99%|█████████▉| 99/100 [01:18<00:00,  1.26it/s]Measuring inference with batch_size=2: 100%|██████████| 100/100 [01:19<00:00,  1.26it/s]Measuring inference with batch_size=2: 100%|██████████| 100/100 [01:19<00:00,  1.26it/s]
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "11.029 \xB5s +/- 11.919 \xB5s [5.245 \xB5s, 116.825 \xB5s]"
      batches_per_second: 117.57 K +/- 40.42 K [8.56 K, 190.65 K]
    metrics:
      batches_per_second_max: 190650.18181818182
      batches_per_second_mean: 117569.87172282275
      batches_per_second_min: 8559.804081632654
      batches_per_second_std: 40418.944456922596
      seconds_per_batch_max: 0.00011682510375976562
      seconds_per_batch_mean: 1.102924346923828e-05
      seconds_per_batch_min: 5.245208740234375e-06
      seconds_per_batch_std: 1.1919098713564817e-05
  gpu_to_cpu:
    human_readable:
      batch_latency: "391.955 \xB5s +/- 100.064 \xB5s [227.451 \xB5s, 710.011 \xB5\
        s]"
      batches_per_second: 2.70 K +/- 606.92 [1.41 K, 4.40 K]
    metrics:
      batches_per_second_max: 4396.545073375262
      batches_per_second_mean: 2699.182567640746
      batches_per_second_min: 1408.4298186702486
      batches_per_second_std: 606.9219369695096
      seconds_per_batch_max: 0.0007100105285644531
      seconds_per_batch_mean: 0.0003919553756713867
      seconds_per_batch_min: 0.00022745132446289062
      seconds_per_batch_std: 0.00010006405882848955
  on_device_inference:
    human_readable:
      batch_latency: 794.973 ms +/- 1.940 ms [791.232 ms, 800.285 ms]
      batches_per_second: 1.26 +/- 0.00 [1.25, 1.26]
    metrics:
      batches_per_second_max: 1.2638520225138588
      batches_per_second_mean: 1.2579113698891575
      batches_per_second_min: 1.2495546884845228
      batches_per_second_std: 0.0030640303425122054
      seconds_per_batch_max: 0.8002851009368896
      seconds_per_batch_mean: 0.7949732923507691
      seconds_per_batch_min: 0.7912318706512451
      seconds_per_batch_std: 0.0019396787352356987
  total:
    human_readable:
      batch_latency: 795.376 ms +/- 1.950 ms [791.559 ms, 800.897 ms]
      batches_per_second: 1.26 +/- 0.00 [1.25, 1.26]
    metrics:
      batches_per_second_max: 1.263328976961429
      batches_per_second_mean: 1.2572741082093466
      batches_per_second_min: 1.2486005613234563
      batches_per_second_std: 0.003076876531363843
      seconds_per_batch_max: 0.8008966445922852
      seconds_per_batch_mean: 0.7953762769699096
      seconds_per_batch_min: 0.7915594577789307
      seconds_per_batch_std: 0.0019498174937115896

Inference energy: 29.291893963098534 J (8.136637211971815e-06 kWh)
Energy (batch_size=1):
  joules: 29.291893963098534
  kWh: 8.136637211971815e-06

Warming up with batch_size=2:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=2:  10%|█         | 1/10 [00:01<00:10,  1.16s/it]Warming up with batch_size=2:  20%|██        | 2/10 [00:02<00:09,  1.16s/it]Warming up with batch_size=2:  30%|███       | 3/10 [00:03<00:08,  1.16s/it]Warming up with batch_size=2:  40%|████      | 4/10 [00:04<00:06,  1.16s/it]Warming up with batch_size=2:  50%|█████     | 5/10 [00:05<00:05,  1.16s/it]Warming up with batch_size=2:  60%|██████    | 6/10 [00:06<00:04,  1.16s/it]Warming up with batch_size=2:  70%|███████   | 7/10 [00:08<00:03,  1.16s/it]Warming up with batch_size=2:  80%|████████  | 8/10 [00:09<00:02,  1.16s/it]Warming up with batch_size=2:  90%|█████████ | 9/10 [00:10<00:01,  1.16s/it]Warming up with batch_size=2: 100%|██████████| 10/10 [00:11<00:00,  1.16s/it]Warming up with batch_size=2: 100%|██████████| 10/10 [00:11<00:00,  1.16s/it]
Measuring inference with batch_size=2:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=2:   1%|          | 1/100 [00:01<01:54,  1.16s/it]Measuring inference with batch_size=2:   2%|▏         | 2/100 [00:02<01:53,  1.16s/it]Measuring inference with batch_size=2:   3%|▎         | 3/100 [00:03<01:52,  1.16s/it]Measuring inference with batch_size=2:   4%|▍         | 4/100 [00:04<01:51,  1.16s/it]Measuring inference with batch_size=2:   5%|▌         | 5/100 [00:05<01:50,  1.16s/it]Measuring inference with batch_size=2:   6%|▌         | 6/100 [00:06<01:49,  1.16s/it]Measuring inference with batch_size=2:   7%|▋         | 7/100 [00:08<01:48,  1.16s/it]Measuring inference with batch_size=2:   8%|▊         | 8/100 [00:09<01:46,  1.16s/it]Measuring inference with batch_size=2:   9%|▉         | 9/100 [00:10<01:45,  1.16s/it]Measuring inference with batch_size=2:  10%|█         | 10/100 [00:11<01:44,  1.16s/it]Measuring inference with batch_size=2:  11%|█         | 11/100 [00:12<01:43,  1.16s/it]Measuring inference with batch_size=2:  12%|█▏        | 12/100 [00:13<01:42,  1.16s/it]Measuring inference with batch_size=2:  13%|█▎        | 13/100 [00:15<01:40,  1.16s/it]Measuring inference with batch_size=2:  14%|█▍        | 14/100 [00:16<01:39,  1.16s/it]Measuring inference with batch_size=2:  15%|█▌        | 15/100 [00:17<01:38,  1.16s/it]Measuring inference with batch_size=2:  16%|█▌        | 16/100 [00:18<01:37,  1.16s/it]Measuring inference with batch_size=2:  17%|█▋        | 17/100 [00:19<01:36,  1.16s/it]Measuring inference with batch_size=2:  18%|█▊        | 18/100 [00:20<01:35,  1.16s/it]Measuring inference with batch_size=2:  19%|█▉        | 19/100 [00:22<01:33,  1.16s/it]Measuring inference with batch_size=2:  20%|██        | 20/100 [00:23<01:32,  1.16s/it]Measuring inference with batch_size=2:  21%|██        | 21/100 [00:24<01:31,  1.16s/it]Measuring inference with batch_size=2:  22%|██▏       | 22/100 [00:25<01:30,  1.16s/it]Measuring inference with batch_size=2:  23%|██▎       | 23/100 [00:26<01:29,  1.16s/it]Measuring inference with batch_size=2:  24%|██▍       | 24/100 [00:27<01:28,  1.16s/it]Measuring inference with batch_size=2:  25%|██▌       | 25/100 [00:28<01:26,  1.16s/it]Measuring inference with batch_size=2:  26%|██▌       | 26/100 [00:30<01:25,  1.16s/it]Measuring inference with batch_size=2:  27%|██▋       | 27/100 [00:31<01:24,  1.16s/it]Measuring inference with batch_size=2:  28%|██▊       | 28/100 [00:32<01:23,  1.16s/it]Measuring inference with batch_size=2:  29%|██▉       | 29/100 [00:33<01:22,  1.16s/it]Measuring inference with batch_size=2:  30%|███       | 30/100 [00:34<01:21,  1.16s/it]Measuring inference with batch_size=2:  31%|███       | 31/100 [00:35<01:20,  1.16s/it]Measuring inference with batch_size=2:  32%|███▏      | 32/100 [00:37<01:18,  1.16s/it]Measuring inference with batch_size=2:  33%|███▎      | 33/100 [00:38<01:17,  1.16s/it]Measuring inference with batch_size=2:  34%|███▍      | 34/100 [00:39<01:16,  1.16s/it]Measuring inference with batch_size=2:  35%|███▌      | 35/100 [00:40<01:15,  1.16s/it]Measuring inference with batch_size=2:  36%|███▌      | 36/100 [00:41<01:14,  1.16s/it]Measuring inference with batch_size=2:  37%|███▋      | 37/100 [00:42<01:13,  1.16s/it]Measuring inference with batch_size=2:  38%|███▊      | 38/100 [00:44<01:11,  1.16s/it]Measuring inference with batch_size=2:  39%|███▉      | 39/100 [00:45<01:10,  1.16s/it]Measuring inference with batch_size=2:  40%|████      | 40/100 [00:46<01:09,  1.16s/it]Measuring inference with batch_size=2:  41%|████      | 41/100 [00:47<01:08,  1.16s/it]Measuring inference with batch_size=2:  42%|████▏     | 42/100 [00:48<01:07,  1.16s/it]Measuring inference with batch_size=2:  43%|████▎     | 43/100 [00:49<01:06,  1.16s/it]Measuring inference with batch_size=2:  44%|████▍     | 44/100 [00:51<01:05,  1.16s/it]Measuring inference with batch_size=2:  45%|████▌     | 45/100 [00:52<01:03,  1.16s/it]Measuring inference with batch_size=2:  46%|████▌     | 46/100 [00:53<01:02,  1.16s/it]Measuring inference with batch_size=2:  47%|████▋     | 47/100 [00:54<01:01,  1.16s/it]Measuring inference with batch_size=2:  48%|████▊     | 48/100 [00:55<01:00,  1.16s/it]Measuring inference with batch_size=2:  49%|████▉     | 49/100 [00:56<00:59,  1.16s/it]Measuring inference with batch_size=2:  50%|█████     | 50/100 [00:57<00:57,  1.16s/it]Measuring inference with batch_size=2:  51%|█████     | 51/100 [00:59<00:56,  1.16s/it]Measuring inference with batch_size=2:  52%|█████▏    | 52/100 [01:00<00:55,  1.16s/it]Measuring inference with batch_size=2:  53%|█████▎    | 53/100 [01:01<00:54,  1.16s/it]Measuring inference with batch_size=2:  54%|█████▍    | 54/100 [01:02<00:53,  1.16s/it]Measuring inference with batch_size=2:  55%|█████▌    | 55/100 [01:03<00:52,  1.16s/it]Measuring inference with batch_size=2:  56%|█████▌    | 56/100 [01:04<00:50,  1.16s/it]Measuring inference with batch_size=2:  57%|█████▋    | 57/100 [01:06<00:49,  1.16s/it]Measuring inference with batch_size=2:  58%|█████▊    | 58/100 [01:07<00:48,  1.16s/it]Measuring inference with batch_size=2:  59%|█████▉    | 59/100 [01:08<00:47,  1.16s/it]Measuring inference with batch_size=2:  60%|██████    | 60/100 [01:09<00:46,  1.16s/it]Measuring inference with batch_size=2:  61%|██████    | 61/100 [01:10<00:45,  1.16s/it]Measuring inference with batch_size=2:  62%|██████▏   | 62/100 [01:11<00:44,  1.16s/it]Measuring inference with batch_size=2:  63%|██████▎   | 63/100 [01:13<00:42,  1.16s/it]Measuring inference with batch_size=2:  64%|██████▍   | 64/100 [01:14<00:41,  1.16s/it]Measuring inference with batch_size=2:  65%|██████▌   | 65/100 [01:15<00:40,  1.16s/it]Measuring inference with batch_size=2:  66%|██████▌   | 66/100 [01:16<00:39,  1.16s/it]Measuring inference with batch_size=2:  67%|██████▋   | 67/100 [01:17<00:38,  1.16s/it]Measuring inference with batch_size=2:  68%|██████▊   | 68/100 [01:18<00:37,  1.16s/it]Measuring inference with batch_size=2:  69%|██████▉   | 69/100 [01:20<00:36,  1.16s/it]Measuring inference with batch_size=2:  70%|███████   | 70/100 [01:21<00:34,  1.16s/it]Measuring inference with batch_size=2:  71%|███████   | 71/100 [01:22<00:33,  1.16s/it]Measuring inference with batch_size=2:  72%|███████▏  | 72/100 [01:23<00:32,  1.16s/it]Measuring inference with batch_size=2:  73%|███████▎  | 73/100 [01:24<00:31,  1.16s/it]Measuring inference with batch_size=2:  74%|███████▍  | 74/100 [01:25<00:30,  1.16s/it]Measuring inference with batch_size=2:  75%|███████▌  | 75/100 [01:26<00:28,  1.16s/it]Measuring inference with batch_size=2:  76%|███████▌  | 76/100 [01:28<00:27,  1.16s/it]Measuring inference with batch_size=2:  77%|███████▋  | 77/100 [01:29<00:26,  1.16s/it]Measuring inference with batch_size=2:  78%|███████▊  | 78/100 [01:30<00:25,  1.16s/it]Measuring inference with batch_size=2:  79%|███████▉  | 79/100 [01:31<00:24,  1.16s/it]Measuring inference with batch_size=2:  80%|████████  | 80/100 [01:32<00:23,  1.16s/it]Measuring inference with batch_size=2:  81%|████████  | 81/100 [01:33<00:22,  1.16s/it]Measuring inference with batch_size=2:  82%|████████▏ | 82/100 [01:35<00:20,  1.16s/it]Measuring inference with batch_size=2:  83%|████████▎ | 83/100 [01:36<00:19,  1.16s/it]Measuring inference with batch_size=2:  84%|████████▍ | 84/100 [01:37<00:18,  1.16s/it]Measuring inference with batch_size=2:  85%|████████▌ | 85/100 [01:38<00:17,  1.16s/it]Measuring inference with batch_size=2:  86%|████████▌ | 86/100 [01:39<00:16,  1.16s/it]Measuring inference with batch_size=2:  87%|████████▋ | 87/100 [01:40<00:15,  1.16s/it]Measuring inference with batch_size=2:  88%|████████▊ | 88/100 [01:42<00:13,  1.16s/it]Measuring inference with batch_size=2:  89%|████████▉ | 89/100 [01:43<00:12,  1.16s/it]Measuring inference with batch_size=2:  90%|█████████ | 90/100 [01:44<00:11,  1.16s/it]Measuring inference with batch_size=2:  91%|█████████ | 91/100 [01:45<00:10,  1.16s/it]Measuring inference with batch_size=2:  92%|█████████▏| 92/100 [01:46<00:09,  1.16s/it]Measuring inference with batch_size=2:  93%|█████████▎| 93/100 [01:47<00:08,  1.16s/it]Measuring inference with batch_size=2:  94%|█████████▍| 94/100 [01:49<00:06,  1.16s/it]Measuring inference with batch_size=2:  95%|█████████▌| 95/100 [01:50<00:05,  1.16s/it]Measuring inference with batch_size=2:  96%|█████████▌| 96/100 [01:51<00:04,  1.16s/it]Measuring inference with batch_size=2:  97%|█████████▋| 97/100 [01:52<00:03,  1.16s/it]Measuring inference with batch_size=2:  98%|█████████▊| 98/100 [01:53<00:02,  1.16s/it]Measuring inference with batch_size=2:  99%|█████████▉| 99/100 [01:54<00:01,  1.16s/it]Measuring inference with batch_size=2: 100%|██████████| 100/100 [01:55<00:00,  1.16s/it]Measuring inference with batch_size=2: 100%|██████████| 100/100 [01:55<00:00,  1.16s/it]
INFO:benchmark:learner.infer:
  device: cuda
  energy:
    batch_size_1:
      joules: 29.291893963098534
      kWh: 8.136637211971815e-06
    batch_size_2:
      joules: 28.106835600280764
      kWh: 7.807454333411323e-06
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 11.14 GB
      total: 31.17 GB
      used: 21.91 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  max_inference_memory: 6681312768
  post_inference_memory: 71496192
  pre_inference_memory: 71496192
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "11.029 \xB5s +/- 11.919 \xB5s [5.245 \xB5s, 116.825 \xB5\
            s]"
          batches_per_second: 117.57 K +/- 40.42 K [8.56 K, 190.65 K]
        metrics:
          batches_per_second_max: 190650.18181818182
          batches_per_second_mean: 117569.87172282275
          batches_per_second_min: 8559.804081632654
          batches_per_second_std: 40418.944456922596
          seconds_per_batch_max: 0.00011682510375976562
          seconds_per_batch_mean: 1.102924346923828e-05
          seconds_per_batch_min: 5.245208740234375e-06
          seconds_per_batch_std: 1.1919098713564817e-05
      gpu_to_cpu:
        human_readable:
          batch_latency: "391.955 \xB5s +/- 100.064 \xB5s [227.451 \xB5s, 710.011\
            \ \xB5s]"
          batches_per_second: 2.70 K +/- 606.92 [1.41 K, 4.40 K]
        metrics:
          batches_per_second_max: 4396.545073375262
          batches_per_second_mean: 2699.182567640746
          batches_per_second_min: 1408.4298186702486
          batches_per_second_std: 606.9219369695096
          seconds_per_batch_max: 0.0007100105285644531
          seconds_per_batch_mean: 0.0003919553756713867
          seconds_per_batch_min: 0.00022745132446289062
          seconds_per_batch_std: 0.00010006405882848955
      on_device_inference:
        human_readable:
          batch_latency: 794.973 ms +/- 1.940 ms [791.232 ms, 800.285 ms]
          batches_per_second: 1.26 +/- 0.00 [1.25, 1.26]
        metrics:
          batches_per_second_max: 1.2638520225138588
          batches_per_second_mean: 1.2579113698891575
          batches_per_second_min: 1.2495546884845228
          batches_per_second_std: 0.0030640303425122054
          seconds_per_batch_max: 0.8002851009368896
          seconds_per_batch_mean: 0.7949732923507691
          seconds_per_batch_min: 0.7912318706512451
          seconds_per_batch_std: 0.0019396787352356987
      total:
        human_readable:
          batch_latency: 795.376 ms +/- 1.950 ms [791.559 ms, 800.897 ms]
          batches_per_second: 1.26 +/- 0.00 [1.25, 1.26]
        metrics:
          batches_per_second_max: 1.263328976961429
          batches_per_second_mean: 1.2572741082093466
          batches_per_second_min: 1.2486005613234563
          batches_per_second_std: 0.003076876531363843
          seconds_per_batch_max: 0.8008966445922852
          seconds_per_batch_mean: 0.7953762769699096
          seconds_per_batch_min: 0.7915594577789307
          seconds_per_batch_std: 0.0019498174937115896
    batch_size_2:
      cpu_to_gpu:
        human_readable:
          batch_latency: "11.284 \xB5s +/- 4.615 \xB5s [5.722 \xB5s, 40.293 \xB5s]"
          batches_per_second: 99.39 K +/- 30.89 K [24.82 K, 174.76 K]
        metrics:
          batches_per_second_max: 174762.66666666666
          batches_per_second_mean: 99390.77947514475
          batches_per_second_min: 24818.366863905325
          batches_per_second_std: 30885.03132347979
          seconds_per_batch_max: 4.029273986816406e-05
          seconds_per_batch_mean: 1.1284351348876953e-05
          seconds_per_batch_min: 5.7220458984375e-06
          seconds_per_batch_std: 4.615238720075347e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: "535.886 \xB5s +/- 148.398 \xB5s [340.462 \xB5s, 922.680\
            \ \xB5s]"
          batches_per_second: 1.99 K +/- 485.79 [1.08 K, 2.94 K]
        metrics:
          batches_per_second_max: 2937.187675070028
          batches_per_second_mean: 1993.804742604114
          batches_per_second_min: 1083.7994832041343
          batches_per_second_std: 485.7900887747819
          seconds_per_batch_max: 0.0009226799011230469
          seconds_per_batch_mean: 0.000535886287689209
          seconds_per_batch_min: 0.00034046173095703125
          seconds_per_batch_std: 0.00014839771483209145
      on_device_inference:
        human_readable:
          batch_latency: 1.158 s +/- 2.720 ms [1.152 s, 1.168 s]
          batches_per_second: 0.86 +/- 0.00 [0.86, 0.87]
        metrics:
          batches_per_second_max: 0.8678163606908096
          batches_per_second_mean: 0.8635308268187204
          batches_per_second_min: 0.8559449475868652
          batches_per_second_std: 0.002025015841159924
          seconds_per_batch_max: 1.1682994365692139
          seconds_per_batch_mean: 1.1580426287651062
          seconds_per_batch_min: 1.1523175239562988
          seconds_per_batch_std: 0.002720350820914059
      total:
        human_readable:
          batch_latency: 1.159 s +/- 2.735 ms [1.153 s, 1.169 s]
          batches_per_second: 0.86 +/- 0.00 [0.86, 0.87]
        metrics:
          batches_per_second_max: 0.8675354492024899
          batches_per_second_mean: 0.8631230501945508
          batches_per_second_min: 0.855570781825681
          batches_per_second_std: 0.002033810079487952
          seconds_per_batch_max: 1.1688103675842285
          seconds_per_batch_mean: 1.1585897994041443
          seconds_per_batch_min: 1.1526906490325928
          seconds_per_batch_std: 0.002734587691263547

INFO:benchmark:== Benchmarking model directly ==
Timing results (batch_size=2):
  cpu_to_gpu:
    human_readable:
      batch_latency: "11.284 \xB5s +/- 4.615 \xB5s [5.722 \xB5s, 40.293 \xB5s]"
      batches_per_second: 99.39 K +/- 30.89 K [24.82 K, 174.76 K]
    metrics:
      batches_per_second_max: 174762.66666666666
      batches_per_second_mean: 99390.77947514475
      batches_per_second_min: 24818.366863905325
      batches_per_second_std: 30885.03132347979
      seconds_per_batch_max: 4.029273986816406e-05
      seconds_per_batch_mean: 1.1284351348876953e-05
      seconds_per_batch_min: 5.7220458984375e-06
      seconds_per_batch_std: 4.615238720075347e-06
  gpu_to_cpu:
    human_readable:
      batch_latency: "535.886 \xB5s +/- 148.398 \xB5s [340.462 \xB5s, 922.680 \xB5\
        s]"
      batches_per_second: 1.99 K +/- 485.79 [1.08 K, 2.94 K]
    metrics:
      batches_per_second_max: 2937.187675070028
      batches_per_second_mean: 1993.804742604114
      batches_per_second_min: 1083.7994832041343
      batches_per_second_std: 485.7900887747819
      seconds_per_batch_max: 0.0009226799011230469
      seconds_per_batch_mean: 0.000535886287689209
      seconds_per_batch_min: 0.00034046173095703125
      seconds_per_batch_std: 0.00014839771483209145
  on_device_inference:
    human_readable:
      batch_latency: 1.158 s +/- 2.720 ms [1.152 s, 1.168 s]
      batches_per_second: 0.86 +/- 0.00 [0.86, 0.87]
    metrics:
      batches_per_second_max: 0.8678163606908096
      batches_per_second_mean: 0.8635308268187204
      batches_per_second_min: 0.8559449475868652
      batches_per_second_std: 0.002025015841159924
      seconds_per_batch_max: 1.1682994365692139
      seconds_per_batch_mean: 1.1580426287651062
      seconds_per_batch_min: 1.1523175239562988
      seconds_per_batch_std: 0.002720350820914059
  total:
    human_readable:
      batch_latency: 1.159 s +/- 2.735 ms [1.153 s, 1.169 s]
      batches_per_second: 0.86 +/- 0.00 [0.86, 0.87]
    metrics:
      batches_per_second_max: 0.8675354492024899
      batches_per_second_mean: 0.8631230501945508
      batches_per_second_min: 0.855570781825681
      batches_per_second_std: 0.002033810079487952
      seconds_per_batch_max: 1.1688103675842285
      seconds_per_batch_mean: 1.1585897994041443
      seconds_per_batch_min: 1.1526906490325928
      seconds_per_batch_std: 0.002734587691263547

Inference energy: 28.106835600280764 J (7.807454333411323e-06 kWh)
Energy (batch_size=2):
  joules: 28.106835600280764
  kWh: 7.807454333411323e-06

Machine info:
  cpu:
    architecture: aarch64
    cores:
      physical: 8
      total: 8
    frequency: 2.27 GHz
    model: ARMv8 Processor rev 0 (v8l)
  gpus: null
  memory:
    available: 11.06 GB
    total: 31.17 GB
    used: 21.99 GB
  system:
    node: xavier
    release: 4.9.140-tegra
    system: Linux

Model device: cuda:0
Model parameters: 6153432 (6.15 M)
Model FLOPs: 19166052038 (19.17 G)
Allocated GPU memory prior to inference: 71496192 (68.18 MB)
Allocated GPU memory after to inference: 71496192 (68.18 MB)
Max allocated GPU memory during inference: 6681312256 (6.22 GB)
Warming up with batch_size=2:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=2:  10%|█         | 1/10 [00:00<00:07,  1.28it/s]Warming up with batch_size=2:  20%|██        | 2/10 [00:01<00:06,  1.28it/s]Warming up with batch_size=2:  30%|███       | 3/10 [00:02<00:05,  1.28it/s]Warming up with batch_size=2:  40%|████      | 4/10 [00:03<00:04,  1.28it/s]Warming up with batch_size=2:  50%|█████     | 5/10 [00:03<00:03,  1.28it/s]Warming up with batch_size=2:  60%|██████    | 6/10 [00:04<00:03,  1.28it/s]Warming up with batch_size=2:  70%|███████   | 7/10 [00:05<00:02,  1.28it/s]Warming up with batch_size=2:  80%|████████  | 8/10 [00:06<00:01,  1.28it/s]Warming up with batch_size=2:  90%|█████████ | 9/10 [00:07<00:00,  1.28it/s]Warming up with batch_size=2: 100%|██████████| 10/10 [00:07<00:00,  1.28it/s]Warming up with batch_size=2: 100%|██████████| 10/10 [00:07<00:00,  1.28it/s]
Measuring inference with batch_size=2:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=2:   1%|          | 1/100 [00:00<01:17,  1.28it/s]Measuring inference with batch_size=2:   2%|▏         | 2/100 [00:01<01:16,  1.28it/s]Measuring inference with batch_size=2:   3%|▎         | 3/100 [00:02<01:15,  1.28it/s]Measuring inference with batch_size=2:   4%|▍         | 4/100 [00:03<01:15,  1.28it/s]Measuring inference with batch_size=2:   5%|▌         | 5/100 [00:03<01:14,  1.28it/s]Measuring inference with batch_size=2:   6%|▌         | 6/100 [00:04<01:13,  1.28it/s]Measuring inference with batch_size=2:   7%|▋         | 7/100 [00:05<01:12,  1.28it/s]Measuring inference with batch_size=2:   8%|▊         | 8/100 [00:06<01:12,  1.27it/s]Measuring inference with batch_size=2:   9%|▉         | 9/100 [00:07<01:11,  1.28it/s]Measuring inference with batch_size=2:  10%|█         | 10/100 [00:07<01:10,  1.28it/s]Measuring inference with batch_size=2:  11%|█         | 11/100 [00:08<01:09,  1.28it/s]Measuring inference with batch_size=2:  12%|█▏        | 12/100 [00:09<01:08,  1.28it/s]Measuring inference with batch_size=2:  13%|█▎        | 13/100 [00:10<01:08,  1.28it/s]Measuring inference with batch_size=2:  14%|█▍        | 14/100 [00:10<01:07,  1.28it/s]Measuring inference with batch_size=2:  15%|█▌        | 15/100 [00:11<01:06,  1.28it/s]Measuring inference with batch_size=2:  16%|█▌        | 16/100 [00:12<01:05,  1.28it/s]Measuring inference with batch_size=2:  17%|█▋        | 17/100 [00:13<01:04,  1.28it/s]Measuring inference with batch_size=2:  18%|█▊        | 18/100 [00:14<01:04,  1.28it/s]Measuring inference with batch_size=2:  19%|█▉        | 19/100 [00:14<01:03,  1.28it/s]Measuring inference with batch_size=2:  20%|██        | 20/100 [00:15<01:02,  1.28it/s]Measuring inference with batch_size=2:  21%|██        | 21/100 [00:16<01:02,  1.27it/s]Measuring inference with batch_size=2:  22%|██▏       | 22/100 [00:17<01:01,  1.27it/s]Measuring inference with batch_size=2:  23%|██▎       | 23/100 [00:18<01:00,  1.27it/s]Measuring inference with batch_size=2:  24%|██▍       | 24/100 [00:18<00:59,  1.27it/s]Measuring inference with batch_size=2:  25%|██▌       | 25/100 [00:19<00:58,  1.28it/s]Measuring inference with batch_size=2:  26%|██▌       | 26/100 [00:20<00:57,  1.28it/s]Measuring inference with batch_size=2:  27%|██▋       | 27/100 [00:21<00:57,  1.28it/s]Measuring inference with batch_size=2:  28%|██▊       | 28/100 [00:21<00:56,  1.28it/s]Measuring inference with batch_size=2:  29%|██▉       | 29/100 [00:22<00:55,  1.28it/s]Measuring inference with batch_size=2:  30%|███       | 30/100 [00:23<00:54,  1.28it/s]Measuring inference with batch_size=2:  31%|███       | 31/100 [00:24<00:54,  1.27it/s]Measuring inference with batch_size=2:  32%|███▏      | 32/100 [00:25<00:53,  1.28it/s]Measuring inference with batch_size=2:  33%|███▎      | 33/100 [00:25<00:52,  1.28it/s]Measuring inference with batch_size=2:  34%|███▍      | 34/100 [00:26<00:51,  1.28it/s]Measuring inference with batch_size=2:  35%|███▌      | 35/100 [00:27<00:50,  1.28it/s]Measuring inference with batch_size=2:  36%|███▌      | 36/100 [00:28<00:50,  1.28it/s]Measuring inference with batch_size=2:  37%|███▋      | 37/100 [00:28<00:49,  1.28it/s]Measuring inference with batch_size=2:  38%|███▊      | 38/100 [00:29<00:48,  1.28it/s]Measuring inference with batch_size=2:  39%|███▉      | 39/100 [00:30<00:47,  1.28it/s]Measuring inference with batch_size=2:  40%|████      | 40/100 [00:31<00:46,  1.28it/s]Measuring inference with batch_size=2:  41%|████      | 41/100 [00:32<00:46,  1.28it/s]Measuring inference with batch_size=2:  42%|████▏     | 42/100 [00:32<00:45,  1.28it/s]Measuring inference with batch_size=2:  43%|████▎     | 43/100 [00:33<00:44,  1.28it/s]Measuring inference with batch_size=2:  44%|████▍     | 44/100 [00:34<00:43,  1.28it/s]Measuring inference with batch_size=2:  45%|████▌     | 45/100 [00:35<00:43,  1.28it/s]Measuring inference with batch_size=2:  46%|████▌     | 46/100 [00:36<00:42,  1.28it/s]Measuring inference with batch_size=2:  47%|████▋     | 47/100 [00:36<00:41,  1.28it/s]Measuring inference with batch_size=2:  48%|████▊     | 48/100 [00:37<00:40,  1.28it/s]Measuring inference with batch_size=2:  49%|████▉     | 49/100 [00:38<00:39,  1.28it/s]Measuring inference with batch_size=2:  50%|█████     | 50/100 [00:39<00:39,  1.28it/s]Measuring inference with batch_size=2:  51%|█████     | 51/100 [00:39<00:38,  1.28it/s]Measuring inference with batch_size=2:  52%|█████▏    | 52/100 [00:40<00:37,  1.28it/s]Measuring inference with batch_size=2:  53%|█████▎    | 53/100 [00:41<00:36,  1.28it/s]Measuring inference with batch_size=2:  54%|█████▍    | 54/100 [00:42<00:36,  1.28it/s]Measuring inference with batch_size=2:  55%|█████▌    | 55/100 [00:43<00:35,  1.28it/s]Measuring inference with batch_size=2:  56%|█████▌    | 56/100 [00:43<00:34,  1.28it/s]Measuring inference with batch_size=2:  57%|█████▋    | 57/100 [00:44<00:33,  1.28it/s]Measuring inference with batch_size=2:  58%|█████▊    | 58/100 [00:45<00:32,  1.28it/s]Measuring inference with batch_size=2:  59%|█████▉    | 59/100 [00:46<00:32,  1.28it/s]Measuring inference with batch_size=2:  60%|██████    | 60/100 [00:46<00:31,  1.28it/s]Measuring inference with batch_size=2:  61%|██████    | 61/100 [00:47<00:30,  1.28it/s]Measuring inference with batch_size=2:  62%|██████▏   | 62/100 [00:48<00:29,  1.28it/s]Measuring inference with batch_size=2:  63%|██████▎   | 63/100 [00:49<00:28,  1.28it/s]Measuring inference with batch_size=2:  64%|██████▍   | 64/100 [00:50<00:28,  1.28it/s]Measuring inference with batch_size=2:  65%|██████▌   | 65/100 [00:50<00:27,  1.28it/s]Measuring inference with batch_size=2:  66%|██████▌   | 66/100 [00:51<00:26,  1.28it/s]Measuring inference with batch_size=2:  67%|██████▋   | 67/100 [00:52<00:25,  1.28it/s]Measuring inference with batch_size=2:  68%|██████▊   | 68/100 [00:53<00:24,  1.28it/s]Measuring inference with batch_size=2:  69%|██████▉   | 69/100 [00:54<00:24,  1.28it/s]Measuring inference with batch_size=2:  70%|███████   | 70/100 [00:54<00:23,  1.28it/s]Measuring inference with batch_size=2:  71%|███████   | 71/100 [00:55<00:22,  1.28it/s]Measuring inference with batch_size=2:  72%|███████▏  | 72/100 [00:56<00:21,  1.28it/s]Measuring inference with batch_size=2:  73%|███████▎  | 73/100 [00:57<00:21,  1.28it/s]Measuring inference with batch_size=2:  74%|███████▍  | 74/100 [00:57<00:20,  1.28it/s]Measuring inference with batch_size=2:  75%|███████▌  | 75/100 [00:58<00:19,  1.28it/s]Measuring inference with batch_size=2:  76%|███████▌  | 76/100 [00:59<00:18,  1.28it/s]Measuring inference with batch_size=2:  77%|███████▋  | 77/100 [01:00<00:18,  1.28it/s]Measuring inference with batch_size=2:  78%|███████▊  | 78/100 [01:01<00:17,  1.28it/s]Measuring inference with batch_size=2:  79%|███████▉  | 79/100 [01:01<00:16,  1.28it/s]Measuring inference with batch_size=2:  80%|████████  | 80/100 [01:02<00:15,  1.28it/s]Measuring inference with batch_size=2:  81%|████████  | 81/100 [01:03<00:14,  1.28it/s]Measuring inference with batch_size=2:  82%|████████▏ | 82/100 [01:04<00:14,  1.28it/s]Measuring inference with batch_size=2:  83%|████████▎ | 83/100 [01:04<00:13,  1.28it/s]Measuring inference with batch_size=2:  84%|████████▍ | 84/100 [01:05<00:12,  1.28it/s]Measuring inference with batch_size=2:  85%|████████▌ | 85/100 [01:06<00:11,  1.28it/s]Measuring inference with batch_size=2:  86%|████████▌ | 86/100 [01:07<00:10,  1.28it/s]Measuring inference with batch_size=2:  87%|████████▋ | 87/100 [01:08<00:10,  1.28it/s]Measuring inference with batch_size=2:  88%|████████▊ | 88/100 [01:08<00:09,  1.28it/s]Measuring inference with batch_size=2:  89%|████████▉ | 89/100 [01:09<00:08,  1.28it/s]Measuring inference with batch_size=2:  90%|█████████ | 90/100 [01:10<00:07,  1.28it/s]Measuring inference with batch_size=2:  91%|█████████ | 91/100 [01:11<00:07,  1.28it/s]Measuring inference with batch_size=2:  92%|█████████▏| 92/100 [01:12<00:06,  1.28it/s]Measuring inference with batch_size=2:  93%|█████████▎| 93/100 [01:12<00:05,  1.28it/s]Measuring inference with batch_size=2:  94%|█████████▍| 94/100 [01:13<00:04,  1.28it/s]Measuring inference with batch_size=2:  95%|█████████▌| 95/100 [01:14<00:03,  1.28it/s]Measuring inference with batch_size=2:  96%|█████████▌| 96/100 [01:15<00:03,  1.28it/s]Measuring inference with batch_size=2:  97%|█████████▋| 97/100 [01:15<00:02,  1.28it/s]Measuring inference with batch_size=2:  98%|█████████▊| 98/100 [01:16<00:01,  1.28it/s]Measuring inference with batch_size=2:  99%|█████████▉| 99/100 [01:17<00:00,  1.28it/s]Measuring inference with batch_size=2: 100%|██████████| 100/100 [01:18<00:00,  1.28it/s]Measuring inference with batch_size=2: 100%|██████████| 100/100 [01:18<00:00,  1.28it/s]
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "2.754 ms +/- 553.912 \xB5s [2.369 ms, 4.298 ms]"
      batches_per_second: 374.61 +/- 57.17 [232.67, 422.13]
    metrics:
      batches_per_second_max: 422.1320450885668
      batches_per_second_mean: 374.60611308429293
      batches_per_second_min: 232.66788705830143
      batches_per_second_std: 57.16750444890723
      seconds_per_batch_max: 0.004297971725463867
      seconds_per_batch_mean: 0.0027536582946777344
      seconds_per_batch_min: 0.002368927001953125
      seconds_per_batch_std: 0.0005539124421900796
  gpu_to_cpu:
    human_readable:
      batch_latency: "38.922 ms +/- 470.528 \xB5s [38.160 ms, 41.562 ms]"
      batches_per_second: 25.70 +/- 0.30 [24.06, 26.21]
    metrics:
      batches_per_second_max: 26.20522817014045
      batches_per_second_mean: 25.695888233611196
      batches_per_second_min: 24.06025527032841
      batches_per_second_std: 0.29754734619539114
      seconds_per_batch_max: 0.04156231880187988
      seconds_per_batch_mean: 0.03892217874526978
      seconds_per_batch_min: 0.03816032409667969
      seconds_per_batch_std: 0.00047052761414545063
  on_device_inference:
    human_readable:
      batch_latency: 739.892 ms +/- 1.749 ms [736.073 ms, 751.145 ms]
      batches_per_second: 1.35 +/- 0.00 [1.33, 1.36]
    metrics:
      batches_per_second_max: 1.3585604319611053
      batches_per_second_mean: 1.3515569451260168
      batches_per_second_min: 1.331301505239279
      batches_per_second_std: 0.00317564054127214
      seconds_per_batch_max: 0.7511446475982666
      seconds_per_batch_mean: 0.7398915433883667
      seconds_per_batch_min: 0.7360732555389404
      seconds_per_batch_std: 0.0017490223370515516
  total:
    human_readable:
      batch_latency: 781.567 ms +/- 1.778 ms [777.284 ms, 792.862 ms]
      batches_per_second: 1.28 +/- 0.00 [1.26, 1.29]
    metrics:
      batches_per_second_max: 1.2865315977969207
      batches_per_second_mean: 1.2794867995271597
      batches_per_second_min: 1.2612532532652936
      batches_per_second_std: 0.0028941716259017597
      seconds_per_batch_max: 0.7928621768951416
      seconds_per_batch_mean: 0.7815673804283142
      seconds_per_batch_min: 0.7772836685180664
      seconds_per_batch_std: 0.0017781700340443326

Inference energy: 26.950170356496177 J (7.486158432360049e-06 kWh)
Energy (batch_size=1):
  joules: 26.950170356496177
  kWh: 7.486158432360049e-06

Warming up with batch_size=2:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=2:  10%|█         | 1/10 [00:01<00:10,  1.14s/it]Warming up with batch_size=2:  20%|██        | 2/10 [00:02<00:09,  1.14s/it]Warming up with batch_size=2:  30%|███       | 3/10 [00:03<00:07,  1.14s/it]Warming up with batch_size=2:  40%|████      | 4/10 [00:04<00:06,  1.14s/it]Warming up with batch_size=2:  50%|█████     | 5/10 [00:05<00:05,  1.14s/it]Warming up with batch_size=2:  60%|██████    | 6/10 [00:06<00:04,  1.14s/it]Warming up with batch_size=2:  70%|███████   | 7/10 [00:07<00:03,  1.14s/it]Warming up with batch_size=2:  80%|████████  | 8/10 [00:09<00:02,  1.14s/it]Warming up with batch_size=2:  90%|█████████ | 9/10 [00:10<00:01,  1.14s/it]Warming up with batch_size=2: 100%|██████████| 10/10 [00:11<00:00,  1.14s/it]Warming up with batch_size=2: 100%|██████████| 10/10 [00:11<00:00,  1.14s/it]
Measuring inference with batch_size=2:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=2:   1%|          | 1/100 [00:01<01:52,  1.14s/it]Measuring inference with batch_size=2:   2%|▏         | 2/100 [00:02<01:51,  1.14s/it]Measuring inference with batch_size=2:   3%|▎         | 3/100 [00:03<01:50,  1.14s/it]Measuring inference with batch_size=2:   4%|▍         | 4/100 [00:04<01:49,  1.14s/it]Measuring inference with batch_size=2:   5%|▌         | 5/100 [00:05<01:48,  1.14s/it]Measuring inference with batch_size=2:   6%|▌         | 6/100 [00:06<01:47,  1.14s/it]Measuring inference with batch_size=2:   7%|▋         | 7/100 [00:07<01:46,  1.14s/it]Measuring inference with batch_size=2:   8%|▊         | 8/100 [00:09<01:44,  1.14s/it]Measuring inference with batch_size=2:   9%|▉         | 9/100 [00:10<01:43,  1.14s/it]Measuring inference with batch_size=2:  10%|█         | 10/100 [00:11<01:42,  1.14s/it]Measuring inference with batch_size=2:  11%|█         | 11/100 [00:12<01:41,  1.14s/it]Measuring inference with batch_size=2:  12%|█▏        | 12/100 [00:13<01:40,  1.14s/it]Measuring inference with batch_size=2:  13%|█▎        | 13/100 [00:14<01:39,  1.14s/it]Measuring inference with batch_size=2:  14%|█▍        | 14/100 [00:15<01:38,  1.14s/it]Measuring inference with batch_size=2:  15%|█▌        | 15/100 [00:17<01:36,  1.14s/it]Measuring inference with batch_size=2:  16%|█▌        | 16/100 [00:18<01:35,  1.14s/it]Measuring inference with batch_size=2:  17%|█▋        | 17/100 [00:19<01:34,  1.14s/it]Measuring inference with batch_size=2:  18%|█▊        | 18/100 [00:20<01:33,  1.14s/it]Measuring inference with batch_size=2:  19%|█▉        | 19/100 [00:21<01:32,  1.14s/it]Measuring inference with batch_size=2:  20%|██        | 20/100 [00:22<01:31,  1.14s/it]Measuring inference with batch_size=2:  21%|██        | 21/100 [00:23<01:30,  1.14s/it]Measuring inference with batch_size=2:  22%|██▏       | 22/100 [00:25<01:28,  1.14s/it]Measuring inference with batch_size=2:  23%|██▎       | 23/100 [00:26<01:27,  1.14s/it]Measuring inference with batch_size=2:  24%|██▍       | 24/100 [00:27<01:26,  1.14s/it]Measuring inference with batch_size=2:  25%|██▌       | 25/100 [00:28<01:25,  1.14s/it]Measuring inference with batch_size=2:  26%|██▌       | 26/100 [00:29<01:24,  1.14s/it]Measuring inference with batch_size=2:  27%|██▋       | 27/100 [00:30<01:23,  1.14s/it]Measuring inference with batch_size=2:  28%|██▊       | 28/100 [00:31<01:22,  1.14s/it]Measuring inference with batch_size=2:  29%|██▉       | 29/100 [00:33<01:20,  1.14s/it]Measuring inference with batch_size=2:  30%|███       | 30/100 [00:34<01:19,  1.14s/it]Measuring inference with batch_size=2:  31%|███       | 31/100 [00:35<01:18,  1.14s/it]Measuring inference with batch_size=2:  32%|███▏      | 32/100 [00:36<01:17,  1.14s/it]Measuring inference with batch_size=2:  33%|███▎      | 33/100 [00:37<01:16,  1.14s/it]Measuring inference with batch_size=2:  34%|███▍      | 34/100 [00:38<01:15,  1.14s/it]Measuring inference with batch_size=2:  35%|███▌      | 35/100 [00:39<01:14,  1.14s/it]Measuring inference with batch_size=2:  36%|███▌      | 36/100 [00:41<01:12,  1.14s/it]Measuring inference with batch_size=2:  37%|███▋      | 37/100 [00:42<01:11,  1.14s/it]Measuring inference with batch_size=2:  38%|███▊      | 38/100 [00:43<01:10,  1.14s/it]Measuring inference with batch_size=2:  39%|███▉      | 39/100 [00:44<01:09,  1.14s/it]Measuring inference with batch_size=2:  40%|████      | 40/100 [00:45<01:08,  1.14s/it]Measuring inference with batch_size=2:  41%|████      | 41/100 [00:46<01:07,  1.14s/it]Measuring inference with batch_size=2:  42%|████▏     | 42/100 [00:47<01:06,  1.14s/it]Measuring inference with batch_size=2:  43%|████▎     | 43/100 [00:49<01:05,  1.14s/it]Measuring inference with batch_size=2:  44%|████▍     | 44/100 [00:50<01:03,  1.14s/it]Measuring inference with batch_size=2:  45%|████▌     | 45/100 [00:51<01:02,  1.14s/it]Measuring inference with batch_size=2:  46%|████▌     | 46/100 [00:52<01:01,  1.14s/it]Measuring inference with batch_size=2:  47%|████▋     | 47/100 [00:53<01:00,  1.14s/it]Measuring inference with batch_size=2:  48%|████▊     | 48/100 [00:54<00:59,  1.14s/it]Measuring inference with batch_size=2:  49%|████▉     | 49/100 [00:55<00:58,  1.14s/it]Measuring inference with batch_size=2:  50%|█████     | 50/100 [00:57<00:57,  1.14s/it]Measuring inference with batch_size=2:  51%|█████     | 51/100 [00:58<00:55,  1.14s/it]Measuring inference with batch_size=2:  52%|█████▏    | 52/100 [00:59<00:54,  1.14s/it]Measuring inference with batch_size=2:  53%|█████▎    | 53/100 [01:00<00:53,  1.14s/it]Measuring inference with batch_size=2:  54%|█████▍    | 54/100 [01:01<00:52,  1.14s/it]Measuring inference with batch_size=2:  55%|█████▌    | 55/100 [01:02<00:51,  1.14s/it]Measuring inference with batch_size=2:  56%|█████▌    | 56/100 [01:03<00:50,  1.14s/it]Measuring inference with batch_size=2:  57%|█████▋    | 57/100 [01:05<00:49,  1.14s/it]Measuring inference with batch_size=2:  58%|█████▊    | 58/100 [01:06<00:47,  1.14s/it]Measuring inference with batch_size=2:  59%|█████▉    | 59/100 [01:07<00:46,  1.14s/it]Measuring inference with batch_size=2:  60%|██████    | 60/100 [01:08<00:45,  1.14s/it]Measuring inference with batch_size=2:  61%|██████    | 61/100 [01:09<00:44,  1.14s/it]Measuring inference with batch_size=2:  62%|██████▏   | 62/100 [01:10<00:43,  1.14s/it]Measuring inference with batch_size=2:  63%|██████▎   | 63/100 [01:11<00:42,  1.14s/it]Measuring inference with batch_size=2:  64%|██████▍   | 64/100 [01:12<00:41,  1.14s/it]Measuring inference with batch_size=2:  65%|██████▌   | 65/100 [01:14<00:39,  1.14s/it]Measuring inference with batch_size=2:  66%|██████▌   | 66/100 [01:15<00:38,  1.14s/it]Measuring inference with batch_size=2:  67%|██████▋   | 67/100 [01:16<00:37,  1.14s/it]Measuring inference with batch_size=2:  68%|██████▊   | 68/100 [01:17<00:36,  1.14s/it]Measuring inference with batch_size=2:  69%|██████▉   | 69/100 [01:18<00:35,  1.14s/it]Measuring inference with batch_size=2:  70%|███████   | 70/100 [01:19<00:34,  1.14s/it]Measuring inference with batch_size=2:  71%|███████   | 71/100 [01:20<00:33,  1.14s/it]Measuring inference with batch_size=2:  72%|███████▏  | 72/100 [01:22<00:31,  1.14s/it]Measuring inference with batch_size=2:  73%|███████▎  | 73/100 [01:23<00:30,  1.14s/it]Measuring inference with batch_size=2:  74%|███████▍  | 74/100 [01:24<00:29,  1.14s/it]Measuring inference with batch_size=2:  75%|███████▌  | 75/100 [01:25<00:28,  1.14s/it]Measuring inference with batch_size=2:  76%|███████▌  | 76/100 [01:26<00:27,  1.14s/it]Measuring inference with batch_size=2:  77%|███████▋  | 77/100 [01:27<00:26,  1.14s/it]Measuring inference with batch_size=2:  78%|███████▊  | 78/100 [01:28<00:25,  1.14s/it]Measuring inference with batch_size=2:  79%|███████▉  | 79/100 [01:30<00:23,  1.14s/it]Measuring inference with batch_size=2:  80%|████████  | 80/100 [01:31<00:22,  1.14s/it]Measuring inference with batch_size=2:  81%|████████  | 81/100 [01:32<00:21,  1.14s/it]Measuring inference with batch_size=2:  82%|████████▏ | 82/100 [01:33<00:20,  1.14s/it]Measuring inference with batch_size=2:  83%|████████▎ | 83/100 [01:34<00:19,  1.14s/it]Measuring inference with batch_size=2:  84%|████████▍ | 84/100 [01:35<00:18,  1.14s/it]Measuring inference with batch_size=2:  85%|████████▌ | 85/100 [01:36<00:17,  1.14s/it]Measuring inference with batch_size=2:  86%|████████▌ | 86/100 [01:38<00:15,  1.14s/it]Measuring inference with batch_size=2:  87%|████████▋ | 87/100 [01:39<00:14,  1.14s/it]Measuring inference with batch_size=2:  88%|████████▊ | 88/100 [01:40<00:13,  1.14s/it]Measuring inference with batch_size=2:  89%|████████▉ | 89/100 [01:41<00:12,  1.14s/it]Measuring inference with batch_size=2:  90%|█████████ | 90/100 [01:42<00:11,  1.14s/it]Measuring inference with batch_size=2:  91%|█████████ | 91/100 [01:43<00:10,  1.14s/it]Measuring inference with batch_size=2:  92%|█████████▏| 92/100 [01:44<00:09,  1.14s/it]Measuring inference with batch_size=2:  93%|█████████▎| 93/100 [01:46<00:08,  1.14s/it]Measuring inference with batch_size=2:  94%|█████████▍| 94/100 [01:47<00:06,  1.14s/it]Measuring inference with batch_size=2:  95%|█████████▌| 95/100 [01:48<00:05,  1.14s/it]Measuring inference with batch_size=2:  96%|█████████▌| 96/100 [01:49<00:04,  1.14s/it]Measuring inference with batch_size=2:  97%|█████████▋| 97/100 [01:50<00:03,  1.14s/it]Measuring inference with batch_size=2:  98%|█████████▊| 98/100 [01:51<00:02,  1.14s/it]Measuring inference with batch_size=2:  99%|█████████▉| 99/100 [01:52<00:01,  1.14s/it]Measuring inference with batch_size=2: 100%|██████████| 100/100 [01:54<00:00,  1.14s/it]Measuring inference with batch_size=2: 100%|██████████| 100/100 [01:54<00:00,  1.14s/it]
INFO:benchmark:learner.model.forward:
  device: cuda
  energy:
    batch_size_1:
      joules: 26.950170356496177
      kWh: 7.486158432360049e-06
    batch_size_2:
      joules: 28.352939332183208
      kWh: 7.875816481162003e-06
  flops: 19166052038
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 11.06 GB
      total: 31.17 GB
      used: 21.99 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  max_inference_memory: 6681312256
  params: 6153432
  post_inference_memory: 71496192
  pre_inference_memory: 71496192
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "2.754 ms +/- 553.912 \xB5s [2.369 ms, 4.298 ms]"
          batches_per_second: 374.61 +/- 57.17 [232.67, 422.13]
        metrics:
          batches_per_second_max: 422.1320450885668
          batches_per_second_mean: 374.60611308429293
          batches_per_second_min: 232.66788705830143
          batches_per_second_std: 57.16750444890723
          seconds_per_batch_max: 0.004297971725463867
          seconds_per_batch_mean: 0.0027536582946777344
          seconds_per_batch_min: 0.002368927001953125
          seconds_per_batch_std: 0.0005539124421900796
      gpu_to_cpu:
        human_readable:
          batch_latency: "38.922 ms +/- 470.528 \xB5s [38.160 ms, 41.562 ms]"
          batches_per_second: 25.70 +/- 0.30 [24.06, 26.21]
        metrics:
          batches_per_second_max: 26.20522817014045
          batches_per_second_mean: 25.695888233611196
          batches_per_second_min: 24.06025527032841
          batches_per_second_std: 0.29754734619539114
          seconds_per_batch_max: 0.04156231880187988
          seconds_per_batch_mean: 0.03892217874526978
          seconds_per_batch_min: 0.03816032409667969
          seconds_per_batch_std: 0.00047052761414545063
      on_device_inference:
        human_readable:
          batch_latency: 739.892 ms +/- 1.749 ms [736.073 ms, 751.145 ms]
          batches_per_second: 1.35 +/- 0.00 [1.33, 1.36]
        metrics:
          batches_per_second_max: 1.3585604319611053
          batches_per_second_mean: 1.3515569451260168
          batches_per_second_min: 1.331301505239279
          batches_per_second_std: 0.00317564054127214
          seconds_per_batch_max: 0.7511446475982666
          seconds_per_batch_mean: 0.7398915433883667
          seconds_per_batch_min: 0.7360732555389404
          seconds_per_batch_std: 0.0017490223370515516
      total:
        human_readable:
          batch_latency: 781.567 ms +/- 1.778 ms [777.284 ms, 792.862 ms]
          batches_per_second: 1.28 +/- 0.00 [1.26, 1.29]
        metrics:
          batches_per_second_max: 1.2865315977969207
          batches_per_second_mean: 1.2794867995271597
          batches_per_second_min: 1.2612532532652936
          batches_per_second_std: 0.0028941716259017597
          seconds_per_batch_max: 0.7928621768951416
          seconds_per_batch_mean: 0.7815673804283142
          seconds_per_batch_min: 0.7772836685180664
          seconds_per_batch_std: 0.0017781700340443326
    batch_size_2:
      cpu_to_gpu:
        human_readable:
          batch_latency: "5.663 ms +/- 640.658 \xB5s [4.616 ms, 7.994 ms]"
          batches_per_second: 178.73 +/- 19.09 [125.10, 216.66]
        metrics:
          batches_per_second_max: 216.65912495480137
          batches_per_second_mean: 178.72695786629677
          batches_per_second_min: 125.0985445001193
          batches_per_second_std: 19.09052943801352
          seconds_per_batch_max: 0.007993698120117188
          seconds_per_batch_mean: 0.005662827491760254
          seconds_per_batch_min: 0.0046155452728271484
          seconds_per_batch_std: 0.0006406582791449725
      gpu_to_cpu:
        human_readable:
          batch_latency: "41.205 ms +/- 697.243 \xB5s [40.318 ms, 44.601 ms]"
          batches_per_second: 24.28 +/- 0.39 [22.42, 24.80]
        metrics:
          batches_per_second_max: 24.802810071789292
          batches_per_second_mean: 24.275663333271673
          batches_per_second_min: 22.42104025231197
          batches_per_second_std: 0.38732815099985124
          seconds_per_batch_max: 0.0446009635925293
          seconds_per_batch_mean: 0.04120464086532593
          seconds_per_batch_min: 0.04031801223754883
          seconds_per_batch_std: 0.000697242628435556
      on_device_inference:
        human_readable:
          batch_latency: 1.093 s +/- 2.222 ms [1.086 s, 1.101 s]
          batches_per_second: 0.92 +/- 0.00 [0.91, 0.92]
        metrics:
          batches_per_second_max: 0.9205013537675383
          batches_per_second_mean: 0.9151901234549265
          batches_per_second_min: 0.9083936069665318
          batches_per_second_std: 0.001858966095793544
          seconds_per_batch_max: 1.100844383239746
          seconds_per_batch_mean: 1.092673647403717
          seconds_per_batch_min: 1.086364507675171
          seconds_per_batch_std: 0.0022218260851460904
      total:
        human_readable:
          batch_latency: 1.140 s +/- 2.155 ms [1.135 s, 1.147 s]
          batches_per_second: 0.88 +/- 0.00 [0.87, 0.88]
        metrics:
          batches_per_second_max: 0.881115208492377
          batches_per_second_mean: 0.8775493551366434
          batches_per_second_min: 0.8715927691720406
          batches_per_second_std: 0.0016570267060127745
          seconds_per_batch_max: 1.147324800491333
          seconds_per_batch_mean: 1.1395411157608033
          seconds_per_batch_min: 1.134925365447998
          seconds_per_batch_std: 0.002155249674348789

Timing results (batch_size=2):
  cpu_to_gpu:
    human_readable:
      batch_latency: "5.663 ms +/- 640.658 \xB5s [4.616 ms, 7.994 ms]"
      batches_per_second: 178.73 +/- 19.09 [125.10, 216.66]
    metrics:
      batches_per_second_max: 216.65912495480137
      batches_per_second_mean: 178.72695786629677
      batches_per_second_min: 125.0985445001193
      batches_per_second_std: 19.09052943801352
      seconds_per_batch_max: 0.007993698120117188
      seconds_per_batch_mean: 0.005662827491760254
      seconds_per_batch_min: 0.0046155452728271484
      seconds_per_batch_std: 0.0006406582791449725
  gpu_to_cpu:
    human_readable:
      batch_latency: "41.205 ms +/- 697.243 \xB5s [40.318 ms, 44.601 ms]"
      batches_per_second: 24.28 +/- 0.39 [22.42, 24.80]
    metrics:
      batches_per_second_max: 24.802810071789292
      batches_per_second_mean: 24.275663333271673
      batches_per_second_min: 22.42104025231197
      batches_per_second_std: 0.38732815099985124
      seconds_per_batch_max: 0.0446009635925293
      seconds_per_batch_mean: 0.04120464086532593
      seconds_per_batch_min: 0.04031801223754883
      seconds_per_batch_std: 0.000697242628435556
  on_device_inference:
    human_readable:
      batch_latency: 1.093 s +/- 2.222 ms [1.086 s, 1.101 s]
      batches_per_second: 0.92 +/- 0.00 [0.91, 0.92]
    metrics:
      batches_per_second_max: 0.9205013537675383
      batches_per_second_mean: 0.9151901234549265
      batches_per_second_min: 0.9083936069665318
      batches_per_second_std: 0.001858966095793544
      seconds_per_batch_max: 1.100844383239746
      seconds_per_batch_mean: 1.092673647403717
      seconds_per_batch_min: 1.086364507675171
      seconds_per_batch_std: 0.0022218260851460904
  total:
    human_readable:
      batch_latency: 1.140 s +/- 2.155 ms [1.135 s, 1.147 s]
      batches_per_second: 0.88 +/- 0.00 [0.87, 0.88]
    metrics:
      batches_per_second_max: 0.881115208492377
      batches_per_second_mean: 0.8775493551366434
      batches_per_second_min: 0.8715927691720406
      batches_per_second_std: 0.0016570267060127745
      seconds_per_batch_max: 1.147324800491333
      seconds_per_batch_mean: 1.1395411157608033
      seconds_per_batch_min: 1.134925365447998
      seconds_per_batch_std: 0.002155249674348789

Inference energy: 28.352939332183208 J (7.875816481162003e-06 kWh)
Energy (batch_size=2):
  joules: 28.352939332183208
  kWh: 7.875816481162003e-06

