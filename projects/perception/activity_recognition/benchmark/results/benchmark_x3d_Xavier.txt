/home/maleci/.local/lib/python3.6/site-packages/pkg_resources/__init__.py:119: PkgResourcesDeprecationWarning: 0.18ubuntu0.18.04.1 is an invalid version and will not be supported in a future release
  PkgResourcesDeprecationWarning,
/home/maleci/.local/lib/python3.6/site-packages/torchvision-0.10.0-py3.6-linux-aarch64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The _functional_video module is deprecated. Please use the functional module instead.
  "The _functional_video module is deprecated. Please use the functional module instead."
/home/maleci/.local/lib/python3.6/site-packages/torchvision-0.10.0-py3.6-linux-aarch64.egg/torchvision/transforms/_transforms_video.py:26: UserWarning: The _transforms_video module is deprecated. Please use the transforms module instead.
  "The _transforms_video module is deprecated. Please use the transforms module instead."
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
==== Benchmarking X3DLearner (xs) ====
== Benchmarking learner.infer ==
Warming up with batch_size=32:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=32:  10%|█         | 1/10 [00:00<00:01,  7.81it/s]Warming up with batch_size=32:  20%|██        | 2/10 [00:00<00:00,  8.71it/s]Warming up with batch_size=32:  30%|███       | 3/10 [00:00<00:00,  9.17it/s]Warming up with batch_size=32:  50%|█████     | 5/10 [00:00<00:00,  9.63it/s]Warming up with batch_size=32:  70%|███████   | 7/10 [00:00<00:00,  9.85it/s]Warming up with batch_size=32:  90%|█████████ | 9/10 [00:00<00:00, 10.09it/s]Warming up with batch_size=32: 100%|██████████| 10/10 [00:01<00:00,  9.84it/s]
Measuring inference for batch_size=32:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference for batch_size=32:   2%|▏         | 2/100 [00:00<00:09, 10.49it/s]Measuring inference for batch_size=32:   4%|▍         | 4/100 [00:00<00:08, 10.72it/s]Measuring inference for batch_size=32:   6%|▌         | 6/100 [00:00<00:08, 10.65it/s]Measuring inference for batch_size=32:   8%|▊         | 8/100 [00:00<00:08, 10.68it/s]Measuring inference for batch_size=32:  10%|█         | 10/100 [00:00<00:08, 10.78it/s]Measuring inference for batch_size=32:  12%|█▏        | 12/100 [00:01<00:08, 10.79it/s]Measuring inference for batch_size=32:  14%|█▍        | 14/100 [00:01<00:07, 10.78it/s]Measuring inference for batch_size=32:  16%|█▌        | 16/100 [00:01<00:07, 10.82it/s]Measuring inference for batch_size=32:  18%|█▊        | 18/100 [00:01<00:07, 10.77it/s]Measuring inference for batch_size=32:  20%|██        | 20/100 [00:01<00:07, 10.92it/s]Measuring inference for batch_size=32:  22%|██▏       | 22/100 [00:02<00:07, 10.93it/s]Measuring inference for batch_size=32:  24%|██▍       | 24/100 [00:02<00:06, 10.92it/s]Measuring inference for batch_size=32:  26%|██▌       | 26/100 [00:02<00:06, 11.07it/s]Measuring inference for batch_size=32:  28%|██▊       | 28/100 [00:02<00:06, 10.88it/s]Measuring inference for batch_size=32:  30%|███       | 30/100 [00:02<00:06, 10.73it/s]Measuring inference for batch_size=32:  32%|███▏      | 32/100 [00:02<00:06, 10.72it/s]Measuring inference for batch_size=32:  34%|███▍      | 34/100 [00:03<00:06, 10.65it/s]Measuring inference for batch_size=32:  36%|███▌      | 36/100 [00:03<00:05, 10.78it/s]Measuring inference for batch_size=32:  38%|███▊      | 38/100 [00:03<00:05, 10.98it/s]Measuring inference for batch_size=32:  40%|████      | 40/100 [00:03<00:05, 11.13it/s]Measuring inference for batch_size=32:  42%|████▏     | 42/100 [00:03<00:05, 11.32it/s]Measuring inference for batch_size=32:  44%|████▍     | 44/100 [00:04<00:04, 11.46it/s]Measuring inference for batch_size=32:  46%|████▌     | 46/100 [00:04<00:04, 11.36it/s]Measuring inference for batch_size=32:  48%|████▊     | 48/100 [00:04<00:04, 11.51it/s]Measuring inference for batch_size=32:  50%|█████     | 50/100 [00:04<00:04, 11.50it/s]Measuring inference for batch_size=32:  52%|█████▏    | 52/100 [00:04<00:04, 11.46it/s]Measuring inference for batch_size=32:  54%|█████▍    | 54/100 [00:04<00:03, 11.57it/s]Measuring inference for batch_size=32:  56%|█████▌    | 56/100 [00:05<00:03, 11.72it/s]Measuring inference for batch_size=32:  58%|█████▊    | 58/100 [00:05<00:03, 11.71it/s]Measuring inference for batch_size=32:  60%|██████    | 60/100 [00:05<00:03, 11.29it/s]Measuring inference for batch_size=32:  62%|██████▏   | 62/100 [00:05<00:03, 11.26it/s]Measuring inference for batch_size=32:  64%|██████▍   | 64/100 [00:05<00:03, 11.14it/s]Measuring inference for batch_size=32:  66%|██████▌   | 66/100 [00:05<00:03, 11.14it/s]Measuring inference for batch_size=32:  68%|██████▊   | 68/100 [00:06<00:02, 11.31it/s]Measuring inference for batch_size=32:  70%|███████   | 70/100 [00:06<00:02, 11.25it/s]Measuring inference for batch_size=32:  72%|███████▏  | 72/100 [00:06<00:02, 11.13it/s]Measuring inference for batch_size=32:  74%|███████▍  | 74/100 [00:06<00:02, 11.29it/s]Measuring inference for batch_size=32:  76%|███████▌  | 76/100 [00:06<00:02, 10.89it/s]Measuring inference for batch_size=32:  78%|███████▊  | 78/100 [00:07<00:02, 10.73it/s]Measuring inference for batch_size=32:  80%|████████  | 80/100 [00:07<00:01, 10.84it/s]Measuring inference for batch_size=32:  82%|████████▏ | 82/100 [00:07<00:01, 10.96it/s]Measuring inference for batch_size=32:  84%|████████▍ | 84/100 [00:07<00:01, 11.06it/s]Measuring inference for batch_size=32:  86%|████████▌ | 86/100 [00:07<00:01, 11.10it/s]Measuring inference for batch_size=32:  88%|████████▊ | 88/100 [00:07<00:01, 11.07it/s]Measuring inference for batch_size=32:  90%|█████████ | 90/100 [00:08<00:00, 11.11it/s]Measuring inference for batch_size=32:  92%|█████████▏| 92/100 [00:08<00:00, 11.26it/s]Measuring inference for batch_size=32:  94%|█████████▍| 94/100 [00:08<00:00, 11.40it/s]Measuring inference for batch_size=32:  96%|█████████▌| 96/100 [00:08<00:00, 11.51it/s]Measuring inference for batch_size=32:  98%|█████████▊| 98/100 [00:08<00:00, 11.42it/s]Measuring inference for batch_size=32: 100%|██████████| 100/100 [00:08<00:00, 11.48it/s]Measuring inference for batch_size=32: 100%|██████████| 100/100 [00:08<00:00, 11.12it/s]
Measuring energy for batch_size=1:   0%|          | 0/10 [00:00<?, ?it/s]Measuring energy for batch_size=1:  10%|█         | 1/10 [00:00<00:01,  5.77it/s]Measuring energy for batch_size=1:  20%|██        | 2/10 [00:00<00:01,  6.89it/s]Measuring energy for batch_size=1:  30%|███       | 3/10 [00:00<00:00,  7.33it/s]Measuring energy for batch_size=1:  40%|████      | 4/10 [00:00<00:00,  7.52it/s]Measuring energy for batch_size=1:  50%|█████     | 5/10 [00:00<00:00,  7.64it/s]Measuring energy for batch_size=1:  60%|██████    | 6/10 [00:00<00:00,  7.74it/s]Measuring energy for batch_size=1:  70%|███████   | 7/10 [00:00<00:00,  7.81it/s]Measuring energy for batch_size=1:  80%|████████  | 8/10 [00:01<00:00,  7.86it/s]Measuring energy for batch_size=1:  90%|█████████ | 9/10 [00:01<00:00,  7.79it/s]Measuring energy for batch_size=1: 100%|██████████| 10/10 [00:01<00:00,  7.85it/s]Measuring energy for batch_size=1: 100%|██████████| 10/10 [00:01<00:00,  7.61it/s]
Warming up with batch_size=32:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=32:  10%|█         | 1/10 [00:00<00:07,  1.21it/s]Warming up with batch_size=32:  20%|██        | 2/10 [00:01<00:06,  1.19it/s]Warming up with batch_size=32:  30%|███       | 3/10 [00:02<00:05,  1.21it/s]Warming up with batch_size=32:  40%|████      | 4/10 [00:03<00:05,  1.15it/s]Warming up with batch_size=32:  50%|█████     | 5/10 [00:04<00:04,  1.11it/s]Warming up with batch_size=32:  60%|██████    | 6/10 [00:05<00:03,  1.14it/s]Warming up with batch_size=32:  70%|███████   | 7/10 [00:06<00:02,  1.16it/s]Warming up with batch_size=32:  80%|████████  | 8/10 [00:06<00:01,  1.16it/s]Warming up with batch_size=32:  90%|█████████ | 9/10 [00:07<00:00,  1.16it/s]Warming up with batch_size=32: 100%|██████████| 10/10 [00:08<00:00,  1.10it/s]Warming up with batch_size=32: 100%|██████████| 10/10 [00:08<00:00,  1.14it/s]
Measuring inference for batch_size=32:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference for batch_size=32:   1%|          | 1/100 [00:00<01:37,  1.01it/s]Measuring inference for batch_size=32:   2%|▏         | 2/100 [00:01<01:33,  1.05it/s]Measuring inference for batch_size=32:   3%|▎         | 3/100 [00:02<01:28,  1.09it/s]Measuring inference for batch_size=32:   4%|▍         | 4/100 [00:03<01:28,  1.08it/s]Measuring inference for batch_size=32:   5%|▌         | 5/100 [00:04<01:24,  1.12it/s]Measuring inference for batch_size=32:   6%|▌         | 6/100 [00:05<01:21,  1.15it/s]Measuring inference for batch_size=32:   7%|▋         | 7/100 [00:06<01:19,  1.17it/s]Measuring inference for batch_size=32:   8%|▊         | 8/100 [00:07<01:18,  1.17it/s]Measuring inference for batch_size=32:   9%|▉         | 9/100 [00:07<01:16,  1.19it/s]Measuring inference for batch_size=32:  10%|█         | 10/100 [00:08<01:15,  1.19it/s]Measuring inference for batch_size=32:  11%|█         | 11/100 [00:09<01:15,  1.19it/s]Measuring inference for batch_size=32:  12%|█▏        | 12/100 [00:10<01:13,  1.19it/s]Measuring inference for batch_size=32:  13%|█▎        | 13/100 [00:11<01:12,  1.19it/s]Measuring inference for batch_size=32:  14%|█▍        | 14/100 [00:12<01:11,  1.20it/s]Measuring inference for batch_size=32:  15%|█▌        | 15/100 [00:12<01:11,  1.18it/s]Measuring inference for batch_size=32:  16%|█▌        | 16/100 [00:13<01:11,  1.18it/s]Measuring inference for batch_size=32:  17%|█▋        | 17/100 [00:14<01:09,  1.19it/s]Measuring inference for batch_size=32:  18%|█▊        | 18/100 [00:15<01:10,  1.17it/s]Measuring inference for batch_size=32:  19%|█▉        | 19/100 [00:16<01:08,  1.18it/s]Measuring inference for batch_size=32:  20%|██        | 20/100 [00:17<01:10,  1.14it/s]Measuring inference for batch_size=32:  21%|██        | 21/100 [00:18<01:10,  1.12it/s]Measuring inference for batch_size=32:  22%|██▏       | 22/100 [00:19<01:09,  1.12it/s]Measuring inference for batch_size=32:  23%|██▎       | 23/100 [00:20<01:09,  1.10it/s]Measuring inference for batch_size=32:  24%|██▍       | 24/100 [00:21<01:11,  1.07it/s]Measuring inference for batch_size=32:  25%|██▌       | 25/100 [00:22<01:11,  1.05it/s]Measuring inference for batch_size=32:  26%|██▌       | 26/100 [00:22<01:09,  1.06it/s]Measuring inference for batch_size=32:  27%|██▋       | 27/100 [00:23<01:09,  1.05it/s]Measuring inference for batch_size=32:  28%|██▊       | 28/100 [00:24<01:06,  1.09it/s]Measuring inference for batch_size=32:  29%|██▉       | 29/100 [00:25<01:05,  1.09it/s]Measuring inference for batch_size=32:  30%|███       | 30/100 [00:26<01:03,  1.10it/s]Measuring inference for batch_size=32:  31%|███       | 31/100 [00:27<01:02,  1.10it/s]Measuring inference for batch_size=32:  32%|███▏      | 32/100 [00:28<01:02,  1.09it/s]Measuring inference for batch_size=32:  33%|███▎      | 33/100 [00:29<00:59,  1.12it/s]Measuring inference for batch_size=32:  34%|███▍      | 34/100 [00:30<00:57,  1.15it/s]Measuring inference for batch_size=32:  35%|███▌      | 35/100 [00:30<00:55,  1.17it/s]Measuring inference for batch_size=32:  36%|███▌      | 36/100 [00:31<00:53,  1.19it/s]Measuring inference for batch_size=32:  37%|███▋      | 37/100 [00:32<00:52,  1.20it/s]Measuring inference for batch_size=32:  38%|███▊      | 38/100 [00:33<00:51,  1.21it/s]Measuring inference for batch_size=32:  39%|███▉      | 39/100 [00:34<00:50,  1.21it/s]Measuring inference for batch_size=32:  40%|████      | 40/100 [00:34<00:49,  1.21it/s]Measuring inference for batch_size=32:  41%|████      | 41/100 [00:35<00:48,  1.21it/s]Measuring inference for batch_size=32:  42%|████▏     | 42/100 [00:36<00:47,  1.22it/s]Measuring inference for batch_size=32:  43%|████▎     | 43/100 [00:37<00:46,  1.21it/s]Measuring inference for batch_size=32:  44%|████▍     | 44/100 [00:38<00:45,  1.22it/s]Measuring inference for batch_size=32:  45%|████▌     | 45/100 [00:39<00:45,  1.22it/s]Measuring inference for batch_size=32:  46%|████▌     | 46/100 [00:39<00:44,  1.22it/s]Measuring inference for batch_size=32:  47%|████▋     | 47/100 [00:40<00:43,  1.22it/s]Measuring inference for batch_size=32:  48%|████▊     | 48/100 [00:41<00:42,  1.22it/s]Measuring inference for batch_size=32:  49%|████▉     | 49/100 [00:42<00:41,  1.22it/s]Measuring inference for batch_size=32:  50%|█████     | 50/100 [00:43<00:40,  1.22it/s]Measuring inference for batch_size=32:  51%|█████     | 51/100 [00:43<00:40,  1.22it/s]Measuring inference for batch_size=32:  52%|█████▏    | 52/100 [00:44<00:39,  1.22it/s]Measuring inference for batch_size=32:  53%|█████▎    | 53/100 [00:45<00:38,  1.22it/s]Measuring inference for batch_size=32:  54%|█████▍    | 54/100 [00:46<00:37,  1.22it/s]Measuring inference for batch_size=32:  55%|█████▌    | 55/100 [00:47<00:36,  1.23it/s]Measuring inference for batch_size=32:  56%|█████▌    | 56/100 [00:48<00:35,  1.23it/s]Measuring inference for batch_size=32:  57%|█████▋    | 57/100 [00:48<00:35,  1.23it/s]Measuring inference for batch_size=32:  58%|█████▊    | 58/100 [00:49<00:34,  1.22it/s]Measuring inference for batch_size=32:  59%|█████▉    | 59/100 [00:50<00:33,  1.22it/s]Measuring inference for batch_size=32:  60%|██████    | 60/100 [00:51<00:32,  1.22it/s]Measuring inference for batch_size=32:  61%|██████    | 61/100 [00:52<00:31,  1.22it/s]Measuring inference for batch_size=32:  62%|██████▏   | 62/100 [00:52<00:31,  1.22it/s]Measuring inference for batch_size=32:  63%|██████▎   | 63/100 [00:53<00:30,  1.22it/s]Measuring inference for batch_size=32:  64%|██████▍   | 64/100 [00:54<00:29,  1.22it/s]Measuring inference for batch_size=32:  65%|██████▌   | 65/100 [00:55<00:28,  1.22it/s]Measuring inference for batch_size=32:  66%|██████▌   | 66/100 [00:56<00:28,  1.21it/s]Measuring inference for batch_size=32:  67%|██████▋   | 67/100 [00:57<00:27,  1.21it/s]Measuring inference for batch_size=32:  68%|██████▊   | 68/100 [00:57<00:26,  1.22it/s]Measuring inference for batch_size=32:  69%|██████▉   | 69/100 [00:58<00:25,  1.22it/s]Measuring inference for batch_size=32:  70%|███████   | 70/100 [00:59<00:24,  1.22it/s]Measuring inference for batch_size=32:  71%|███████   | 71/100 [01:00<00:23,  1.22it/s]Measuring inference for batch_size=32:  72%|███████▏  | 72/100 [01:01<00:23,  1.22it/s]Measuring inference for batch_size=32:  73%|███████▎  | 73/100 [01:01<00:22,  1.22it/s]Measuring inference for batch_size=32:  74%|███████▍  | 74/100 [01:02<00:21,  1.22it/s]Measuring inference for batch_size=32:  75%|███████▌  | 75/100 [01:03<00:20,  1.22it/s]Measuring inference for batch_size=32:  76%|███████▌  | 76/100 [01:04<00:19,  1.22it/s]Measuring inference for batch_size=32:  77%|███████▋  | 77/100 [01:05<00:18,  1.22it/s]Measuring inference for batch_size=32:  78%|███████▊  | 78/100 [01:06<00:18,  1.22it/s]Measuring inference for batch_size=32:  79%|███████▉  | 79/100 [01:06<00:17,  1.22it/s]Measuring inference for batch_size=32:  80%|████████  | 80/100 [01:07<00:16,  1.22it/s]Measuring inference for batch_size=32:  81%|████████  | 81/100 [01:08<00:15,  1.22it/s]Measuring inference for batch_size=32:  82%|████████▏ | 82/100 [01:09<00:14,  1.22it/s]Measuring inference for batch_size=32:  83%|████████▎ | 83/100 [01:10<00:13,  1.22it/s]Measuring inference for batch_size=32:  84%|████████▍ | 84/100 [01:10<00:13,  1.22it/s]Measuring inference for batch_size=32:  85%|████████▌ | 85/100 [01:11<00:12,  1.22it/s]Measuring inference for batch_size=32:  86%|████████▌ | 86/100 [01:12<00:11,  1.23it/s]Measuring inference for batch_size=32:  87%|████████▋ | 87/100 [01:13<00:10,  1.23it/s]Measuring inference for batch_size=32:  88%|████████▊ | 88/100 [01:14<00:09,  1.23it/s]Measuring inference for batch_size=32:  89%|████████▉ | 89/100 [01:15<00:08,  1.23it/s]Measuring inference for batch_size=32:  90%|█████████ | 90/100 [01:15<00:08,  1.23it/s]Measuring inference for batch_size=32:  91%|█████████ | 91/100 [01:16<00:07,  1.23it/s]Measuring inference for batch_size=32:  92%|█████████▏| 92/100 [01:17<00:06,  1.23it/s]Measuring inference for batch_size=32:  93%|█████████▎| 93/100 [01:18<00:05,  1.23it/s]Measuring inference for batch_size=32:  94%|█████████▍| 94/100 [01:19<00:04,  1.23it/s]Measuring inference for batch_size=32:  95%|█████████▌| 95/100 [01:19<00:04,  1.22it/s]Measuring inference for batch_size=32:  96%|█████████▌| 96/100 [01:20<00:03,  1.22it/s]Measuring inference for batch_size=32:  97%|█████████▋| 97/100 [01:21<00:02,  1.22it/s]Measuring inference for batch_size=32:  98%|█████████▊| 98/100 [01:22<00:01,  1.22it/s]Measuring inference for batch_size=32:  99%|█████████▉| 99/100 [01:23<00:00,  1.23it/s]Measuring inference for batch_size=32: 100%|██████████| 100/100 [01:24<00:00,  1.23it/s]Measuring inference for batch_size=32: 100%|██████████| 100/100 [01:24<00:00,  1.19it/s]
Measuring energy for batch_size=32:   0%|          | 0/10 [00:00<?, ?it/s]Measuring energy for batch_size=32:  10%|█         | 1/10 [00:01<00:09,  1.06s/it]Measuring energy for batch_size=32:  20%|██        | 2/10 [00:01<00:07,  1.07it/s]Measuring energy for batch_size=32:  30%|███       | 3/10 [00:02<00:06,  1.12it/s]Measuring energy for batch_size=32:  40%|████      | 4/10 [00:03<00:05,  1.14it/s]Measuring energy for batch_size=32:  50%|█████     | 5/10 [00:04<00:04,  1.16it/s]Measuring energy for batch_size=32:  60%|██████    | 6/10 [00:05<00:03,  1.17it/s]Measuring energy for batch_size=32:  70%|███████   | 7/10 [00:06<00:02,  1.17it/s]Measuring energy for batch_size=32:  80%|████████  | 8/10 [00:06<00:01,  1.17it/s]Measuring energy for batch_size=32:  90%|█████████ | 9/10 [00:07<00:00,  1.18it/s]Measuring energy for batch_size=32: 100%|██████████| 10/10 [00:08<00:00,  1.18it/s]Measuring energy for batch_size=32: 100%|██████████| 10/10 [00:08<00:00,  1.15it/s]
learner.infer:
  device: cuda
  energy:
    batch_size_1:
      joules: 1.449205931030909
      kWh: 4.0255720306414135e-07
    batch_size_32:
      joules: 24.609684791680973
      kWh: 6.836023553244715e-06
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 26.24 GB
      total: 31.17 GB
      used: 4.50 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: 11.587 us +/- 9.154 us [3.576 us, 69.380 us]
          batches_per_second: 128.29 K +/- 75.27 K [14.41 K, 279.62 K]
        metrics:
          batches_per_second_max: 279620.26666666666
          batches_per_second_mean: 128292.98517157439
          batches_per_second_min: 14413.415807560137
          batches_per_second_std: 75272.91432498021
          seconds_per_batch_max: 6.937980651855469e-05
          seconds_per_batch_mean: 1.1587142944335938e-05
          seconds_per_batch_min: 3.5762786865234375e-06
          seconds_per_batch_std: 9.154217877731577e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: 231.543 us +/- 117.468 us [175.238 us, 1.348 ms]
          batches_per_second: 4.59 K +/- 688.39 [741.57, 5.71 K]
        metrics:
          batches_per_second_max: 5706.536054421768
          batches_per_second_mean: 4588.339612531452
          batches_per_second_min: 741.5671852899576
          batches_per_second_std: 688.3863315085755
          seconds_per_batch_max: 0.0013484954833984375
          seconds_per_batch_mean: 0.00023154258728027343
          seconds_per_batch_min: 0.00017523765563964844
          seconds_per_batch_std: 0.00011746800751237647
      on_device_inference:
        human_readable:
          batch_latency: 89.252 ms +/- 4.277 ms [81.868 ms, 101.145 ms]
          batches_per_second: 11.23 +/- 0.53 [9.89, 12.21]
        metrics:
          batches_per_second_max: 12.214759158949269
          batches_per_second_mean: 11.22961337498667
          batches_per_second_min: 9.886816647494767
          batches_per_second_std: 0.5298903295569116
          seconds_per_batch_max: 0.10114479064941406
          seconds_per_batch_mean: 0.08925171375274658
          seconds_per_batch_min: 0.08186817169189453
          seconds_per_batch_std: 0.004277321237511269
      total:
        human_readable:
          batch_latency: 89.495 ms +/- 4.292 ms [82.094 ms, 101.399 ms]
          batches_per_second: 11.20 +/- 0.53 [9.86, 12.18]
        metrics:
          batches_per_second_max: 12.181129620594318
          batches_per_second_mean: 11.199156249146847
          batches_per_second_min: 9.861989184105337
          batches_per_second_std: 0.5290938724468057
          seconds_per_batch_max: 0.10139942169189453
          seconds_per_batch_mean: 0.0894948434829712
          seconds_per_batch_min: 0.08209419250488281
          seconds_per_batch_std: 0.004292265555061541
    batch_size_32:
      cpu_to_gpu:
        human_readable:
          batch_latency: 10.962 us +/- 3.928 us [6.676 us, 33.379 us]
          batches_per_second: 99.66 K +/- 26.32 K [29.96 K, 149.80 K]
        metrics:
          batches_per_second_max: 149796.57142857142
          batches_per_second_mean: 99656.11067415793
          batches_per_second_min: 29959.314285714285
          batches_per_second_std: 26318.15859733731
          seconds_per_batch_max: 3.337860107421875e-05
          seconds_per_batch_mean: 1.0962486267089843e-05
          seconds_per_batch_min: 6.67572021484375e-06
          seconds_per_batch_std: 3.9276130503244e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: 3.477 ms +/- 664.933 us [2.464 ms, 7.365 ms]
          batches_per_second: 295.03 +/- 41.64 [135.78, 405.80]
        metrics:
          batches_per_second_max: 405.79566563467495
          batches_per_second_mean: 295.03251653490867
          batches_per_second_min: 135.78193590158628
          batches_per_second_std: 41.63690066918157
          seconds_per_batch_max: 0.007364749908447266
          seconds_per_batch_mean: 0.003476979732513428
          seconds_per_batch_min: 0.00246429443359375
          seconds_per_batch_std: 0.0006649331845303904
      on_device_inference:
        human_readable:
          batch_latency: 836.024 ms +/- 45.861 ms [806.102 ms, 1.002 s]
          batches_per_second: 1.20 +/- 0.06 [1.00, 1.24]
        metrics:
          batches_per_second_max: 1.2405381072546995
          batches_per_second_mean: 1.199375219447435
          batches_per_second_min: 0.9980848819898952
          batches_per_second_std: 0.05907605313636828
          seconds_per_batch_max: 1.0019187927246094
          seconds_per_batch_mean: 0.8360235142707825
          seconds_per_batch_min: 0.8061017990112305
          seconds_per_batch_std: 0.045861311675717574
      total:
        human_readable:
          batch_latency: 839.511 ms +/- 46.000 ms [809.585 ms, 1.008 s]
          batches_per_second: 1.19 +/- 0.06 [0.99, 1.24]
        metrics:
          batches_per_second_max: 1.2352002431348135
          batches_per_second_mean: 1.1943843665363574
          batches_per_second_min: 0.992087528230106
          batches_per_second_std: 0.05875668354760408
          seconds_per_batch_max: 1.0079755783081055
          seconds_per_batch_mean: 0.8395114564895629
          seconds_per_batch_min: 0.8095853328704834
          seconds_per_batch_std: 0.046000313786927094

== Benchmarking model directly ==
Warming up with batch_size=32:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=32:  20%|██        | 2/10 [00:00<00:00, 13.07it/s]Warming up with batch_size=32:  40%|████      | 4/10 [00:00<00:00, 13.19it/s]Warming up with batch_size=32:  60%|██████    | 6/10 [00:00<00:00, 13.21it/s]Warming up with batch_size=32:  80%|████████  | 8/10 [00:00<00:00, 13.27it/s]Warming up with batch_size=32: 100%|██████████| 10/10 [00:00<00:00, 13.31it/s]Warming up with batch_size=32: 100%|██████████| 10/10 [00:00<00:00, 13.24it/s]
Measuring inference for batch_size=32:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference for batch_size=32:   2%|▏         | 2/100 [00:00<00:07, 13.45it/s]Measuring inference for batch_size=32:   4%|▍         | 4/100 [00:00<00:07, 13.38it/s]Measuring inference for batch_size=32:   6%|▌         | 6/100 [00:00<00:07, 13.37it/s]Measuring inference for batch_size=32:   8%|▊         | 8/100 [00:00<00:06, 13.33it/s]Measuring inference for batch_size=32:  10%|█         | 10/100 [00:00<00:06, 13.33it/s]Measuring inference for batch_size=32:  12%|█▏        | 12/100 [00:00<00:06, 13.27it/s]Measuring inference for batch_size=32:  14%|█▍        | 14/100 [00:01<00:06, 13.29it/s]Measuring inference for batch_size=32:  16%|█▌        | 16/100 [00:01<00:06, 13.27it/s]Measuring inference for batch_size=32:  18%|█▊        | 18/100 [00:01<00:06, 13.25it/s]Measuring inference for batch_size=32:  20%|██        | 20/100 [00:01<00:06, 13.26it/s]Measuring inference for batch_size=32:  22%|██▏       | 22/100 [00:01<00:06, 12.98it/s]Measuring inference for batch_size=32:  24%|██▍       | 24/100 [00:01<00:05, 12.72it/s]Measuring inference for batch_size=32:  26%|██▌       | 26/100 [00:01<00:05, 12.66it/s]Measuring inference for batch_size=32:  28%|██▊       | 28/100 [00:02<00:05, 12.64it/s]Measuring inference for batch_size=32:  30%|███       | 30/100 [00:02<00:05, 12.54it/s]Measuring inference for batch_size=32:  32%|███▏      | 32/100 [00:02<00:05, 12.64it/s]Measuring inference for batch_size=32:  34%|███▍      | 34/100 [00:02<00:05, 12.59it/s]Measuring inference for batch_size=32:  36%|███▌      | 36/100 [00:02<00:05, 12.61it/s]Measuring inference for batch_size=32:  38%|███▊      | 38/100 [00:02<00:04, 12.63it/s]Measuring inference for batch_size=32:  40%|████      | 40/100 [00:03<00:04, 12.52it/s]Measuring inference for batch_size=32:  42%|████▏     | 42/100 [00:03<00:04, 12.44it/s]Measuring inference for batch_size=32:  44%|████▍     | 44/100 [00:03<00:04, 12.54it/s]Measuring inference for batch_size=32:  46%|████▌     | 46/100 [00:03<00:04, 12.52it/s]Measuring inference for batch_size=32:  48%|████▊     | 48/100 [00:03<00:04, 12.79it/s]Measuring inference for batch_size=32:  50%|█████     | 50/100 [00:03<00:03, 12.77it/s]Measuring inference for batch_size=32:  52%|█████▏    | 52/100 [00:04<00:03, 12.69it/s]Measuring inference for batch_size=32:  54%|█████▍    | 54/100 [00:04<00:03, 12.66it/s]Measuring inference for batch_size=32:  56%|█████▌    | 56/100 [00:04<00:03, 12.73it/s]Measuring inference for batch_size=32:  58%|█████▊    | 58/100 [00:04<00:03, 12.81it/s]Measuring inference for batch_size=32:  60%|██████    | 60/100 [00:04<00:03, 12.89it/s]Measuring inference for batch_size=32:  62%|██████▏   | 62/100 [00:04<00:02, 13.02it/s]Measuring inference for batch_size=32:  64%|██████▍   | 64/100 [00:04<00:02, 13.13it/s]Measuring inference for batch_size=32:  66%|██████▌   | 66/100 [00:05<00:02, 13.27it/s]Measuring inference for batch_size=32:  68%|██████▊   | 68/100 [00:05<00:02, 13.30it/s]Measuring inference for batch_size=32:  70%|███████   | 70/100 [00:05<00:02, 13.37it/s]Measuring inference for batch_size=32:  72%|███████▏  | 72/100 [00:05<00:02, 13.44it/s]Measuring inference for batch_size=32:  74%|███████▍  | 74/100 [00:05<00:01, 13.48it/s]Measuring inference for batch_size=32:  76%|███████▌  | 76/100 [00:05<00:01, 13.48it/s]Measuring inference for batch_size=32:  78%|███████▊  | 78/100 [00:06<00:01, 13.48it/s]Measuring inference for batch_size=32:  80%|████████  | 80/100 [00:06<00:01, 13.52it/s]Measuring inference for batch_size=32:  82%|████████▏ | 82/100 [00:06<00:01, 13.53it/s]Measuring inference for batch_size=32:  84%|████████▍ | 84/100 [00:06<00:01, 13.42it/s]Measuring inference for batch_size=32:  86%|████████▌ | 86/100 [00:06<00:01, 13.45it/s]Measuring inference for batch_size=32:  88%|████████▊ | 88/100 [00:06<00:00, 13.42it/s]Measuring inference for batch_size=32:  90%|█████████ | 90/100 [00:06<00:00, 13.37it/s]Measuring inference for batch_size=32:  92%|█████████▏| 92/100 [00:07<00:00, 13.43it/s]Measuring inference for batch_size=32:  94%|█████████▍| 94/100 [00:07<00:00, 13.50it/s]Measuring inference for batch_size=32:  96%|█████████▌| 96/100 [00:07<00:00, 13.55it/s]Measuring inference for batch_size=32:  98%|█████████▊| 98/100 [00:07<00:00, 13.43it/s]Measuring inference for batch_size=32: 100%|██████████| 100/100 [00:07<00:00, 13.31it/s]Measuring inference for batch_size=32: 100%|██████████| 100/100 [00:07<00:00, 13.07it/s]
Measuring energy for batch_size=1:   0%|          | 0/10 [00:00<?, ?it/s]Measuring energy for batch_size=1:  10%|█         | 1/10 [00:00<00:01,  5.68it/s]Measuring energy for batch_size=1:  20%|██        | 2/10 [00:00<00:01,  6.85it/s]Measuring energy for batch_size=1:  30%|███       | 3/10 [00:00<00:00,  7.33it/s]Measuring energy for batch_size=1:  40%|████      | 4/10 [00:00<00:00,  7.59it/s]Measuring energy for batch_size=1:  50%|█████     | 5/10 [00:00<00:00,  7.72it/s]Measuring energy for batch_size=1:  60%|██████    | 6/10 [00:00<00:00,  7.82it/s]Measuring energy for batch_size=1:  70%|███████   | 7/10 [00:00<00:00,  7.87it/s]Measuring energy for batch_size=1:  80%|████████  | 8/10 [00:01<00:00,  7.92it/s]Measuring energy for batch_size=1:  90%|█████████ | 9/10 [00:01<00:00,  7.93it/s]Measuring energy for batch_size=1: 100%|██████████| 10/10 [00:01<00:00,  7.92it/s]Measuring energy for batch_size=1: 100%|██████████| 10/10 [00:01<00:00,  7.67it/s]
Warming up with batch_size=32:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=32:  10%|█         | 1/10 [00:00<00:07,  1.24it/s]Warming up with batch_size=32:  20%|██        | 2/10 [00:01<00:06,  1.27it/s]Warming up with batch_size=32:  30%|███       | 3/10 [00:02<00:05,  1.28it/s]Warming up with batch_size=32:  40%|████      | 4/10 [00:03<00:04,  1.28it/s]Warming up with batch_size=32:  50%|█████     | 5/10 [00:03<00:03,  1.28it/s]Warming up with batch_size=32:  60%|██████    | 6/10 [00:04<00:03,  1.28it/s]Warming up with batch_size=32:  70%|███████   | 7/10 [00:05<00:02,  1.29it/s]Warming up with batch_size=32:  80%|████████  | 8/10 [00:06<00:01,  1.29it/s]Warming up with batch_size=32:  90%|█████████ | 9/10 [00:07<00:00,  1.29it/s]Warming up with batch_size=32: 100%|██████████| 10/10 [00:07<00:00,  1.29it/s]Warming up with batch_size=32: 100%|██████████| 10/10 [00:07<00:00,  1.28it/s]
Measuring inference for batch_size=32:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference for batch_size=32:   1%|          | 1/100 [00:00<01:16,  1.29it/s]Measuring inference for batch_size=32:   2%|▏         | 2/100 [00:01<01:16,  1.29it/s]Measuring inference for batch_size=32:   3%|▎         | 3/100 [00:02<01:15,  1.29it/s]Measuring inference for batch_size=32:   4%|▍         | 4/100 [00:03<01:14,  1.29it/s]Measuring inference for batch_size=32:   5%|▌         | 5/100 [00:03<01:13,  1.29it/s]Measuring inference for batch_size=32:   6%|▌         | 6/100 [00:04<01:12,  1.29it/s]Measuring inference for batch_size=32:   7%|▋         | 7/100 [00:05<01:12,  1.29it/s]Measuring inference for batch_size=32:   8%|▊         | 8/100 [00:06<01:11,  1.29it/s]Measuring inference for batch_size=32:   9%|▉         | 9/100 [00:06<01:10,  1.29it/s]Measuring inference for batch_size=32:  10%|█         | 10/100 [00:07<01:09,  1.29it/s]Measuring inference for batch_size=32:  11%|█         | 11/100 [00:08<01:09,  1.29it/s]Measuring inference for batch_size=32:  12%|█▏        | 12/100 [00:09<01:08,  1.29it/s]Measuring inference for batch_size=32:  13%|█▎        | 13/100 [00:10<01:07,  1.29it/s]Measuring inference for batch_size=32:  14%|█▍        | 14/100 [00:10<01:06,  1.29it/s]Measuring inference for batch_size=32:  15%|█▌        | 15/100 [00:11<01:06,  1.29it/s]Measuring inference for batch_size=32:  16%|█▌        | 16/100 [00:12<01:05,  1.29it/s]Measuring inference for batch_size=32:  17%|█▋        | 17/100 [00:13<01:04,  1.29it/s]Measuring inference for batch_size=32:  18%|█▊        | 18/100 [00:13<01:03,  1.29it/s]Measuring inference for batch_size=32:  19%|█▉        | 19/100 [00:14<01:02,  1.29it/s]Measuring inference for batch_size=32:  20%|██        | 20/100 [00:15<01:02,  1.29it/s]Measuring inference for batch_size=32:  21%|██        | 21/100 [00:16<01:01,  1.29it/s]Measuring inference for batch_size=32:  22%|██▏       | 22/100 [00:17<01:00,  1.29it/s]Measuring inference for batch_size=32:  23%|██▎       | 23/100 [00:17<00:59,  1.29it/s]Measuring inference for batch_size=32:  24%|██▍       | 24/100 [00:18<00:58,  1.29it/s]Measuring inference for batch_size=32:  25%|██▌       | 25/100 [00:19<00:58,  1.29it/s]Measuring inference for batch_size=32:  26%|██▌       | 26/100 [00:20<00:57,  1.29it/s]Measuring inference for batch_size=32:  27%|██▋       | 27/100 [00:20<00:56,  1.29it/s]Measuring inference for batch_size=32:  28%|██▊       | 28/100 [00:21<00:55,  1.29it/s]Measuring inference for batch_size=32:  29%|██▉       | 29/100 [00:22<00:55,  1.29it/s]Measuring inference for batch_size=32:  30%|███       | 30/100 [00:23<00:54,  1.29it/s]Measuring inference for batch_size=32:  31%|███       | 31/100 [00:24<00:53,  1.29it/s]Measuring inference for batch_size=32:  32%|███▏      | 32/100 [00:24<00:52,  1.29it/s]Measuring inference for batch_size=32:  33%|███▎      | 33/100 [00:25<00:52,  1.29it/s]Measuring inference for batch_size=32:  34%|███▍      | 34/100 [00:26<00:51,  1.29it/s]Measuring inference for batch_size=32:  35%|███▌      | 35/100 [00:27<00:50,  1.29it/s]Measuring inference for batch_size=32:  36%|███▌      | 36/100 [00:27<00:49,  1.29it/s]Measuring inference for batch_size=32:  37%|███▋      | 37/100 [00:28<00:48,  1.29it/s]Measuring inference for batch_size=32:  38%|███▊      | 38/100 [00:29<00:48,  1.29it/s]Measuring inference for batch_size=32:  39%|███▉      | 39/100 [00:30<00:47,  1.29it/s]Measuring inference for batch_size=32:  40%|████      | 40/100 [00:31<00:46,  1.29it/s]Measuring inference for batch_size=32:  41%|████      | 41/100 [00:31<00:45,  1.29it/s]Measuring inference for batch_size=32:  42%|████▏     | 42/100 [00:32<00:45,  1.29it/s]Measuring inference for batch_size=32:  43%|████▎     | 43/100 [00:33<00:44,  1.29it/s]Measuring inference for batch_size=32:  44%|████▍     | 44/100 [00:34<00:43,  1.29it/s]Measuring inference for batch_size=32:  45%|████▌     | 45/100 [00:34<00:42,  1.29it/s]Measuring inference for batch_size=32:  46%|████▌     | 46/100 [00:35<00:41,  1.29it/s]Measuring inference for batch_size=32:  47%|████▋     | 47/100 [00:36<00:41,  1.29it/s]Measuring inference for batch_size=32:  48%|████▊     | 48/100 [00:37<00:40,  1.29it/s]Measuring inference for batch_size=32:  49%|████▉     | 49/100 [00:38<00:39,  1.29it/s]Measuring inference for batch_size=32:  50%|█████     | 50/100 [00:38<00:38,  1.29it/s]Measuring inference for batch_size=32:  51%|█████     | 51/100 [00:39<00:38,  1.29it/s]Measuring inference for batch_size=32:  52%|█████▏    | 52/100 [00:40<00:37,  1.29it/s]Measuring inference for batch_size=32:  53%|█████▎    | 53/100 [00:41<00:36,  1.29it/s]Measuring inference for batch_size=32:  54%|█████▍    | 54/100 [00:41<00:35,  1.29it/s]Measuring inference for batch_size=32:  55%|█████▌    | 55/100 [00:42<00:34,  1.29it/s]Measuring inference for batch_size=32:  56%|█████▌    | 56/100 [00:43<00:34,  1.29it/s]Measuring inference for batch_size=32:  57%|█████▋    | 57/100 [00:44<00:33,  1.29it/s]Measuring inference for batch_size=32:  58%|█████▊    | 58/100 [00:45<00:32,  1.29it/s]Measuring inference for batch_size=32:  59%|█████▉    | 59/100 [00:45<00:31,  1.29it/s]Measuring inference for batch_size=32:  60%|██████    | 60/100 [00:46<00:31,  1.29it/s]Measuring inference for batch_size=32:  61%|██████    | 61/100 [00:47<00:30,  1.29it/s]Measuring inference for batch_size=32:  62%|██████▏   | 62/100 [00:48<00:29,  1.29it/s]Measuring inference for batch_size=32:  63%|██████▎   | 63/100 [00:48<00:28,  1.29it/s]Measuring inference for batch_size=32:  64%|██████▍   | 64/100 [00:49<00:27,  1.29it/s]Measuring inference for batch_size=32:  65%|██████▌   | 65/100 [00:50<00:27,  1.29it/s]Measuring inference for batch_size=32:  66%|██████▌   | 66/100 [00:51<00:26,  1.29it/s]Measuring inference for batch_size=32:  67%|██████▋   | 67/100 [00:51<00:25,  1.29it/s]Measuring inference for batch_size=32:  68%|██████▊   | 68/100 [00:52<00:24,  1.29it/s]Measuring inference for batch_size=32:  69%|██████▉   | 69/100 [00:53<00:24,  1.29it/s]Measuring inference for batch_size=32:  70%|███████   | 70/100 [00:54<00:23,  1.29it/s]Measuring inference for batch_size=32:  71%|███████   | 71/100 [00:55<00:22,  1.29it/s]Measuring inference for batch_size=32:  72%|███████▏  | 72/100 [00:55<00:21,  1.29it/s]Measuring inference for batch_size=32:  73%|███████▎  | 73/100 [00:56<00:20,  1.29it/s]Measuring inference for batch_size=32:  74%|███████▍  | 74/100 [00:57<00:20,  1.29it/s]Measuring inference for batch_size=32:  75%|███████▌  | 75/100 [00:58<00:19,  1.29it/s]Measuring inference for batch_size=32:  76%|███████▌  | 76/100 [00:58<00:18,  1.29it/s]Measuring inference for batch_size=32:  77%|███████▋  | 77/100 [00:59<00:17,  1.29it/s]Measuring inference for batch_size=32:  78%|███████▊  | 78/100 [01:00<00:17,  1.29it/s]Measuring inference for batch_size=32:  79%|███████▉  | 79/100 [01:01<00:16,  1.29it/s]Measuring inference for batch_size=32:  80%|████████  | 80/100 [01:02<00:15,  1.29it/s]Measuring inference for batch_size=32:  81%|████████  | 81/100 [01:02<00:14,  1.29it/s]Measuring inference for batch_size=32:  82%|████████▏ | 82/100 [01:03<00:13,  1.29it/s]Measuring inference for batch_size=32:  83%|████████▎ | 83/100 [01:04<00:13,  1.29it/s]Measuring inference for batch_size=32:  84%|████████▍ | 84/100 [01:05<00:12,  1.29it/s]Measuring inference for batch_size=32:  85%|████████▌ | 85/100 [01:05<00:11,  1.29it/s]Measuring inference for batch_size=32:  86%|████████▌ | 86/100 [01:06<00:10,  1.29it/s]Measuring inference for batch_size=32:  87%|████████▋ | 87/100 [01:07<00:10,  1.29it/s]Measuring inference for batch_size=32:  88%|████████▊ | 88/100 [01:08<00:09,  1.29it/s]Measuring inference for batch_size=32:  89%|████████▉ | 89/100 [01:09<00:08,  1.29it/s]Measuring inference for batch_size=32:  90%|█████████ | 90/100 [01:09<00:07,  1.29it/s]Measuring inference for batch_size=32:  91%|█████████ | 91/100 [01:10<00:06,  1.29it/s]Measuring inference for batch_size=32:  92%|█████████▏| 92/100 [01:11<00:06,  1.29it/s]Measuring inference for batch_size=32:  93%|█████████▎| 93/100 [01:12<00:05,  1.29it/s]Measuring inference for batch_size=32:  94%|█████████▍| 94/100 [01:12<00:04,  1.29it/s]Measuring inference for batch_size=32:  95%|█████████▌| 95/100 [01:13<00:03,  1.29it/s]Measuring inference for batch_size=32:  96%|█████████▌| 96/100 [01:14<00:03,  1.29it/s]Measuring inference for batch_size=32:  97%|█████████▋| 97/100 [01:15<00:02,  1.29it/s]Measuring inference for batch_size=32:  98%|█████████▊| 98/100 [01:16<00:01,  1.29it/s]Measuring inference for batch_size=32:  99%|█████████▉| 99/100 [01:16<00:00,  1.29it/s]Measuring inference for batch_size=32: 100%|██████████| 100/100 [01:17<00:00,  1.29it/s]Measuring inference for batch_size=32: 100%|██████████| 100/100 [01:17<00:00,  1.29it/s]
Measuring energy for batch_size=32:   0%|          | 0/10 [00:00<?, ?it/s]Measuring energy for batch_size=32:  10%|█         | 1/10 [00:00<00:08,  1.12it/s]Measuring energy for batch_size=32:  20%|██        | 2/10 [00:01<00:06,  1.21it/s]Measuring energy for batch_size=32:  30%|███       | 3/10 [00:02<00:05,  1.23it/s]Measuring energy for batch_size=32:  40%|████      | 4/10 [00:03<00:04,  1.25it/s]Measuring energy for batch_size=32:  50%|█████     | 5/10 [00:04<00:03,  1.26it/s]Measuring energy for batch_size=32:  60%|██████    | 6/10 [00:04<00:03,  1.26it/s]Measuring energy for batch_size=32:  70%|███████   | 7/10 [00:05<00:02,  1.26it/s]Measuring energy for batch_size=32:  80%|████████  | 8/10 [00:06<00:01,  1.26it/s]Measuring energy for batch_size=32:  90%|█████████ | 9/10 [00:07<00:00,  1.27it/s]Measuring energy for batch_size=32: 100%|██████████| 10/10 [00:07<00:00,  1.27it/s]Measuring energy for batch_size=32: 100%|██████████| 10/10 [00:07<00:00,  1.25it/s]
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
learner.model.forward:
  device: cuda
  energy:
    batch_size_1:
      joules: 1.0760428494262695
      kWh: 2.989007915072971e-07
    batch_size_32:
      joules: 23.621138803798356
      kWh: 6.5614274454995435e-06
  flops: 635560544
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 21.00 GB
      total: 31.17 GB
      used: 9.74 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  params: 3794322
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: 330.935 us +/- 54.948 us [262.022 us, 566.959 us]
          batches_per_second: 3.09 K +/- 431.46 [1.76 K, 3.82 K]
        metrics:
          batches_per_second_max: 3816.4731574158327
          batches_per_second_mean: 3091.766274399645
          batches_per_second_min: 1763.7947855340622
          batches_per_second_std: 431.46469639787114
          seconds_per_batch_max: 0.0005669593811035156
          seconds_per_batch_mean: 0.0003309345245361328
          seconds_per_batch_min: 0.0002620220184326172
          seconds_per_batch_std: 5.494828725286731e-05
      gpu_to_cpu:
        human_readable:
          batch_latency: 200.570 us +/- 20.456 us [153.542 us, 270.605 us]
          batches_per_second: 5.03 K +/- 472.99 [3.70 K, 6.51 K]
        metrics:
          batches_per_second_max: 6512.894409937888
          batches_per_second_mean: 5033.4137656916
          batches_per_second_min: 3695.422026431718
          batches_per_second_std: 472.98679395531
          seconds_per_batch_max: 0.00027060508728027344
          seconds_per_batch_mean: 0.00020056962966918945
          seconds_per_batch_min: 0.00015354156494140625
          seconds_per_batch_std: 2.0456129992485863e-05
      on_device_inference:
        human_readable:
          batch_latency: 75.463 ms +/- 2.806 ms [72.035 ms, 82.844 ms]
          batches_per_second: 13.27 +/- 0.48 [12.07, 13.88]
        metrics:
          batches_per_second_max: 13.882172266793761
          batches_per_second_mean: 13.269244923730541
          batches_per_second_min: 12.070842710533737
          batches_per_second_std: 0.4782250646089328
          seconds_per_batch_max: 0.08284425735473633
          seconds_per_batch_mean: 0.0754633116722107
          seconds_per_batch_min: 0.07203483581542969
          seconds_per_batch_std: 0.0028062718404534945
      total:
        human_readable:
          batch_latency: 75.995 ms +/- 2.841 ms [72.545 ms, 83.474 ms]
          batches_per_second: 13.18 +/- 0.48 [11.98, 13.78]
        metrics:
          batches_per_second_max: 13.784628328414522
          batches_per_second_mean: 13.176624912811587
          batches_per_second_min: 11.979789497736459
          batches_per_second_std: 0.47724654815137274
          seconds_per_batch_max: 0.08347392082214355
          seconds_per_batch_mean: 0.07599481582641601
          seconds_per_batch_min: 0.07254457473754883
          seconds_per_batch_std: 0.0028414619868025403
    batch_size_32:
      cpu_to_gpu:
        human_readable:
          batch_latency: 7.401 ms +/- 213.573 us [6.183 ms, 7.898 ms]
          batches_per_second: 135.23 +/- 4.14 [126.62, 161.73]
        metrics:
          batches_per_second_max: 161.72992982185548
          batches_per_second_mean: 135.23482221797735
          batches_per_second_min: 126.62049811320755
          batches_per_second_std: 4.138683065919314
          seconds_per_batch_max: 0.007897615432739258
          seconds_per_batch_mean: 0.007401053905487061
          seconds_per_batch_min: 0.006183147430419922
          seconds_per_batch_std: 0.00021357314571914551
      gpu_to_cpu:
        human_readable:
          batch_latency: 76.338 ms +/- 919.370 us [75.554 ms, 81.618 ms]
          batches_per_second: 13.10 +/- 0.15 [12.25, 13.24]
        metrics:
          batches_per_second_max: 13.235626942678174
          batches_per_second_mean: 13.1015041845506
          batches_per_second_min: 12.252224461776649
          batches_per_second_std: 0.14935399728422719
          seconds_per_batch_max: 0.08161783218383789
          seconds_per_batch_mean: 0.07633759260177612
          seconds_per_batch_min: 0.07555365562438965
          seconds_per_batch_std: 0.000919369864497228
      on_device_inference:
        human_readable:
          batch_latency: 690.922 ms +/- 1.108 ms [685.880 ms, 693.431 ms]
          batches_per_second: 1.45 +/- 0.00 [1.44, 1.46]
        metrics:
          batches_per_second_max: 1.4579816100456482
          batches_per_second_mean: 1.447345674164011
          batches_per_second_min: 1.4421037648076545
          batches_per_second_std: 0.0023271113761538886
          seconds_per_batch_max: 0.6934313774108887
          seconds_per_batch_mean: 0.6909217309951782
          seconds_per_batch_min: 0.6858797073364258
          seconds_per_batch_std: 0.001108050594709429
      total:
        human_readable:
          batch_latency: 774.660 ms +/- 924.795 us [772.394 ms, 777.305 ms]
          batches_per_second: 1.29 +/- 0.00 [1.29, 1.29]
        metrics:
          batches_per_second_max: 1.2946757310035386
          batches_per_second_mean: 1.2908901167768863
          batches_per_second_min: 1.2864968720209358
          batches_per_second_std: 0.001541075690561489
          seconds_per_batch_max: 0.7773046493530273
          seconds_per_batch_mean: 0.7746603775024414
          seconds_per_batch_min: 0.7723941802978516
          seconds_per_batch_std: 0.0009247945168341107

==== Benchmarking X3DLearner (s) ====
== Benchmarking learner.infer ==
Warming up with batch_size=16:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=16:  10%|█         | 1/10 [00:00<00:00,  9.38it/s]Warming up with batch_size=16:  20%|██        | 2/10 [00:00<00:00,  9.36it/s]Warming up with batch_size=16:  30%|███       | 3/10 [00:00<00:00,  9.47it/s]Warming up with batch_size=16:  40%|████      | 4/10 [00:00<00:00,  9.55it/s]Warming up with batch_size=16:  50%|█████     | 5/10 [00:00<00:00,  9.53it/s]Warming up with batch_size=16:  60%|██████    | 6/10 [00:00<00:00,  9.57it/s]Warming up with batch_size=16:  70%|███████   | 7/10 [00:00<00:00,  9.59it/s]Warming up with batch_size=16:  80%|████████  | 8/10 [00:00<00:00,  9.54it/s]Warming up with batch_size=16:  90%|█████████ | 9/10 [00:00<00:00,  9.45it/s]Warming up with batch_size=16: 100%|██████████| 10/10 [00:01<00:00,  9.48it/s]Warming up with batch_size=16: 100%|██████████| 10/10 [00:01<00:00,  9.49it/s]
Measuring inference for batch_size=16:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference for batch_size=16:   1%|          | 1/100 [00:00<00:10,  9.54it/s]Measuring inference for batch_size=16:   2%|▏         | 2/100 [00:00<00:10,  9.54it/s]Measuring inference for batch_size=16:   3%|▎         | 3/100 [00:00<00:10,  9.48it/s]Measuring inference for batch_size=16:   4%|▍         | 4/100 [00:00<00:10,  9.49it/s]Measuring inference for batch_size=16:   5%|▌         | 5/100 [00:00<00:10,  9.49it/s]Measuring inference for batch_size=16:   6%|▌         | 6/100 [00:00<00:09,  9.48it/s]Measuring inference for batch_size=16:   7%|▋         | 7/100 [00:00<00:09,  9.51it/s]Measuring inference for batch_size=16:   8%|▊         | 8/100 [00:00<00:09,  9.52it/s]Measuring inference for batch_size=16:   9%|▉         | 9/100 [00:00<00:09,  9.50it/s]Measuring inference for batch_size=16:  10%|█         | 10/100 [00:01<00:09,  9.52it/s]Measuring inference for batch_size=16:  11%|█         | 11/100 [00:01<00:09,  9.57it/s]Measuring inference for batch_size=16:  12%|█▏        | 12/100 [00:01<00:09,  9.60it/s]Measuring inference for batch_size=16:  13%|█▎        | 13/100 [00:01<00:09,  9.60it/s]Measuring inference for batch_size=16:  14%|█▍        | 14/100 [00:01<00:08,  9.61it/s]Measuring inference for batch_size=16:  15%|█▌        | 15/100 [00:01<00:08,  9.59it/s]Measuring inference for batch_size=16:  16%|█▌        | 16/100 [00:01<00:08,  9.61it/s]Measuring inference for batch_size=16:  17%|█▋        | 17/100 [00:01<00:08,  9.63it/s]Measuring inference for batch_size=16:  18%|█▊        | 18/100 [00:01<00:08,  9.62it/s]Measuring inference for batch_size=16:  19%|█▉        | 19/100 [00:01<00:08,  9.60it/s]Measuring inference for batch_size=16:  20%|██        | 20/100 [00:02<00:08,  9.58it/s]Measuring inference for batch_size=16:  21%|██        | 21/100 [00:02<00:08,  9.59it/s]Measuring inference for batch_size=16:  22%|██▏       | 22/100 [00:02<00:08,  9.52it/s]Measuring inference for batch_size=16:  23%|██▎       | 23/100 [00:02<00:08,  9.54it/s]Measuring inference for batch_size=16:  24%|██▍       | 24/100 [00:02<00:07,  9.56it/s]Measuring inference for batch_size=16:  25%|██▌       | 25/100 [00:02<00:07,  9.52it/s]Measuring inference for batch_size=16:  26%|██▌       | 26/100 [00:02<00:07,  9.48it/s]Measuring inference for batch_size=16:  27%|██▋       | 27/100 [00:02<00:07,  9.50it/s]Measuring inference for batch_size=16:  28%|██▊       | 28/100 [00:02<00:07,  9.52it/s]Measuring inference for batch_size=16:  29%|██▉       | 29/100 [00:03<00:07,  9.49it/s]Measuring inference for batch_size=16:  30%|███       | 30/100 [00:03<00:07,  9.49it/s]Measuring inference for batch_size=16:  31%|███       | 31/100 [00:03<00:07,  9.47it/s]Measuring inference for batch_size=16:  32%|███▏      | 32/100 [00:03<00:07,  9.46it/s]Measuring inference for batch_size=16:  33%|███▎      | 33/100 [00:03<00:07,  9.48it/s]Measuring inference for batch_size=16:  34%|███▍      | 34/100 [00:03<00:06,  9.47it/s]Measuring inference for batch_size=16:  35%|███▌      | 35/100 [00:03<00:06,  9.38it/s]Measuring inference for batch_size=16:  36%|███▌      | 36/100 [00:03<00:06,  9.41it/s]Measuring inference for batch_size=16:  37%|███▋      | 37/100 [00:03<00:06,  9.38it/s]Measuring inference for batch_size=16:  38%|███▊      | 38/100 [00:03<00:06,  9.44it/s]Measuring inference for batch_size=16:  39%|███▉      | 39/100 [00:04<00:06,  9.48it/s]Measuring inference for batch_size=16:  40%|████      | 40/100 [00:04<00:06,  9.50it/s]Measuring inference for batch_size=16:  41%|████      | 41/100 [00:04<00:06,  9.52it/s]Measuring inference for batch_size=16:  42%|████▏     | 42/100 [00:04<00:06,  9.50it/s]Measuring inference for batch_size=16:  43%|████▎     | 43/100 [00:04<00:05,  9.53it/s]Measuring inference for batch_size=16:  44%|████▍     | 44/100 [00:04<00:05,  9.47it/s]Measuring inference for batch_size=16:  45%|████▌     | 45/100 [00:04<00:05,  9.51it/s]Measuring inference for batch_size=16:  46%|████▌     | 46/100 [00:04<00:05,  9.52it/s]Measuring inference for batch_size=16:  47%|████▋     | 47/100 [00:04<00:05,  9.51it/s]Measuring inference for batch_size=16:  48%|████▊     | 48/100 [00:05<00:05,  9.52it/s]Measuring inference for batch_size=16:  49%|████▉     | 49/100 [00:05<00:05,  9.53it/s]Measuring inference for batch_size=16:  50%|█████     | 50/100 [00:05<00:05,  9.53it/s]Measuring inference for batch_size=16:  51%|█████     | 51/100 [00:05<00:05,  9.43it/s]Measuring inference for batch_size=16:  52%|█████▏    | 52/100 [00:05<00:05,  9.48it/s]Measuring inference for batch_size=16:  53%|█████▎    | 53/100 [00:05<00:04,  9.51it/s]Measuring inference for batch_size=16:  54%|█████▍    | 54/100 [00:05<00:04,  9.52it/s]Measuring inference for batch_size=16:  55%|█████▌    | 55/100 [00:05<00:04,  9.53it/s]Measuring inference for batch_size=16:  56%|█████▌    | 56/100 [00:05<00:04,  9.50it/s]Measuring inference for batch_size=16:  57%|█████▋    | 57/100 [00:05<00:04,  9.53it/s]Measuring inference for batch_size=16:  58%|█████▊    | 58/100 [00:06<00:04,  9.54it/s]Measuring inference for batch_size=16:  59%|█████▉    | 59/100 [00:06<00:04,  9.49it/s]Measuring inference for batch_size=16:  60%|██████    | 60/100 [00:06<00:04,  9.50it/s]Measuring inference for batch_size=16:  61%|██████    | 61/100 [00:06<00:04,  9.53it/s]Measuring inference for batch_size=16:  62%|██████▏   | 62/100 [00:06<00:03,  9.54it/s]Measuring inference for batch_size=16:  63%|██████▎   | 63/100 [00:06<00:03,  9.53it/s]Measuring inference for batch_size=16:  64%|██████▍   | 64/100 [00:06<00:03,  9.52it/s]Measuring inference for batch_size=16:  65%|██████▌   | 65/100 [00:06<00:03,  9.53it/s]Measuring inference for batch_size=16:  66%|██████▌   | 66/100 [00:06<00:03,  9.54it/s]Measuring inference for batch_size=16:  67%|██████▋   | 67/100 [00:07<00:03,  9.55it/s]Measuring inference for batch_size=16:  68%|██████▊   | 68/100 [00:07<00:03,  9.49it/s]Measuring inference for batch_size=16:  69%|██████▉   | 69/100 [00:07<00:03,  9.51it/s]Measuring inference for batch_size=16:  70%|███████   | 70/100 [00:07<00:03,  9.51it/s]Measuring inference for batch_size=16:  71%|███████   | 71/100 [00:07<00:03,  9.53it/s]Measuring inference for batch_size=16:  72%|███████▏  | 72/100 [00:07<00:02,  9.55it/s]Measuring inference for batch_size=16:  73%|███████▎  | 73/100 [00:07<00:02,  9.54it/s]Measuring inference for batch_size=16:  74%|███████▍  | 74/100 [00:07<00:02,  9.52it/s]Measuring inference for batch_size=16:  75%|███████▌  | 75/100 [00:07<00:02,  9.53it/s]Measuring inference for batch_size=16:  76%|███████▌  | 76/100 [00:07<00:02,  9.46it/s]Measuring inference for batch_size=16:  77%|███████▋  | 77/100 [00:08<00:02,  9.41it/s]Measuring inference for batch_size=16:  78%|███████▊  | 78/100 [00:08<00:02,  9.41it/s]Measuring inference for batch_size=16:  79%|███████▉  | 79/100 [00:08<00:02,  9.46it/s]Measuring inference for batch_size=16:  80%|████████  | 80/100 [00:08<00:02,  9.50it/s]Measuring inference for batch_size=16:  81%|████████  | 81/100 [00:08<00:01,  9.54it/s]Measuring inference for batch_size=16:  82%|████████▏ | 82/100 [00:08<00:01,  9.50it/s]Measuring inference for batch_size=16:  83%|████████▎ | 83/100 [00:08<00:01,  9.45it/s]Measuring inference for batch_size=16:  84%|████████▍ | 84/100 [00:08<00:01,  9.50it/s]Measuring inference for batch_size=16:  85%|████████▌ | 85/100 [00:08<00:01,  9.51it/s]Measuring inference for batch_size=16:  86%|████████▌ | 86/100 [00:09<00:01,  9.51it/s]Measuring inference for batch_size=16:  87%|████████▋ | 87/100 [00:09<00:01,  9.53it/s]Measuring inference for batch_size=16:  88%|████████▊ | 88/100 [00:09<00:01,  9.52it/s]Measuring inference for batch_size=16:  89%|████████▉ | 89/100 [00:09<00:01,  9.53it/s]Measuring inference for batch_size=16:  90%|█████████ | 90/100 [00:09<00:01,  9.54it/s]Measuring inference for batch_size=16:  91%|█████████ | 91/100 [00:09<00:00,  9.50it/s]Measuring inference for batch_size=16:  92%|█████████▏| 92/100 [00:09<00:00,  9.49it/s]Measuring inference for batch_size=16:  93%|█████████▎| 93/100 [00:09<00:00,  9.53it/s]Measuring inference for batch_size=16:  94%|█████████▍| 94/100 [00:09<00:00,  9.54it/s]Measuring inference for batch_size=16:  95%|█████████▌| 95/100 [00:09<00:00,  9.50it/s]Measuring inference for batch_size=16:  96%|█████████▌| 96/100 [00:10<00:00,  9.54it/s]Measuring inference for batch_size=16:  97%|█████████▋| 97/100 [00:10<00:00,  9.56it/s]Measuring inference for batch_size=16:  98%|█████████▊| 98/100 [00:10<00:00,  9.57it/s]Measuring inference for batch_size=16:  99%|█████████▉| 99/100 [00:10<00:00,  9.57it/s]Measuring inference for batch_size=16: 100%|██████████| 100/100 [00:10<00:00,  9.57it/s]Measuring inference for batch_size=16: 100%|██████████| 100/100 [00:10<00:00,  9.52it/s]
Measuring energy for batch_size=1:   0%|          | 0/10 [00:00<?, ?it/s]Measuring energy for batch_size=1:  10%|█         | 1/10 [00:00<00:03,  2.26it/s]Measuring energy for batch_size=1:  20%|██        | 2/10 [00:00<00:02,  3.55it/s]Measuring energy for batch_size=1:  30%|███       | 3/10 [00:00<00:01,  4.75it/s]Measuring energy for batch_size=1:  40%|████      | 4/10 [00:00<00:01,  5.66it/s]Measuring energy for batch_size=1:  50%|█████     | 5/10 [00:00<00:00,  6.33it/s]Measuring energy for batch_size=1:  60%|██████    | 6/10 [00:01<00:00,  6.80it/s]Measuring energy for batch_size=1:  70%|███████   | 7/10 [00:01<00:00,  7.14it/s]Measuring energy for batch_size=1:  80%|████████  | 8/10 [00:01<00:00,  7.38it/s]Measuring energy for batch_size=1:  90%|█████████ | 9/10 [00:01<00:00,  7.57it/s]Measuring energy for batch_size=1: 100%|██████████| 10/10 [00:01<00:00,  7.69it/s]Measuring energy for batch_size=1: 100%|██████████| 10/10 [00:01<00:00,  6.19it/s]
Warming up with batch_size=16:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=16:  10%|█         | 1/10 [00:02<00:18,  2.02s/it]Warming up with batch_size=16:  20%|██        | 2/10 [00:04<00:16,  2.02s/it]Warming up with batch_size=16:  30%|███       | 3/10 [00:06<00:14,  2.02s/it]Warming up with batch_size=16:  40%|████      | 4/10 [00:08<00:12,  2.02s/it]Warming up with batch_size=16:  50%|█████     | 5/10 [00:10<00:10,  2.03s/it]Warming up with batch_size=16:  60%|██████    | 6/10 [00:12<00:08,  2.03s/it]Warming up with batch_size=16:  70%|███████   | 7/10 [00:14<00:06,  2.02s/it]Warming up with batch_size=16:  80%|████████  | 8/10 [00:16<00:04,  2.02s/it]Warming up with batch_size=16:  90%|█████████ | 9/10 [00:18<00:02,  2.02s/it]Warming up with batch_size=16: 100%|██████████| 10/10 [00:20<00:00,  2.02s/it]Warming up with batch_size=16: 100%|██████████| 10/10 [00:20<00:00,  2.02s/it]
Measuring inference for batch_size=16:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference for batch_size=16:   1%|          | 1/100 [00:02<03:20,  2.03s/it]Measuring inference for batch_size=16:   2%|▏         | 2/100 [00:04<03:18,  2.02s/it]Measuring inference for batch_size=16:   3%|▎         | 3/100 [00:06<03:16,  2.03s/it]Measuring inference for batch_size=16:   4%|▍         | 4/100 [00:08<03:14,  2.03s/it]Measuring inference for batch_size=16:   5%|▌         | 5/100 [00:10<03:12,  2.03s/it]Measuring inference for batch_size=16:   6%|▌         | 6/100 [00:12<03:10,  2.03s/it]Measuring inference for batch_size=16:   7%|▋         | 7/100 [00:14<03:08,  2.03s/it]Measuring inference for batch_size=16:   8%|▊         | 8/100 [00:16<03:06,  2.03s/it]Measuring inference for batch_size=16:   9%|▉         | 9/100 [00:18<03:04,  2.03s/it]Measuring inference for batch_size=16:  10%|█         | 10/100 [00:20<03:02,  2.02s/it]Measuring inference for batch_size=16:  11%|█         | 11/100 [00:22<02:59,  2.02s/it]Measuring inference for batch_size=16:  12%|█▏        | 12/100 [00:24<02:57,  2.02s/it]Measuring inference for batch_size=16:  13%|█▎        | 13/100 [00:26<02:55,  2.02s/it]Measuring inference for batch_size=16:  14%|█▍        | 14/100 [00:28<02:53,  2.02s/it]Measuring inference for batch_size=16:  15%|█▌        | 15/100 [00:30<02:51,  2.02s/it]Measuring inference for batch_size=16:  16%|█▌        | 16/100 [00:32<02:49,  2.02s/it]Measuring inference for batch_size=16:  17%|█▋        | 17/100 [00:34<02:47,  2.02s/it]Measuring inference for batch_size=16:  18%|█▊        | 18/100 [00:36<02:45,  2.02s/it]Measuring inference for batch_size=16:  19%|█▉        | 19/100 [00:38<02:43,  2.02s/it]Measuring inference for batch_size=16:  20%|██        | 20/100 [00:40<02:41,  2.02s/it]Measuring inference for batch_size=16:  21%|██        | 21/100 [00:42<02:39,  2.02s/it]Measuring inference for batch_size=16:  22%|██▏       | 22/100 [00:44<02:37,  2.02s/it]Measuring inference for batch_size=16:  23%|██▎       | 23/100 [00:46<02:35,  2.02s/it]Measuring inference for batch_size=16:  24%|██▍       | 24/100 [00:48<02:33,  2.02s/it]Measuring inference for batch_size=16:  25%|██▌       | 25/100 [00:50<02:31,  2.02s/it]Measuring inference for batch_size=16:  26%|██▌       | 26/100 [00:52<02:29,  2.02s/it]Measuring inference for batch_size=16:  27%|██▋       | 27/100 [00:54<02:27,  2.02s/it]Measuring inference for batch_size=16:  28%|██▊       | 28/100 [00:56<02:25,  2.02s/it]Measuring inference for batch_size=16:  29%|██▉       | 29/100 [00:58<02:23,  2.02s/it]Measuring inference for batch_size=16:  30%|███       | 30/100 [01:00<02:21,  2.03s/it]Measuring inference for batch_size=16:  31%|███       | 31/100 [01:02<02:19,  2.03s/it]Measuring inference for batch_size=16:  32%|███▏      | 32/100 [01:04<02:17,  2.03s/it]Measuring inference for batch_size=16:  33%|███▎      | 33/100 [01:06<02:15,  2.02s/it]Measuring inference for batch_size=16:  34%|███▍      | 34/100 [01:08<02:13,  2.02s/it]Measuring inference for batch_size=16:  35%|███▌      | 35/100 [01:10<02:11,  2.02s/it]Measuring inference for batch_size=16:  36%|███▌      | 36/100 [01:12<02:09,  2.02s/it]Measuring inference for batch_size=16:  37%|███▋      | 37/100 [01:14<02:07,  2.02s/it]Measuring inference for batch_size=16:  38%|███▊      | 38/100 [01:16<02:05,  2.02s/it]Measuring inference for batch_size=16:  39%|███▉      | 39/100 [01:18<02:03,  2.02s/it]Measuring inference for batch_size=16:  40%|████      | 40/100 [01:20<02:01,  2.03s/it]Measuring inference for batch_size=16:  41%|████      | 41/100 [01:22<01:59,  2.03s/it]Measuring inference for batch_size=16:  42%|████▏     | 42/100 [01:24<01:57,  2.02s/it]Measuring inference for batch_size=16:  43%|████▎     | 43/100 [01:26<01:55,  2.02s/it]Measuring inference for batch_size=16:  44%|████▍     | 44/100 [01:28<01:53,  2.02s/it]Measuring inference for batch_size=16:  45%|████▌     | 45/100 [01:31<01:51,  2.02s/it]Measuring inference for batch_size=16:  46%|████▌     | 46/100 [01:33<01:49,  2.02s/it]Measuring inference for batch_size=16:  47%|████▋     | 47/100 [01:35<01:47,  2.02s/it]Measuring inference for batch_size=16:  48%|████▊     | 48/100 [01:37<01:45,  2.02s/it]Measuring inference for batch_size=16:  49%|████▉     | 49/100 [01:39<01:43,  2.02s/it]Measuring inference for batch_size=16:  50%|█████     | 50/100 [01:41<01:41,  2.02s/it]Measuring inference for batch_size=16:  51%|█████     | 51/100 [01:43<01:39,  2.02s/it]Measuring inference for batch_size=16:  52%|█████▏    | 52/100 [01:45<01:36,  2.02s/it]Measuring inference for batch_size=16:  53%|█████▎    | 53/100 [01:47<01:35,  2.02s/it]Measuring inference for batch_size=16:  54%|█████▍    | 54/100 [01:49<01:33,  2.02s/it]Measuring inference for batch_size=16:  55%|█████▌    | 55/100 [01:51<01:30,  2.02s/it]Measuring inference for batch_size=16:  56%|█████▌    | 56/100 [01:53<01:28,  2.02s/it]Measuring inference for batch_size=16:  57%|█████▋    | 57/100 [01:55<01:26,  2.02s/it]Measuring inference for batch_size=16:  58%|█████▊    | 58/100 [01:57<01:24,  2.02s/it]Measuring inference for batch_size=16:  59%|█████▉    | 59/100 [01:59<01:22,  2.02s/it]Measuring inference for batch_size=16:  60%|██████    | 60/100 [02:01<01:20,  2.02s/it]Measuring inference for batch_size=16:  61%|██████    | 61/100 [02:03<01:18,  2.02s/it]Measuring inference for batch_size=16:  62%|██████▏   | 62/100 [02:05<01:16,  2.02s/it]Measuring inference for batch_size=16:  63%|██████▎   | 63/100 [02:07<01:14,  2.02s/it]Measuring inference for batch_size=16:  64%|██████▍   | 64/100 [02:09<01:12,  2.02s/it]Measuring inference for batch_size=16:  65%|██████▌   | 65/100 [02:11<01:10,  2.02s/it]Measuring inference for batch_size=16:  66%|██████▌   | 66/100 [02:13<01:08,  2.02s/it]Measuring inference for batch_size=16:  67%|██████▋   | 67/100 [02:15<01:06,  2.02s/it]Measuring inference for batch_size=16:  68%|██████▊   | 68/100 [02:17<01:04,  2.02s/it]Measuring inference for batch_size=16:  69%|██████▉   | 69/100 [02:19<01:02,  2.03s/it]Measuring inference for batch_size=16:  70%|███████   | 70/100 [02:21<01:00,  2.02s/it]Measuring inference for batch_size=16:  71%|███████   | 71/100 [02:23<00:58,  2.02s/it]Measuring inference for batch_size=16:  72%|███████▏  | 72/100 [02:25<00:56,  2.02s/it]Measuring inference for batch_size=16:  73%|███████▎  | 73/100 [02:27<00:54,  2.02s/it]Measuring inference for batch_size=16:  74%|███████▍  | 74/100 [02:29<00:52,  2.02s/it]Measuring inference for batch_size=16:  75%|███████▌  | 75/100 [02:31<00:50,  2.02s/it]Measuring inference for batch_size=16:  76%|███████▌  | 76/100 [02:33<00:48,  2.02s/it]Measuring inference for batch_size=16:  77%|███████▋  | 77/100 [02:35<00:46,  2.02s/it]Measuring inference for batch_size=16:  78%|███████▊  | 78/100 [02:37<00:44,  2.02s/it]Measuring inference for batch_size=16:  79%|███████▉  | 79/100 [02:39<00:42,  2.02s/it]Measuring inference for batch_size=16:  80%|████████  | 80/100 [02:41<00:40,  2.02s/it]Measuring inference for batch_size=16:  81%|████████  | 81/100 [02:43<00:38,  2.02s/it]Measuring inference for batch_size=16:  82%|████████▏ | 82/100 [02:45<00:36,  2.02s/it]Measuring inference for batch_size=16:  83%|████████▎ | 83/100 [02:47<00:34,  2.02s/it]Measuring inference for batch_size=16:  84%|████████▍ | 84/100 [02:49<00:32,  2.02s/it]Measuring inference for batch_size=16:  85%|████████▌ | 85/100 [02:51<00:30,  2.02s/it]Measuring inference for batch_size=16:  86%|████████▌ | 86/100 [02:53<00:28,  2.02s/it]Measuring inference for batch_size=16:  87%|████████▋ | 87/100 [02:55<00:26,  2.02s/it]Measuring inference for batch_size=16:  88%|████████▊ | 88/100 [02:57<00:24,  2.03s/it]Measuring inference for batch_size=16:  89%|████████▉ | 89/100 [02:59<00:22,  2.02s/it]Measuring inference for batch_size=16:  90%|█████████ | 90/100 [03:02<00:20,  2.02s/it]Measuring inference for batch_size=16:  91%|█████████ | 91/100 [03:04<00:18,  2.02s/it]Measuring inference for batch_size=16:  92%|█████████▏| 92/100 [03:06<00:16,  2.02s/it]Measuring inference for batch_size=16:  93%|█████████▎| 93/100 [03:08<00:14,  2.03s/it]Measuring inference for batch_size=16:  94%|█████████▍| 94/100 [03:10<00:12,  2.02s/it]Measuring inference for batch_size=16:  95%|█████████▌| 95/100 [03:12<00:10,  2.02s/it]Measuring inference for batch_size=16:  96%|█████████▌| 96/100 [03:14<00:08,  2.02s/it]Measuring inference for batch_size=16:  97%|█████████▋| 97/100 [03:16<00:06,  2.02s/it]Measuring inference for batch_size=16:  98%|█████████▊| 98/100 [03:18<00:04,  2.02s/it]Measuring inference for batch_size=16:  99%|█████████▉| 99/100 [03:20<00:02,  2.02s/it]Measuring inference for batch_size=16: 100%|██████████| 100/100 [03:22<00:00,  2.02s/it]Measuring inference for batch_size=16: 100%|██████████| 100/100 [03:22<00:00,  2.02s/it]
Measuring energy for batch_size=16:   0%|          | 0/10 [00:00<?, ?it/s]Measuring energy for batch_size=16:  10%|█         | 1/10 [00:02<00:23,  2.57s/it]Measuring energy for batch_size=16:  20%|██        | 2/10 [00:04<00:18,  2.27s/it]Measuring energy for batch_size=16:  30%|███       | 3/10 [00:06<00:15,  2.17s/it]Measuring energy for batch_size=16:  40%|████      | 4/10 [00:08<00:12,  2.12s/it]Measuring energy for batch_size=16:  50%|█████     | 5/10 [00:10<00:10,  2.09s/it]Measuring energy for batch_size=16:  60%|██████    | 6/10 [00:12<00:08,  2.08s/it]Measuring energy for batch_size=16:  70%|███████   | 7/10 [00:14<00:06,  2.07s/it]Measuring energy for batch_size=16:  80%|████████  | 8/10 [00:16<00:04,  2.06s/it]Measuring energy for batch_size=16:  90%|█████████ | 9/10 [00:18<00:02,  2.05s/it]Measuring energy for batch_size=16: 100%|██████████| 10/10 [00:20<00:00,  2.05s/it]Measuring energy for batch_size=16: 100%|██████████| 10/10 [00:20<00:00,  2.10s/it]
learner.infer:
  device: cuda
  energy:
    batch_size_1:
      joules: 1.9979028091923396
      kWh: 5.549730025534277e-07
    batch_size_16:
      joules: 64.36649637352943
      kWh: 1.7879582325980397e-05
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 20.99 GB
      total: 31.17 GB
      used: 9.75 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: 8.700 us +/- 3.277 us [5.484 us, 36.955 us]
          batches_per_second: 122.16 K +/- 24.37 K [27.06 K, 182.36 K]
        metrics:
          batches_per_second_max: 182361.04347826086
          batches_per_second_mean: 122156.85319124153
          batches_per_second_min: 27060.025806451613
          batches_per_second_std: 24366.87710677912
          seconds_per_batch_max: 3.695487976074219e-05
          seconds_per_batch_mean: 8.699893951416016e-06
          seconds_per_batch_min: 5.4836273193359375e-06
          seconds_per_batch_std: 3.2771056499647015e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: 302.234 us +/- 59.999 us [185.728 us, 465.393 us]
          batches_per_second: 3.46 K +/- 772.69 [2.15 K, 5.38 K]
        metrics:
          batches_per_second_max: 5384.215661103979
          batches_per_second_mean: 3457.994461544143
          batches_per_second_min: 2148.72131147541
          batches_per_second_std: 772.6850208261152
          seconds_per_batch_max: 0.00046539306640625
          seconds_per_batch_mean: 0.00030223369598388674
          seconds_per_batch_min: 0.0001857280731201172
          seconds_per_batch_std: 5.999930586209777e-05
      on_device_inference:
        human_readable:
          batch_latency: 103.845 ms +/- 1.084 ms [102.109 ms, 107.694 ms]
          batches_per_second: 9.63 +/- 0.10 [9.29, 9.79]
        metrics:
          batches_per_second_max: 9.793483159185103
          batches_per_second_mean: 9.630794534083593
          batches_per_second_min: 9.285596634934691
          batches_per_second_std: 0.09929179869331674
          seconds_per_batch_max: 0.10769367218017578
          seconds_per_batch_mean: 0.1038447666168213
          seconds_per_batch_min: 0.10210871696472168
          seconds_per_batch_std: 0.0010839067550844261
      total:
        human_readable:
          batch_latency: 104.156 ms +/- 1.103 ms [102.362 ms, 108.049 ms]
          batches_per_second: 9.60 +/- 0.10 [9.26, 9.77]
        metrics:
          batches_per_second_max: 9.769235427565228
          batches_per_second_mean: 9.602073123183672
          batches_per_second_min: 9.255026567106215
          batches_per_second_std: 0.10037133237921828
          seconds_per_batch_max: 0.10804939270019531
          seconds_per_batch_mean: 0.10415570020675659
          seconds_per_batch_min: 0.10236215591430664
          seconds_per_batch_std: 0.0011025306504979292
    batch_size_16:
      cpu_to_gpu:
        human_readable:
          batch_latency: 10.049 us +/- 4.567 us [8.106 us, 54.836 us]
          batches_per_second: 103.99 K +/- 11.85 K [18.24 K, 123.36 K]
        metrics:
          batches_per_second_max: 123361.88235294117
          batches_per_second_mean: 103989.64515326556
          batches_per_second_min: 18236.104347826087
          batches_per_second_std: 11853.637184700683
          seconds_per_batch_max: 5.4836273193359375e-05
          seconds_per_batch_mean: 1.0049343109130859e-05
          seconds_per_batch_min: 8.106231689453125e-06
          seconds_per_batch_std: 4.566742803935284e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: 2.240 ms +/- 83.524 us [2.097 ms, 2.512 ms]
          batches_per_second: 446.98 +/- 16.09 [398.09, 476.90]
        metrics:
          batches_per_second_max: 476.89641841955654
          batches_per_second_mean: 446.98233114521855
          batches_per_second_min: 398.0926347760061
          batches_per_second_std: 16.086951595911714
          seconds_per_batch_max: 0.0025119781494140625
          seconds_per_batch_mean: 0.0022402262687683107
          seconds_per_batch_min: 0.002096891403198242
          seconds_per_batch_std: 8.352429098719618e-05
      on_device_inference:
        human_readable:
          batch_latency: 2.019 s +/- 5.163 ms [2.007 s, 2.033 s]
          batches_per_second: 0.50 +/- 0.00 [0.49, 0.50]
        metrics:
          batches_per_second_max: 0.49831880088046576
          batches_per_second_mean: 0.4953269702362678
          batches_per_second_min: 0.4919528608668369
          batches_per_second_std: 0.0012664413144840717
          seconds_per_batch_max: 2.032715082168579
          seconds_per_batch_mean: 2.018881664276123
          seconds_per_batch_min: 2.0067474842071533
          seconds_per_batch_std: 0.0051625540799183165
      total:
        human_readable:
          batch_latency: 2.021 s +/- 5.169 ms [2.009 s, 2.035 s]
          batches_per_second: 0.49 +/- 0.00 [0.49, 0.50]
        metrics:
          batches_per_second_max: 0.4977535662336303
          batches_per_second_mean: 0.4947754871854353
          batches_per_second_min: 0.4914255157660703
          batches_per_second_std: 0.001265242150654567
          seconds_per_batch_max: 2.0348963737487793
          seconds_per_batch_mean: 2.0211319398880003
          seconds_per_batch_min: 2.009026288986206
          seconds_per_batch_std: 0.0051691795762297136

== Benchmarking model directly ==
Warming up with batch_size=16:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=16:  20%|██        | 2/10 [00:00<00:00, 10.27it/s]Warming up with batch_size=16:  40%|████      | 4/10 [00:00<00:00, 10.26it/s]Warming up with batch_size=16:  60%|██████    | 6/10 [00:00<00:00, 10.25it/s]Warming up with batch_size=16:  80%|████████  | 8/10 [00:00<00:00, 10.24it/s]Warming up with batch_size=16: 100%|██████████| 10/10 [00:00<00:00, 10.22it/s]Warming up with batch_size=16: 100%|██████████| 10/10 [00:00<00:00, 10.22it/s]
Measuring inference for batch_size=16:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference for batch_size=16:   2%|▏         | 2/100 [00:00<00:09, 10.23it/s]Measuring inference for batch_size=16:   4%|▍         | 4/100 [00:00<00:09, 10.20it/s]Measuring inference for batch_size=16:   6%|▌         | 6/100 [00:00<00:09, 10.20it/s]Measuring inference for batch_size=16:   8%|▊         | 8/100 [00:00<00:09, 10.20it/s]Measuring inference for batch_size=16:  10%|█         | 10/100 [00:00<00:08, 10.21it/s]Measuring inference for batch_size=16:  12%|█▏        | 12/100 [00:01<00:08, 10.21it/s]Measuring inference for batch_size=16:  14%|█▍        | 14/100 [00:01<00:08, 10.22it/s]Measuring inference for batch_size=16:  16%|█▌        | 16/100 [00:01<00:08, 10.24it/s]Measuring inference for batch_size=16:  18%|█▊        | 18/100 [00:01<00:08, 10.24it/s]Measuring inference for batch_size=16:  20%|██        | 20/100 [00:01<00:07, 10.24it/s]Measuring inference for batch_size=16:  22%|██▏       | 22/100 [00:02<00:07, 10.24it/s]Measuring inference for batch_size=16:  24%|██▍       | 24/100 [00:02<00:07, 10.24it/s]Measuring inference for batch_size=16:  26%|██▌       | 26/100 [00:02<00:07, 10.23it/s]Measuring inference for batch_size=16:  28%|██▊       | 28/100 [00:02<00:07, 10.23it/s]Measuring inference for batch_size=16:  30%|███       | 30/100 [00:02<00:06, 10.24it/s]Measuring inference for batch_size=16:  32%|███▏      | 32/100 [00:03<00:06, 10.24it/s]Measuring inference for batch_size=16:  34%|███▍      | 34/100 [00:03<00:06, 10.23it/s]Measuring inference for batch_size=16:  36%|███▌      | 36/100 [00:03<00:06, 10.23it/s]Measuring inference for batch_size=16:  38%|███▊      | 38/100 [00:03<00:06, 10.23it/s]Measuring inference for batch_size=16:  40%|████      | 40/100 [00:03<00:05, 10.22it/s]Measuring inference for batch_size=16:  42%|████▏     | 42/100 [00:04<00:05, 10.23it/s]Measuring inference for batch_size=16:  44%|████▍     | 44/100 [00:04<00:05, 10.23it/s]Measuring inference for batch_size=16:  46%|████▌     | 46/100 [00:04<00:05, 10.24it/s]Measuring inference for batch_size=16:  48%|████▊     | 48/100 [00:04<00:05, 10.24it/s]Measuring inference for batch_size=16:  50%|█████     | 50/100 [00:04<00:04, 10.25it/s]Measuring inference for batch_size=16:  52%|█████▏    | 52/100 [00:05<00:04, 10.23it/s]Measuring inference for batch_size=16:  54%|█████▍    | 54/100 [00:05<00:04, 10.23it/s]Measuring inference for batch_size=16:  56%|█████▌    | 56/100 [00:05<00:04, 10.23it/s]Measuring inference for batch_size=16:  58%|█████▊    | 58/100 [00:05<00:04, 10.23it/s]Measuring inference for batch_size=16:  60%|██████    | 60/100 [00:05<00:03, 10.23it/s]Measuring inference for batch_size=16:  62%|██████▏   | 62/100 [00:06<00:03, 10.23it/s]Measuring inference for batch_size=16:  64%|██████▍   | 64/100 [00:06<00:03, 10.23it/s]Measuring inference for batch_size=16:  66%|██████▌   | 66/100 [00:06<00:03, 10.22it/s]Measuring inference for batch_size=16:  68%|██████▊   | 68/100 [00:06<00:03, 10.22it/s]Measuring inference for batch_size=16:  70%|███████   | 70/100 [00:06<00:02, 10.22it/s]Measuring inference for batch_size=16:  72%|███████▏  | 72/100 [00:07<00:02, 10.23it/s]Measuring inference for batch_size=16:  74%|███████▍  | 74/100 [00:07<00:02, 10.24it/s]Measuring inference for batch_size=16:  76%|███████▌  | 76/100 [00:07<00:02, 10.26it/s]Measuring inference for batch_size=16:  78%|███████▊  | 78/100 [00:07<00:02, 10.25it/s]Measuring inference for batch_size=16:  80%|████████  | 80/100 [00:07<00:01, 10.25it/s]Measuring inference for batch_size=16:  82%|████████▏ | 82/100 [00:08<00:01, 10.24it/s]Measuring inference for batch_size=16:  84%|████████▍ | 84/100 [00:08<00:01, 10.24it/s]Measuring inference for batch_size=16:  86%|████████▌ | 86/100 [00:08<00:01, 10.23it/s]Measuring inference for batch_size=16:  88%|████████▊ | 88/100 [00:08<00:01, 10.23it/s]Measuring inference for batch_size=16:  90%|█████████ | 90/100 [00:08<00:00, 10.23it/s]Measuring inference for batch_size=16:  92%|█████████▏| 92/100 [00:08<00:00, 10.22it/s]Measuring inference for batch_size=16:  94%|█████████▍| 94/100 [00:09<00:00, 10.23it/s]Measuring inference for batch_size=16:  96%|█████████▌| 96/100 [00:09<00:00, 10.23it/s]Measuring inference for batch_size=16:  98%|█████████▊| 98/100 [00:09<00:00, 10.23it/s]Measuring inference for batch_size=16: 100%|██████████| 100/100 [00:09<00:00, 10.23it/s]Measuring inference for batch_size=16: 100%|██████████| 100/100 [00:09<00:00, 10.23it/s]
Measuring energy for batch_size=1:   0%|          | 0/10 [00:00<?, ?it/s]Measuring energy for batch_size=1:  10%|█         | 1/10 [00:00<00:02,  3.81it/s]Measuring energy for batch_size=1:  20%|██        | 2/10 [00:00<00:01,  5.52it/s]Measuring energy for batch_size=1:  30%|███       | 3/10 [00:00<00:01,  6.43it/s]Measuring energy for batch_size=1:  40%|████      | 4/10 [00:00<00:00,  6.98it/s]Measuring energy for batch_size=1:  50%|█████     | 5/10 [00:00<00:00,  7.33it/s]Measuring energy for batch_size=1:  60%|██████    | 6/10 [00:00<00:00,  7.56it/s]Measuring energy for batch_size=1:  70%|███████   | 7/10 [00:01<00:00,  7.71it/s]Measuring energy for batch_size=1:  80%|████████  | 8/10 [00:01<00:00,  7.81it/s]Measuring energy for batch_size=1:  90%|█████████ | 9/10 [00:01<00:00,  7.88it/s]Measuring energy for batch_size=1: 100%|██████████| 10/10 [00:01<00:00,  7.92it/s]Measuring energy for batch_size=1: 100%|██████████| 10/10 [00:01<00:00,  7.23it/s]
Warming up with batch_size=16:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=16:  10%|█         | 1/10 [00:01<00:17,  1.98s/it]Warming up with batch_size=16:  20%|██        | 2/10 [00:03<00:15,  1.98s/it]Warming up with batch_size=16:  30%|███       | 3/10 [00:05<00:13,  1.98s/it]Warming up with batch_size=16:  40%|████      | 4/10 [00:07<00:11,  1.98s/it]Warming up with batch_size=16:  50%|█████     | 5/10 [00:09<00:09,  1.98s/it]Warming up with batch_size=16:  60%|██████    | 6/10 [00:11<00:07,  1.98s/it]Warming up with batch_size=16:  70%|███████   | 7/10 [00:13<00:05,  1.98s/it]Warming up with batch_size=16:  80%|████████  | 8/10 [00:15<00:03,  1.98s/it]Warming up with batch_size=16:  90%|█████████ | 9/10 [00:17<00:01,  1.98s/it]Warming up with batch_size=16: 100%|██████████| 10/10 [00:19<00:00,  1.98s/it]Warming up with batch_size=16: 100%|██████████| 10/10 [00:19<00:00,  1.98s/it]
Measuring inference for batch_size=16:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference for batch_size=16:   1%|          | 1/100 [00:01<03:16,  1.98s/it]Measuring inference for batch_size=16:   2%|▏         | 2/100 [00:03<03:14,  1.98s/it]Measuring inference for batch_size=16:   3%|▎         | 3/100 [00:05<03:12,  1.98s/it]Measuring inference for batch_size=16:   4%|▍         | 4/100 [00:07<03:10,  1.98s/it]Measuring inference for batch_size=16:   5%|▌         | 5/100 [00:09<03:08,  1.98s/it]Measuring inference for batch_size=16:   6%|▌         | 6/100 [00:11<03:06,  1.98s/it]Measuring inference for batch_size=16:   7%|▋         | 7/100 [00:13<03:04,  1.98s/it]Measuring inference for batch_size=16:   8%|▊         | 8/100 [00:15<03:02,  1.98s/it]Measuring inference for batch_size=16:   9%|▉         | 9/100 [00:17<03:00,  1.98s/it]Measuring inference for batch_size=16:  10%|█         | 10/100 [00:19<02:58,  1.98s/it]Measuring inference for batch_size=16:  11%|█         | 11/100 [00:21<02:56,  1.98s/it]Measuring inference for batch_size=16:  12%|█▏        | 12/100 [00:23<02:54,  1.98s/it]Measuring inference for batch_size=16:  13%|█▎        | 13/100 [00:25<02:52,  1.98s/it]Measuring inference for batch_size=16:  14%|█▍        | 14/100 [00:27<02:50,  1.99s/it]Measuring inference for batch_size=16:  15%|█▌        | 15/100 [00:29<02:48,  1.98s/it]Measuring inference for batch_size=16:  16%|█▌        | 16/100 [00:31<02:46,  1.98s/it]Measuring inference for batch_size=16:  17%|█▋        | 17/100 [00:33<02:44,  1.98s/it]Measuring inference for batch_size=16:  18%|█▊        | 18/100 [00:35<02:42,  1.98s/it]Measuring inference for batch_size=16:  19%|█▉        | 19/100 [00:37<02:40,  1.98s/it]Measuring inference for batch_size=16:  20%|██        | 20/100 [00:39<02:38,  1.98s/it]Measuring inference for batch_size=16:  21%|██        | 21/100 [00:41<02:36,  1.98s/it]Measuring inference for batch_size=16:  22%|██▏       | 22/100 [00:43<02:34,  1.98s/it]Measuring inference for batch_size=16:  23%|██▎       | 23/100 [00:45<02:32,  1.98s/it]Measuring inference for batch_size=16:  24%|██▍       | 24/100 [00:47<02:30,  1.98s/it]Measuring inference for batch_size=16:  25%|██▌       | 25/100 [00:49<02:28,  1.98s/it]Measuring inference for batch_size=16:  26%|██▌       | 26/100 [00:51<02:26,  1.98s/it]Measuring inference for batch_size=16:  27%|██▋       | 27/100 [00:53<02:24,  1.98s/it]Measuring inference for batch_size=16:  28%|██▊       | 28/100 [00:55<02:22,  1.98s/it]Measuring inference for batch_size=16:  29%|██▉       | 29/100 [00:57<02:20,  1.98s/it]Measuring inference for batch_size=16:  30%|███       | 30/100 [00:59<02:18,  1.99s/it]Measuring inference for batch_size=16:  31%|███       | 31/100 [01:01<02:16,  1.98s/it]Measuring inference for batch_size=16:  32%|███▏      | 32/100 [01:03<02:14,  1.98s/it]Measuring inference for batch_size=16:  33%|███▎      | 33/100 [01:05<02:12,  1.98s/it]Measuring inference for batch_size=16:  34%|███▍      | 34/100 [01:07<02:10,  1.98s/it]Measuring inference for batch_size=16:  35%|███▌      | 35/100 [01:09<02:09,  1.98s/it]Measuring inference for batch_size=16:  36%|███▌      | 36/100 [01:11<02:06,  1.98s/it]Measuring inference for batch_size=16:  37%|███▋      | 37/100 [01:13<02:04,  1.98s/it]Measuring inference for batch_size=16:  38%|███▊      | 38/100 [01:15<02:02,  1.98s/it]Measuring inference for batch_size=16:  39%|███▉      | 39/100 [01:17<02:00,  1.98s/it]Measuring inference for batch_size=16:  40%|████      | 40/100 [01:19<01:58,  1.98s/it]Measuring inference for batch_size=16:  41%|████      | 41/100 [01:21<01:56,  1.98s/it]Measuring inference for batch_size=16:  42%|████▏     | 42/100 [01:23<01:54,  1.98s/it]Measuring inference for batch_size=16:  43%|████▎     | 43/100 [01:25<01:53,  1.98s/it]Measuring inference for batch_size=16:  44%|████▍     | 44/100 [01:27<01:51,  1.98s/it]Measuring inference for batch_size=16:  45%|████▌     | 45/100 [01:29<01:49,  1.98s/it]Measuring inference for batch_size=16:  46%|████▌     | 46/100 [01:31<01:47,  1.98s/it]Measuring inference for batch_size=16:  47%|████▋     | 47/100 [01:33<01:45,  1.98s/it]Measuring inference for batch_size=16:  48%|████▊     | 48/100 [01:35<01:43,  1.98s/it]Measuring inference for batch_size=16:  49%|████▉     | 49/100 [01:37<01:41,  1.98s/it]Measuring inference for batch_size=16:  50%|█████     | 50/100 [01:39<01:39,  1.98s/it]Measuring inference for batch_size=16:  51%|█████     | 51/100 [01:41<01:37,  1.99s/it]Measuring inference for batch_size=16:  52%|█████▏    | 52/100 [01:43<01:35,  1.99s/it]Measuring inference for batch_size=16:  53%|█████▎    | 53/100 [01:45<01:33,  1.99s/it]Measuring inference for batch_size=16:  54%|█████▍    | 54/100 [01:47<01:31,  1.98s/it]Measuring inference for batch_size=16:  55%|█████▌    | 55/100 [01:49<01:29,  1.98s/it]Measuring inference for batch_size=16:  56%|█████▌    | 56/100 [01:51<01:27,  1.98s/it]Measuring inference for batch_size=16:  57%|█████▋    | 57/100 [01:53<01:25,  1.98s/it]Measuring inference for batch_size=16:  58%|█████▊    | 58/100 [01:55<01:23,  1.98s/it]Measuring inference for batch_size=16:  59%|█████▉    | 59/100 [01:56<01:21,  1.98s/it]Measuring inference for batch_size=16:  60%|██████    | 60/100 [01:58<01:19,  1.98s/it]Measuring inference for batch_size=16:  61%|██████    | 61/100 [02:00<01:17,  1.98s/it]Measuring inference for batch_size=16:  62%|██████▏   | 62/100 [02:02<01:15,  1.98s/it]Measuring inference for batch_size=16:  63%|██████▎   | 63/100 [02:04<01:13,  1.98s/it]Measuring inference for batch_size=16:  64%|██████▍   | 64/100 [02:06<01:11,  1.98s/it]Measuring inference for batch_size=16:  65%|██████▌   | 65/100 [02:08<01:09,  1.98s/it]Measuring inference for batch_size=16:  66%|██████▌   | 66/100 [02:10<01:07,  1.98s/it]Measuring inference for batch_size=16:  67%|██████▋   | 67/100 [02:12<01:05,  1.98s/it]Measuring inference for batch_size=16:  68%|██████▊   | 68/100 [02:14<01:03,  1.98s/it]Measuring inference for batch_size=16:  69%|██████▉   | 69/100 [02:16<01:01,  1.98s/it]Measuring inference for batch_size=16:  70%|███████   | 70/100 [02:18<00:59,  1.98s/it]Measuring inference for batch_size=16:  71%|███████   | 71/100 [02:20<00:57,  1.98s/it]Measuring inference for batch_size=16:  72%|███████▏  | 72/100 [02:22<00:55,  1.98s/it]Measuring inference for batch_size=16:  73%|███████▎  | 73/100 [02:24<00:53,  1.98s/it]Measuring inference for batch_size=16:  74%|███████▍  | 74/100 [02:26<00:51,  1.98s/it]Measuring inference for batch_size=16:  75%|███████▌  | 75/100 [02:28<00:49,  1.98s/it]Measuring inference for batch_size=16:  76%|███████▌  | 76/100 [02:30<00:47,  1.98s/it]Measuring inference for batch_size=16:  77%|███████▋  | 77/100 [02:32<00:45,  1.98s/it]Measuring inference for batch_size=16:  78%|███████▊  | 78/100 [02:34<00:43,  1.98s/it]Measuring inference for batch_size=16:  79%|███████▉  | 79/100 [02:36<00:41,  1.98s/it]Measuring inference for batch_size=16:  80%|████████  | 80/100 [02:38<00:39,  1.98s/it]Measuring inference for batch_size=16:  81%|████████  | 81/100 [02:40<00:37,  1.98s/it]Measuring inference for batch_size=16:  82%|████████▏ | 82/100 [02:42<00:35,  1.98s/it]Measuring inference for batch_size=16:  83%|████████▎ | 83/100 [02:44<00:33,  1.98s/it]Measuring inference for batch_size=16:  84%|████████▍ | 84/100 [02:46<00:31,  1.98s/it]Measuring inference for batch_size=16:  85%|████████▌ | 85/100 [02:48<00:29,  1.99s/it]Measuring inference for batch_size=16:  86%|████████▌ | 86/100 [02:50<00:27,  1.98s/it]Measuring inference for batch_size=16:  87%|████████▋ | 87/100 [02:52<00:25,  1.98s/it]Measuring inference for batch_size=16:  88%|████████▊ | 88/100 [02:54<00:23,  1.98s/it]Measuring inference for batch_size=16:  89%|████████▉ | 89/100 [02:56<00:21,  1.98s/it]Measuring inference for batch_size=16:  90%|█████████ | 90/100 [02:58<00:19,  1.98s/it]Measuring inference for batch_size=16:  91%|█████████ | 91/100 [03:00<00:17,  1.98s/it]Measuring inference for batch_size=16:  92%|█████████▏| 92/100 [03:02<00:15,  1.98s/it]Measuring inference for batch_size=16:  93%|█████████▎| 93/100 [03:04<00:13,  1.98s/it]Measuring inference for batch_size=16:  94%|█████████▍| 94/100 [03:06<00:11,  1.99s/it]Measuring inference for batch_size=16:  95%|█████████▌| 95/100 [03:08<00:09,  1.99s/it]Measuring inference for batch_size=16:  96%|█████████▌| 96/100 [03:10<00:07,  1.98s/it]Measuring inference for batch_size=16:  97%|█████████▋| 97/100 [03:12<00:05,  1.99s/it]Measuring inference for batch_size=16:  98%|█████████▊| 98/100 [03:14<00:03,  1.98s/it]Measuring inference for batch_size=16:  99%|█████████▉| 99/100 [03:16<00:01,  1.98s/it]Measuring inference for batch_size=16: 100%|██████████| 100/100 [03:18<00:00,  1.98s/it]Measuring inference for batch_size=16: 100%|██████████| 100/100 [03:18<00:00,  1.98s/it]
Measuring energy for batch_size=16:   0%|          | 0/10 [00:00<?, ?it/s]Measuring energy for batch_size=16:  10%|█         | 1/10 [00:02<00:18,  2.08s/it]Measuring energy for batch_size=16:  20%|██        | 2/10 [00:04<00:16,  2.03s/it]Measuring energy for batch_size=16:  30%|███       | 3/10 [00:06<00:14,  2.01s/it]Measuring energy for batch_size=16:  40%|████      | 4/10 [00:08<00:12,  2.00s/it]Measuring energy for batch_size=16:  50%|█████     | 5/10 [00:10<00:09,  2.00s/it]Measuring energy for batch_size=16:  60%|██████    | 6/10 [00:12<00:07,  2.00s/it]Measuring energy for batch_size=16:  70%|███████   | 7/10 [00:14<00:05,  2.00s/it]Measuring energy for batch_size=16:  80%|████████  | 8/10 [00:16<00:03,  2.00s/it]Measuring energy for batch_size=16:  90%|█████████ | 9/10 [00:18<00:01,  2.00s/it]Measuring energy for batch_size=16: 100%|██████████| 10/10 [00:20<00:00,  2.00s/it]Measuring energy for batch_size=16: 100%|██████████| 10/10 [00:20<00:00,  2.00s/it]
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
learner.model.forward:
  device: cuda
  energy:
    batch_size_1:
      joules: 1.766456038344701
      kWh: 4.906822328735281e-07
    batch_size_16:
      joules: 62.79384636739095
      kWh: 1.744273510205304e-05
  flops: 2061365744
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 13.69 GB
      total: 31.17 GB
      used: 17.05 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  params: 3794322
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: 871.172 us +/- 85.875 us [670.433 us, 1.246 ms]
          batches_per_second: 1.16 K +/- 109.89 [802.28, 1.49 K]
        metrics:
          batches_per_second_max: 1491.5732574679944
          batches_per_second_mean: 1158.53904625577
          batches_per_second_min: 802.2769701606733
          batches_per_second_std: 109.89310391001419
          seconds_per_batch_max: 0.0012464523315429688
          seconds_per_batch_mean: 0.0008711719512939453
          seconds_per_batch_min: 0.0006704330444335938
          seconds_per_batch_std: 8.58752540284935e-05
      gpu_to_cpu:
        human_readable:
          batch_latency: 946.748 us +/- 560.223 us [192.642 us, 2.831 ms]
          batches_per_second: 1.63 K +/- 1.30 K [353.20, 5.19 K]
        metrics:
          batches_per_second_max: 5190.970297029703
          batches_per_second_mean: 1633.4817906235785
          batches_per_second_min: 353.20454736842106
          batches_per_second_std: 1295.185858254315
          seconds_per_batch_max: 0.0028312206268310547
          seconds_per_batch_mean: 0.0009467482566833496
          seconds_per_batch_min: 0.0001926422119140625
          seconds_per_batch_std: 0.0005602234000153345
      on_device_inference:
        human_readable:
          batch_latency: 95.409 ms +/- 574.749 us [93.515 ms, 96.646 ms]
          batches_per_second: 10.48 +/- 0.06 [10.35, 10.69]
        metrics:
          batches_per_second_max: 10.69342633951335
          batches_per_second_mean: 10.481581413605415
          batches_per_second_min: 10.347032166231255
          batches_per_second_std: 0.06342234823793266
          seconds_per_batch_max: 0.09664607048034668
          seconds_per_batch_mean: 0.09540892839431762
          seconds_per_batch_min: 0.09351539611816406
          seconds_per_batch_std: 0.0005747486267830628
      total:
        human_readable:
          batch_latency: 97.227 ms +/- 235.511 us [96.661 ms, 97.853 ms]
          batches_per_second: 10.29 +/- 0.02 [10.22, 10.35]
        metrics:
          batches_per_second_max: 10.345424319111256
          batches_per_second_mean: 10.285285196072312
          batches_per_second_min: 10.219366659600855
          batches_per_second_std: 0.02491015703095748
          seconds_per_batch_max: 0.09785342216491699
          seconds_per_batch_mean: 0.09722684860229493
          seconds_per_batch_min: 0.09666109085083008
          seconds_per_batch_std: 0.0002355113904131334
    batch_size_16:
      cpu_to_gpu:
        human_readable:
          batch_latency: 10.960 ms +/- 263.731 us [10.406 ms, 11.943 ms]
          batches_per_second: 91.29 +/- 2.19 [83.73, 96.09]
        metrics:
          batches_per_second_max: 96.09384164222874
          batches_per_second_mean: 91.29295071145444
          batches_per_second_min: 83.73201309590354
          batches_per_second_std: 2.1869725094485073
          seconds_per_batch_max: 0.011942863464355469
          seconds_per_batch_mean: 0.010960061550140381
          seconds_per_batch_min: 0.010406494140625
          seconds_per_batch_std: 0.00026373113179227665
      gpu_to_cpu:
        human_readable:
          batch_latency: 788.419 ms +/- 2.995 ms [781.552 ms, 795.356 ms]
          batches_per_second: 1.27 +/- 0.00 [1.26, 1.28]
        metrics:
          batches_per_second_max: 1.2795056597396148
          batches_per_second_mean: 1.2683796538733882
          batches_per_second_min: 1.2572993165691586
          batches_per_second_std: 0.004813516852528043
          seconds_per_batch_max: 0.7953555583953857
          seconds_per_batch_mean: 0.7884188389778137
          seconds_per_batch_min: 0.7815518379211426
          seconds_per_batch_std: 0.0029945341652909013
      on_device_inference:
        human_readable:
          batch_latency: 1.182 s +/- 2.684 ms [1.171 s, 1.187 s]
          batches_per_second: 0.85 +/- 0.00 [0.84, 0.85]
        metrics:
          batches_per_second_max: 0.854317279076508
          batches_per_second_mean: 0.845830860202303
          batches_per_second_min: 0.842214329015273
          batches_per_second_std: 0.001926648934800553
          seconds_per_batch_max: 1.1873462200164795
          seconds_per_batch_mean: 1.1822755813598633
          seconds_per_batch_min: 1.170525312423706
          seconds_per_batch_std: 0.002684467788311193
      total:
        human_readable:
          batch_latency: 1.982 s +/- 3.900 ms [1.973 s, 1.992 s]
          batches_per_second: 0.50 +/- 0.00 [0.50, 0.51]
        metrics:
          batches_per_second_max: 0.5069658023530259
          batches_per_second_mean: 0.5046307927762166
          batches_per_second_min: 0.5019942472325594
          batches_per_second_std: 0.0009928025814400327
          seconds_per_batch_max: 1.9920547008514404
          seconds_per_batch_mean: 1.9816544818878175
          seconds_per_batch_min: 1.9725196361541748
          seconds_per_batch_std: 0.0039002886367168147

==== Benchmarking X3DLearner (m) ====
== Benchmarking learner.infer ==
Warming up with batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=8:  10%|█         | 1/10 [00:00<00:02,  4.45it/s]Warming up with batch_size=8:  20%|██        | 2/10 [00:00<00:01,  4.48it/s]Warming up with batch_size=8:  30%|███       | 3/10 [00:00<00:01,  4.51it/s]Warming up with batch_size=8:  40%|████      | 4/10 [00:00<00:01,  4.50it/s]Warming up with batch_size=8:  50%|█████     | 5/10 [00:01<00:01,  4.49it/s]Warming up with batch_size=8:  60%|██████    | 6/10 [00:01<00:00,  4.49it/s]Warming up with batch_size=8:  70%|███████   | 7/10 [00:01<00:00,  4.50it/s]Warming up with batch_size=8:  80%|████████  | 8/10 [00:01<00:00,  4.52it/s]Warming up with batch_size=8:  90%|█████████ | 9/10 [00:02<00:00,  4.50it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:02<00:00,  4.51it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:02<00:00,  4.50it/s]
Measuring inference for batch_size=8:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference for batch_size=8:   1%|          | 1/100 [00:00<00:21,  4.53it/s]Measuring inference for batch_size=8:   2%|▏         | 2/100 [00:00<00:21,  4.49it/s]Measuring inference for batch_size=8:   3%|▎         | 3/100 [00:00<00:21,  4.48it/s]Measuring inference for batch_size=8:   4%|▍         | 4/100 [00:00<00:21,  4.49it/s]Measuring inference for batch_size=8:   5%|▌         | 5/100 [00:01<00:21,  4.47it/s]Measuring inference for batch_size=8:   6%|▌         | 6/100 [00:01<00:20,  4.48it/s]Measuring inference for batch_size=8:   7%|▋         | 7/100 [00:01<00:20,  4.46it/s]Measuring inference for batch_size=8:   8%|▊         | 8/100 [00:01<00:20,  4.48it/s]Measuring inference for batch_size=8:   9%|▉         | 9/100 [00:02<00:20,  4.49it/s]Measuring inference for batch_size=8:  10%|█         | 10/100 [00:02<00:20,  4.50it/s]Measuring inference for batch_size=8:  11%|█         | 11/100 [00:02<00:19,  4.48it/s]Measuring inference for batch_size=8:  12%|█▏        | 12/100 [00:02<00:19,  4.48it/s]Measuring inference for batch_size=8:  13%|█▎        | 13/100 [00:02<00:19,  4.48it/s]Measuring inference for batch_size=8:  14%|█▍        | 14/100 [00:03<00:19,  4.49it/s]Measuring inference for batch_size=8:  15%|█▌        | 15/100 [00:03<00:18,  4.49it/s]Measuring inference for batch_size=8:  16%|█▌        | 16/100 [00:03<00:18,  4.48it/s]Measuring inference for batch_size=8:  17%|█▋        | 17/100 [00:03<00:18,  4.47it/s]Measuring inference for batch_size=8:  18%|█▊        | 18/100 [00:04<00:18,  4.49it/s]Measuring inference for batch_size=8:  19%|█▉        | 19/100 [00:04<00:18,  4.48it/s]Measuring inference for batch_size=8:  20%|██        | 20/100 [00:04<00:17,  4.49it/s]Measuring inference for batch_size=8:  21%|██        | 21/100 [00:04<00:17,  4.50it/s]Measuring inference for batch_size=8:  22%|██▏       | 22/100 [00:04<00:17,  4.49it/s]Measuring inference for batch_size=8:  23%|██▎       | 23/100 [00:05<00:17,  4.50it/s]Measuring inference for batch_size=8:  24%|██▍       | 24/100 [00:05<00:16,  4.47it/s]Measuring inference for batch_size=8:  25%|██▌       | 25/100 [00:05<00:16,  4.49it/s]Measuring inference for batch_size=8:  26%|██▌       | 26/100 [00:05<00:16,  4.47it/s]Measuring inference for batch_size=8:  27%|██▋       | 27/100 [00:06<00:16,  4.45it/s]Measuring inference for batch_size=8:  28%|██▊       | 28/100 [00:06<00:16,  4.44it/s]Measuring inference for batch_size=8:  29%|██▉       | 29/100 [00:06<00:15,  4.46it/s]Measuring inference for batch_size=8:  30%|███       | 30/100 [00:06<00:15,  4.46it/s]Measuring inference for batch_size=8:  31%|███       | 31/100 [00:06<00:15,  4.48it/s]Measuring inference for batch_size=8:  32%|███▏      | 32/100 [00:07<00:15,  4.50it/s]Measuring inference for batch_size=8:  33%|███▎      | 33/100 [00:07<00:14,  4.51it/s]Measuring inference for batch_size=8:  34%|███▍      | 34/100 [00:07<00:14,  4.51it/s]Measuring inference for batch_size=8:  35%|███▌      | 35/100 [00:07<00:14,  4.49it/s]Measuring inference for batch_size=8:  36%|███▌      | 36/100 [00:08<00:14,  4.50it/s]Measuring inference for batch_size=8:  37%|███▋      | 37/100 [00:08<00:13,  4.51it/s]Measuring inference for batch_size=8:  38%|███▊      | 38/100 [00:08<00:13,  4.49it/s]Measuring inference for batch_size=8:  39%|███▉      | 39/100 [00:08<00:13,  4.50it/s]Measuring inference for batch_size=8:  40%|████      | 40/100 [00:08<00:13,  4.51it/s]Measuring inference for batch_size=8:  41%|████      | 41/100 [00:09<00:13,  4.49it/s]Measuring inference for batch_size=8:  42%|████▏     | 42/100 [00:09<00:12,  4.50it/s]Measuring inference for batch_size=8:  43%|████▎     | 43/100 [00:09<00:12,  4.51it/s]Measuring inference for batch_size=8:  44%|████▍     | 44/100 [00:09<00:12,  4.51it/s]Measuring inference for batch_size=8:  45%|████▌     | 45/100 [00:10<00:12,  4.52it/s]Measuring inference for batch_size=8:  46%|████▌     | 46/100 [00:10<00:11,  4.53it/s]Measuring inference for batch_size=8:  47%|████▋     | 47/100 [00:10<00:11,  4.53it/s]Measuring inference for batch_size=8:  48%|████▊     | 48/100 [00:10<00:11,  4.51it/s]Measuring inference for batch_size=8:  49%|████▉     | 49/100 [00:10<00:11,  4.51it/s]Measuring inference for batch_size=8:  50%|█████     | 50/100 [00:11<00:11,  4.52it/s]Measuring inference for batch_size=8:  51%|█████     | 51/100 [00:11<00:10,  4.52it/s]Measuring inference for batch_size=8:  52%|█████▏    | 52/100 [00:11<00:10,  4.51it/s]Measuring inference for batch_size=8:  53%|█████▎    | 53/100 [00:11<00:10,  4.51it/s]Measuring inference for batch_size=8:  54%|█████▍    | 54/100 [00:12<00:10,  4.49it/s]Measuring inference for batch_size=8:  55%|█████▌    | 55/100 [00:12<00:09,  4.50it/s]Measuring inference for batch_size=8:  56%|█████▌    | 56/100 [00:12<00:09,  4.50it/s]Measuring inference for batch_size=8:  57%|█████▋    | 57/100 [00:12<00:09,  4.51it/s]Measuring inference for batch_size=8:  58%|█████▊    | 58/100 [00:12<00:09,  4.51it/s]Measuring inference for batch_size=8:  59%|█████▉    | 59/100 [00:13<00:09,  4.51it/s]Measuring inference for batch_size=8:  60%|██████    | 60/100 [00:13<00:08,  4.52it/s]Measuring inference for batch_size=8:  61%|██████    | 61/100 [00:13<00:08,  4.53it/s]Measuring inference for batch_size=8:  62%|██████▏   | 62/100 [00:13<00:08,  4.53it/s]Measuring inference for batch_size=8:  63%|██████▎   | 63/100 [00:14<00:08,  4.52it/s]Measuring inference for batch_size=8:  64%|██████▍   | 64/100 [00:14<00:07,  4.51it/s]Measuring inference for batch_size=8:  65%|██████▌   | 65/100 [00:14<00:07,  4.52it/s]Measuring inference for batch_size=8:  66%|██████▌   | 66/100 [00:14<00:07,  4.52it/s]Measuring inference for batch_size=8:  67%|██████▋   | 67/100 [00:14<00:07,  4.53it/s]Measuring inference for batch_size=8:  68%|██████▊   | 68/100 [00:15<00:07,  4.50it/s]Measuring inference for batch_size=8:  69%|██████▉   | 69/100 [00:15<00:06,  4.51it/s]Measuring inference for batch_size=8:  70%|███████   | 70/100 [00:15<00:06,  4.51it/s]Measuring inference for batch_size=8:  71%|███████   | 71/100 [00:15<00:06,  4.51it/s]Measuring inference for batch_size=8:  72%|███████▏  | 72/100 [00:16<00:06,  4.52it/s]Measuring inference for batch_size=8:  73%|███████▎  | 73/100 [00:16<00:06,  4.49it/s]Measuring inference for batch_size=8:  74%|███████▍  | 74/100 [00:16<00:05,  4.48it/s]Measuring inference for batch_size=8:  75%|███████▌  | 75/100 [00:16<00:05,  4.49it/s]Measuring inference for batch_size=8:  76%|███████▌  | 76/100 [00:16<00:05,  4.48it/s]Measuring inference for batch_size=8:  77%|███████▋  | 77/100 [00:17<00:05,  4.49it/s]Measuring inference for batch_size=8:  78%|███████▊  | 78/100 [00:17<00:04,  4.47it/s]Measuring inference for batch_size=8:  79%|███████▉  | 79/100 [00:17<00:04,  4.48it/s]Measuring inference for batch_size=8:  80%|████████  | 80/100 [00:17<00:04,  4.48it/s]Measuring inference for batch_size=8:  81%|████████  | 81/100 [00:18<00:04,  4.48it/s]Measuring inference for batch_size=8:  82%|████████▏ | 82/100 [00:18<00:04,  4.49it/s]Measuring inference for batch_size=8:  83%|████████▎ | 83/100 [00:18<00:03,  4.47it/s]Measuring inference for batch_size=8:  84%|████████▍ | 84/100 [00:18<00:03,  4.45it/s]Measuring inference for batch_size=8:  85%|████████▌ | 85/100 [00:18<00:03,  4.45it/s]Measuring inference for batch_size=8:  86%|████████▌ | 86/100 [00:19<00:03,  4.46it/s]Measuring inference for batch_size=8:  87%|████████▋ | 87/100 [00:19<00:02,  4.48it/s]Measuring inference for batch_size=8:  88%|████████▊ | 88/100 [00:19<00:02,  4.49it/s]Measuring inference for batch_size=8:  89%|████████▉ | 89/100 [00:19<00:02,  4.47it/s]Measuring inference for batch_size=8:  90%|█████████ | 90/100 [00:20<00:02,  4.49it/s]Measuring inference for batch_size=8:  91%|█████████ | 91/100 [00:20<00:02,  4.50it/s]Measuring inference for batch_size=8:  92%|█████████▏| 92/100 [00:20<00:01,  4.50it/s]Measuring inference for batch_size=8:  93%|█████████▎| 93/100 [00:20<00:01,  4.50it/s]Measuring inference for batch_size=8:  94%|█████████▍| 94/100 [00:20<00:01,  4.48it/s]Measuring inference for batch_size=8:  95%|█████████▌| 95/100 [00:21<00:01,  4.47it/s]Measuring inference for batch_size=8:  96%|█████████▌| 96/100 [00:21<00:00,  4.48it/s]Measuring inference for batch_size=8:  97%|█████████▋| 97/100 [00:21<00:00,  4.48it/s]Measuring inference for batch_size=8:  98%|█████████▊| 98/100 [00:21<00:00,  4.50it/s]Measuring inference for batch_size=8:  99%|█████████▉| 99/100 [00:22<00:00,  4.51it/s]Measuring inference for batch_size=8: 100%|██████████| 100/100 [00:22<00:00,  4.51it/s]Measuring inference for batch_size=8: 100%|██████████| 100/100 [00:22<00:00,  4.49it/s]
Measuring energy for batch_size=1:   0%|          | 0/10 [00:00<?, ?it/s]Measuring energy for batch_size=1:  10%|█         | 1/10 [00:00<00:03,  2.87it/s]Measuring energy for batch_size=1:  20%|██        | 2/10 [00:00<00:02,  3.38it/s]Measuring energy for batch_size=1:  30%|███       | 3/10 [00:00<00:01,  3.56it/s]Measuring energy for batch_size=1:  40%|████      | 4/10 [00:01<00:01,  3.67it/s]Measuring energy for batch_size=1:  50%|█████     | 5/10 [00:01<00:01,  3.74it/s]Measuring energy for batch_size=1:  60%|██████    | 6/10 [00:01<00:01,  3.78it/s]Measuring energy for batch_size=1:  70%|███████   | 7/10 [00:01<00:00,  3.79it/s]Measuring energy for batch_size=1:  80%|████████  | 8/10 [00:02<00:00,  3.81it/s]Measuring energy for batch_size=1:  90%|█████████ | 9/10 [00:02<00:00,  3.82it/s]Measuring energy for batch_size=1: 100%|██████████| 10/10 [00:02<00:00,  3.83it/s]Measuring energy for batch_size=1: 100%|██████████| 10/10 [00:02<00:00,  3.72it/s]
Warming up with batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=8:  10%|█         | 1/10 [00:01<00:11,  1.25s/it]Warming up with batch_size=8:  20%|██        | 2/10 [00:02<00:09,  1.25s/it]Warming up with batch_size=8:  30%|███       | 3/10 [00:03<00:08,  1.25s/it]Warming up with batch_size=8:  40%|████      | 4/10 [00:05<00:07,  1.25s/it]Warming up with batch_size=8:  50%|█████     | 5/10 [00:06<00:06,  1.25s/it]Warming up with batch_size=8:  60%|██████    | 6/10 [00:07<00:05,  1.25s/it]Warming up with batch_size=8:  70%|███████   | 7/10 [00:08<00:03,  1.25s/it]Warming up with batch_size=8:  80%|████████  | 8/10 [00:10<00:02,  1.25s/it]Warming up with batch_size=8:  90%|█████████ | 9/10 [00:11<00:01,  1.25s/it]Warming up with batch_size=8: 100%|██████████| 10/10 [00:12<00:00,  1.26s/it]Warming up with batch_size=8: 100%|██████████| 10/10 [00:12<00:00,  1.25s/it]
Measuring inference for batch_size=8:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference for batch_size=8:   1%|          | 1/100 [00:01<02:04,  1.26s/it]Measuring inference for batch_size=8:   2%|▏         | 2/100 [00:02<02:02,  1.25s/it]Measuring inference for batch_size=8:   3%|▎         | 3/100 [00:03<02:01,  1.25s/it]Measuring inference for batch_size=8:   4%|▍         | 4/100 [00:05<02:00,  1.25s/it]Measuring inference for batch_size=8:   5%|▌         | 5/100 [00:06<01:59,  1.25s/it]Measuring inference for batch_size=8:   6%|▌         | 6/100 [00:07<01:57,  1.25s/it]Measuring inference for batch_size=8:   7%|▋         | 7/100 [00:08<01:56,  1.25s/it]Measuring inference for batch_size=8:   8%|▊         | 8/100 [00:10<01:55,  1.25s/it]Measuring inference for batch_size=8:   9%|▉         | 9/100 [00:11<01:53,  1.25s/it]Measuring inference for batch_size=8:  10%|█         | 10/100 [00:12<01:52,  1.25s/it]Measuring inference for batch_size=8:  11%|█         | 11/100 [00:13<01:51,  1.25s/it]Measuring inference for batch_size=8:  12%|█▏        | 12/100 [00:15<01:50,  1.25s/it]Measuring inference for batch_size=8:  13%|█▎        | 13/100 [00:16<01:49,  1.25s/it]Measuring inference for batch_size=8:  14%|█▍        | 14/100 [00:17<01:47,  1.25s/it]Measuring inference for batch_size=8:  15%|█▌        | 15/100 [00:18<01:46,  1.25s/it]Measuring inference for batch_size=8:  16%|█▌        | 16/100 [00:20<01:45,  1.25s/it]Measuring inference for batch_size=8:  17%|█▋        | 17/100 [00:21<01:43,  1.25s/it]Measuring inference for batch_size=8:  18%|█▊        | 18/100 [00:22<01:42,  1.25s/it]Measuring inference for batch_size=8:  19%|█▉        | 19/100 [00:23<01:41,  1.25s/it]Measuring inference for batch_size=8:  20%|██        | 20/100 [00:25<01:40,  1.25s/it]Measuring inference for batch_size=8:  21%|██        | 21/100 [00:26<01:39,  1.25s/it]Measuring inference for batch_size=8:  22%|██▏       | 22/100 [00:27<01:37,  1.25s/it]Measuring inference for batch_size=8:  23%|██▎       | 23/100 [00:28<01:36,  1.25s/it]Measuring inference for batch_size=8:  24%|██▍       | 24/100 [00:30<01:35,  1.25s/it]Measuring inference for batch_size=8:  25%|██▌       | 25/100 [00:31<01:33,  1.25s/it]Measuring inference for batch_size=8:  26%|██▌       | 26/100 [00:32<01:32,  1.25s/it]Measuring inference for batch_size=8:  27%|██▋       | 27/100 [00:33<01:31,  1.25s/it]Measuring inference for batch_size=8:  28%|██▊       | 28/100 [00:35<01:30,  1.25s/it]Measuring inference for batch_size=8:  29%|██▉       | 29/100 [00:36<01:28,  1.25s/it]Measuring inference for batch_size=8:  30%|███       | 30/100 [00:37<01:27,  1.25s/it]Measuring inference for batch_size=8:  31%|███       | 31/100 [00:38<01:26,  1.25s/it]Measuring inference for batch_size=8:  32%|███▏      | 32/100 [00:40<01:25,  1.25s/it]Measuring inference for batch_size=8:  33%|███▎      | 33/100 [00:41<01:23,  1.25s/it]Measuring inference for batch_size=8:  34%|███▍      | 34/100 [00:42<01:22,  1.25s/it]Measuring inference for batch_size=8:  35%|███▌      | 35/100 [00:43<01:21,  1.25s/it]Measuring inference for batch_size=8:  36%|███▌      | 36/100 [00:45<01:20,  1.25s/it]Measuring inference for batch_size=8:  37%|███▋      | 37/100 [00:46<01:18,  1.25s/it]Measuring inference for batch_size=8:  38%|███▊      | 38/100 [00:47<01:17,  1.25s/it]Measuring inference for batch_size=8:  39%|███▉      | 39/100 [00:48<01:16,  1.25s/it]Measuring inference for batch_size=8:  40%|████      | 40/100 [00:50<01:15,  1.25s/it]Measuring inference for batch_size=8:  41%|████      | 41/100 [00:51<01:13,  1.25s/it]Measuring inference for batch_size=8:  42%|████▏     | 42/100 [00:52<01:12,  1.25s/it]Measuring inference for batch_size=8:  43%|████▎     | 43/100 [00:53<01:11,  1.25s/it]Measuring inference for batch_size=8:  44%|████▍     | 44/100 [00:55<01:10,  1.25s/it]Measuring inference for batch_size=8:  45%|████▌     | 45/100 [00:56<01:08,  1.25s/it]Measuring inference for batch_size=8:  46%|████▌     | 46/100 [00:57<01:07,  1.25s/it]Measuring inference for batch_size=8:  47%|████▋     | 47/100 [00:58<01:06,  1.25s/it]Measuring inference for batch_size=8:  48%|████▊     | 48/100 [01:00<01:05,  1.25s/it]Measuring inference for batch_size=8:  49%|████▉     | 49/100 [01:01<01:03,  1.25s/it]Measuring inference for batch_size=8:  50%|█████     | 50/100 [01:02<01:02,  1.25s/it]Measuring inference for batch_size=8:  51%|█████     | 51/100 [01:03<01:01,  1.25s/it]Measuring inference for batch_size=8:  52%|█████▏    | 52/100 [01:05<01:00,  1.25s/it]Measuring inference for batch_size=8:  53%|█████▎    | 53/100 [01:06<00:58,  1.25s/it]Measuring inference for batch_size=8:  54%|█████▍    | 54/100 [01:07<00:57,  1.25s/it]Measuring inference for batch_size=8:  55%|█████▌    | 55/100 [01:08<00:56,  1.25s/it]Measuring inference for batch_size=8:  56%|█████▌    | 56/100 [01:10<00:55,  1.25s/it]Measuring inference for batch_size=8:  57%|█████▋    | 57/100 [01:11<00:53,  1.25s/it]Measuring inference for batch_size=8:  58%|█████▊    | 58/100 [01:12<00:52,  1.25s/it]Measuring inference for batch_size=8:  59%|█████▉    | 59/100 [01:13<00:51,  1.26s/it]Measuring inference for batch_size=8:  60%|██████    | 60/100 [01:15<00:50,  1.25s/it]Measuring inference for batch_size=8:  61%|██████    | 61/100 [01:16<00:48,  1.26s/it]Measuring inference for batch_size=8:  62%|██████▏   | 62/100 [01:17<00:47,  1.26s/it]Measuring inference for batch_size=8:  63%|██████▎   | 63/100 [01:18<00:46,  1.26s/it]Measuring inference for batch_size=8:  64%|██████▍   | 64/100 [01:20<00:45,  1.25s/it]Measuring inference for batch_size=8:  65%|██████▌   | 65/100 [01:21<00:43,  1.25s/it]Measuring inference for batch_size=8:  66%|██████▌   | 66/100 [01:22<00:42,  1.25s/it]Measuring inference for batch_size=8:  67%|██████▋   | 67/100 [01:23<00:41,  1.25s/it]Measuring inference for batch_size=8:  68%|██████▊   | 68/100 [01:25<00:40,  1.25s/it]Measuring inference for batch_size=8:  69%|██████▉   | 69/100 [01:26<00:38,  1.25s/it]Measuring inference for batch_size=8:  70%|███████   | 70/100 [01:27<00:37,  1.25s/it]Measuring inference for batch_size=8:  71%|███████   | 71/100 [01:28<00:36,  1.25s/it]Measuring inference for batch_size=8:  72%|███████▏  | 72/100 [01:30<00:35,  1.25s/it]Measuring inference for batch_size=8:  73%|███████▎  | 73/100 [01:31<00:33,  1.25s/it]Measuring inference for batch_size=8:  74%|███████▍  | 74/100 [01:32<00:32,  1.25s/it]Measuring inference for batch_size=8:  75%|███████▌  | 75/100 [01:33<00:31,  1.25s/it]Measuring inference for batch_size=8:  76%|███████▌  | 76/100 [01:35<00:30,  1.25s/it]Measuring inference for batch_size=8:  77%|███████▋  | 77/100 [01:36<00:28,  1.25s/it]Measuring inference for batch_size=8:  78%|███████▊  | 78/100 [01:37<00:27,  1.25s/it]Measuring inference for batch_size=8:  79%|███████▉  | 79/100 [01:38<00:26,  1.25s/it]Measuring inference for batch_size=8:  80%|████████  | 80/100 [01:40<00:25,  1.25s/it]Measuring inference for batch_size=8:  81%|████████  | 81/100 [01:41<00:23,  1.25s/it]Measuring inference for batch_size=8:  82%|████████▏ | 82/100 [01:42<00:22,  1.26s/it]Measuring inference for batch_size=8:  83%|████████▎ | 83/100 [01:43<00:21,  1.25s/it]Measuring inference for batch_size=8:  84%|████████▍ | 84/100 [01:45<00:20,  1.25s/it]Measuring inference for batch_size=8:  85%|████████▌ | 85/100 [01:46<00:18,  1.25s/it]Measuring inference for batch_size=8:  86%|████████▌ | 86/100 [01:47<00:17,  1.25s/it]Measuring inference for batch_size=8:  87%|████████▋ | 87/100 [01:49<00:16,  1.25s/it]Measuring inference for batch_size=8:  88%|████████▊ | 88/100 [01:50<00:15,  1.25s/it]Measuring inference for batch_size=8:  89%|████████▉ | 89/100 [01:51<00:13,  1.25s/it]Measuring inference for batch_size=8:  90%|█████████ | 90/100 [01:52<00:12,  1.25s/it]Measuring inference for batch_size=8:  91%|█████████ | 91/100 [01:54<00:11,  1.25s/it]Measuring inference for batch_size=8:  92%|█████████▏| 92/100 [01:55<00:10,  1.25s/it]Measuring inference for batch_size=8:  93%|█████████▎| 93/100 [01:56<00:08,  1.25s/it]Measuring inference for batch_size=8:  94%|█████████▍| 94/100 [01:57<00:07,  1.25s/it]Measuring inference for batch_size=8:  95%|█████████▌| 95/100 [01:59<00:06,  1.25s/it]Measuring inference for batch_size=8:  96%|█████████▌| 96/100 [02:00<00:05,  1.25s/it]Measuring inference for batch_size=8:  97%|█████████▋| 97/100 [02:01<00:03,  1.25s/it]Measuring inference for batch_size=8:  98%|█████████▊| 98/100 [02:02<00:02,  1.25s/it]Measuring inference for batch_size=8:  99%|█████████▉| 99/100 [02:04<00:01,  1.25s/it]Measuring inference for batch_size=8: 100%|██████████| 100/100 [02:05<00:00,  1.25s/it]Measuring inference for batch_size=8: 100%|██████████| 100/100 [02:05<00:00,  1.25s/it]
Measuring energy for batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Measuring energy for batch_size=8:  10%|█         | 1/10 [00:01<00:12,  1.37s/it]Measuring energy for batch_size=8:  20%|██        | 2/10 [00:02<00:10,  1.32s/it]Measuring energy for batch_size=8:  30%|███       | 3/10 [00:03<00:09,  1.30s/it]Measuring energy for batch_size=8:  40%|████      | 4/10 [00:05<00:07,  1.30s/it]Measuring energy for batch_size=8:  50%|█████     | 5/10 [00:06<00:06,  1.29s/it]Measuring energy for batch_size=8:  60%|██████    | 6/10 [00:07<00:05,  1.29s/it]Measuring energy for batch_size=8:  70%|███████   | 7/10 [00:09<00:03,  1.29s/it]Measuring energy for batch_size=8:  80%|████████  | 8/10 [00:10<00:02,  1.29s/it]Measuring energy for batch_size=8:  90%|█████████ | 9/10 [00:11<00:01,  1.29s/it]Measuring energy for batch_size=8: 100%|██████████| 10/10 [00:12<00:00,  1.29s/it]Measuring energy for batch_size=8: 100%|██████████| 10/10 [00:12<00:00,  1.30s/it]
learner.infer:
  device: cuda
  energy:
    batch_size_1:
      joules: 5.489291322642962
      kWh: 1.5248031451786006e-06
    batch_size_8:
      joules: 39.97347627270062
      kWh: 1.1103743409083506e-05
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 13.88 GB
      total: 31.17 GB
      used: 16.86 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: 11.532 us +/- 5.961 us [6.676 us, 55.552 us]
          batches_per_second: 97.90 K +/- 28.08 K [18.00 K, 149.80 K]
        metrics:
          batches_per_second_max: 149796.57142857142
          batches_per_second_mean: 97904.74354136929
          batches_per_second_min: 18001.304721030043
          batches_per_second_std: 28083.77043734201
          seconds_per_batch_max: 5.555152893066406e-05
          seconds_per_batch_mean: 1.1532306671142578e-05
          seconds_per_batch_min: 6.67572021484375e-06
          seconds_per_batch_std: 5.960813035310177e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: 404.294 us +/- 99.347 us [236.511 us, 707.626 us]
          batches_per_second: 2.62 K +/- 630.32 [1.41 K, 4.23 K]
        metrics:
          batches_per_second_max: 4228.129032258064
          batches_per_second_mean: 2620.4619054979557
          batches_per_second_min: 1413.1752021563343
          batches_per_second_std: 630.321229177948
          seconds_per_batch_max: 0.0007076263427734375
          seconds_per_batch_mean: 0.0004042935371398926
          seconds_per_batch_min: 0.00023651123046875
          seconds_per_batch_std: 9.93466094032849e-05
      on_device_inference:
        human_readable:
          batch_latency: 220.975 ms +/- 1.919 ms [218.511 ms, 226.070 ms]
          batches_per_second: 4.53 +/- 0.04 [4.42, 4.58]
        metrics:
          batches_per_second_max: 4.576421462886646
          batches_per_second_mean: 4.525739823685949
          batches_per_second_min: 4.423410102867942
          batches_per_second_std: 0.03902899360460757
          seconds_per_batch_max: 0.22606992721557617
          seconds_per_batch_mean: 0.2209748935699463
          seconds_per_batch_min: 0.21851134300231934
          seconds_per_batch_std: 0.0019185212430644799
      total:
        human_readable:
          batch_latency: 221.391 ms +/- 1.920 ms [218.800 ms, 226.571 ms]
          batches_per_second: 4.52 +/- 0.04 [4.41, 4.57]
        metrics:
          batches_per_second_max: 4.570392454277595
          batches_per_second_mean: 4.517238463619422
          batches_per_second_min: 4.413621253718527
          batches_per_second_std: 0.038902638760777576
          seconds_per_batch_max: 0.22657132148742676
          seconds_per_batch_mean: 0.22139071941375732
          seconds_per_batch_min: 0.21879959106445312
          seconds_per_batch_std: 0.0019195469948896655
    batch_size_8:
      cpu_to_gpu:
        human_readable:
          batch_latency: 11.647 us +/- 3.930 us [9.060 us, 48.399 us]
          batches_per_second: 89.15 K +/- 11.93 K [20.66 K, 110.38 K]
        metrics:
          batches_per_second_max: 110376.42105263157
          batches_per_second_mean: 89152.44569596986
          batches_per_second_min: 20661.5960591133
          batches_per_second_std: 11928.966989095536
          seconds_per_batch_max: 4.839897155761719e-05
          seconds_per_batch_mean: 1.1646747589111328e-05
          seconds_per_batch_min: 9.059906005859375e-06
          seconds_per_batch_std: 3.929840500802262e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: 1.181 ms +/- 112.980 us [1.065 ms, 2.070 ms]
          batches_per_second: 852.17 +/- 59.50 [483.16, 938.53]
        metrics:
          batches_per_second_max: 938.5330051465652
          batches_per_second_mean: 852.174297480183
          batches_per_second_min: 483.1590830549476
          batches_per_second_std: 59.50403179773207
          seconds_per_batch_max: 0.002069711685180664
          seconds_per_batch_mean: 0.0011811184883117676
          seconds_per_batch_min: 0.0010654926300048828
          seconds_per_batch_std: 0.0001129799082150227
      on_device_inference:
        human_readable:
          batch_latency: 1.251 s +/- 2.700 ms [1.245 s, 1.261 s]
          batches_per_second: 0.80 +/- 0.00 [0.79, 0.80]
        metrics:
          batches_per_second_max: 0.802946604283603
          batches_per_second_mean: 0.7996201289228586
          batches_per_second_min: 0.7927218515309725
          batches_per_second_std: 0.0017234097609708774
          seconds_per_batch_max: 1.2614765167236328
          seconds_per_batch_mean: 1.2505996489524842
          seconds_per_batch_min: 1.245412826538086
          seconds_per_batch_std: 0.002699624141723418
      total:
        human_readable:
          batch_latency: 1.252 s +/- 2.716 ms [1.247 s, 1.263 s]
          batches_per_second: 0.80 +/- 0.00 [0.79, 0.80]
        metrics:
          batches_per_second_max: 0.8022158986366379
          batches_per_second_mean: 0.7988582517783375
          batches_per_second_min: 0.7920003565065081
          batches_per_second_std: 0.001730468691073114
          seconds_per_batch_max: 1.2626256942749023
          seconds_per_batch_mean: 1.251792414188385
          seconds_per_batch_min: 1.2465472221374512
          seconds_per_batch_std: 0.002715807598591402

== Benchmarking model directly ==
Warming up with batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=8:  10%|█         | 1/10 [00:00<00:01,  4.69it/s]Warming up with batch_size=8:  20%|██        | 2/10 [00:00<00:01,  4.68it/s]Warming up with batch_size=8:  30%|███       | 3/10 [00:00<00:01,  4.68it/s]Warming up with batch_size=8:  40%|████      | 4/10 [00:00<00:01,  4.68it/s]Warming up with batch_size=8:  50%|█████     | 5/10 [00:01<00:01,  4.68it/s]Warming up with batch_size=8:  60%|██████    | 6/10 [00:01<00:00,  4.68it/s]Warming up with batch_size=8:  70%|███████   | 7/10 [00:01<00:00,  4.67it/s]Warming up with batch_size=8:  80%|████████  | 8/10 [00:01<00:00,  4.68it/s]Warming up with batch_size=8:  90%|█████████ | 9/10 [00:01<00:00,  4.68it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:02<00:00,  4.68it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:02<00:00,  4.67it/s]
Measuring inference for batch_size=8:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference for batch_size=8:   1%|          | 1/100 [00:00<00:21,  4.70it/s]Measuring inference for batch_size=8:   2%|▏         | 2/100 [00:00<00:20,  4.70it/s]Measuring inference for batch_size=8:   3%|▎         | 3/100 [00:00<00:20,  4.69it/s]Measuring inference for batch_size=8:   4%|▍         | 4/100 [00:00<00:20,  4.69it/s]Measuring inference for batch_size=8:   5%|▌         | 5/100 [00:01<00:20,  4.68it/s]Measuring inference for batch_size=8:   6%|▌         | 6/100 [00:01<00:20,  4.68it/s]Measuring inference for batch_size=8:   7%|▋         | 7/100 [00:01<00:19,  4.68it/s]Measuring inference for batch_size=8:   8%|▊         | 8/100 [00:01<00:19,  4.68it/s]Measuring inference for batch_size=8:   9%|▉         | 9/100 [00:01<00:19,  4.69it/s]Measuring inference for batch_size=8:  10%|█         | 10/100 [00:02<00:19,  4.69it/s]Measuring inference for batch_size=8:  11%|█         | 11/100 [00:02<00:19,  4.68it/s]Measuring inference for batch_size=8:  12%|█▏        | 12/100 [00:02<00:18,  4.69it/s]Measuring inference for batch_size=8:  13%|█▎        | 13/100 [00:02<00:18,  4.69it/s]Measuring inference for batch_size=8:  14%|█▍        | 14/100 [00:02<00:18,  4.69it/s]Measuring inference for batch_size=8:  15%|█▌        | 15/100 [00:03<00:18,  4.69it/s]Measuring inference for batch_size=8:  16%|█▌        | 16/100 [00:03<00:17,  4.68it/s]Measuring inference for batch_size=8:  17%|█▋        | 17/100 [00:03<00:17,  4.68it/s]Measuring inference for batch_size=8:  18%|█▊        | 18/100 [00:03<00:17,  4.68it/s]Measuring inference for batch_size=8:  19%|█▉        | 19/100 [00:04<00:17,  4.68it/s]Measuring inference for batch_size=8:  20%|██        | 20/100 [00:04<00:17,  4.68it/s]Measuring inference for batch_size=8:  21%|██        | 21/100 [00:04<00:16,  4.69it/s]Measuring inference for batch_size=8:  22%|██▏       | 22/100 [00:04<00:16,  4.69it/s]Measuring inference for batch_size=8:  23%|██▎       | 23/100 [00:04<00:16,  4.69it/s]Measuring inference for batch_size=8:  24%|██▍       | 24/100 [00:05<00:16,  4.69it/s]Measuring inference for batch_size=8:  25%|██▌       | 25/100 [00:05<00:15,  4.69it/s]Measuring inference for batch_size=8:  26%|██▌       | 26/100 [00:05<00:15,  4.70it/s]Measuring inference for batch_size=8:  27%|██▋       | 27/100 [00:05<00:15,  4.69it/s]Measuring inference for batch_size=8:  28%|██▊       | 28/100 [00:05<00:15,  4.69it/s]Measuring inference for batch_size=8:  29%|██▉       | 29/100 [00:06<00:15,  4.69it/s]Measuring inference for batch_size=8:  30%|███       | 30/100 [00:06<00:14,  4.69it/s]Measuring inference for batch_size=8:  31%|███       | 31/100 [00:06<00:14,  4.69it/s]Measuring inference for batch_size=8:  32%|███▏      | 32/100 [00:06<00:14,  4.69it/s]Measuring inference for batch_size=8:  33%|███▎      | 33/100 [00:07<00:14,  4.69it/s]Measuring inference for batch_size=8:  34%|███▍      | 34/100 [00:07<00:14,  4.69it/s]Measuring inference for batch_size=8:  35%|███▌      | 35/100 [00:07<00:13,  4.69it/s]Measuring inference for batch_size=8:  36%|███▌      | 36/100 [00:07<00:13,  4.69it/s]Measuring inference for batch_size=8:  37%|███▋      | 37/100 [00:07<00:13,  4.69it/s]Measuring inference for batch_size=8:  38%|███▊      | 38/100 [00:08<00:13,  4.69it/s]Measuring inference for batch_size=8:  39%|███▉      | 39/100 [00:08<00:13,  4.69it/s]Measuring inference for batch_size=8:  40%|████      | 40/100 [00:08<00:12,  4.69it/s]Measuring inference for batch_size=8:  41%|████      | 41/100 [00:08<00:12,  4.69it/s]Measuring inference for batch_size=8:  42%|████▏     | 42/100 [00:08<00:12,  4.69it/s]Measuring inference for batch_size=8:  43%|████▎     | 43/100 [00:09<00:12,  4.69it/s]Measuring inference for batch_size=8:  44%|████▍     | 44/100 [00:09<00:11,  4.69it/s]Measuring inference for batch_size=8:  45%|████▌     | 45/100 [00:09<00:11,  4.69it/s]Measuring inference for batch_size=8:  46%|████▌     | 46/100 [00:09<00:11,  4.69it/s]Measuring inference for batch_size=8:  47%|████▋     | 47/100 [00:10<00:11,  4.69it/s]Measuring inference for batch_size=8:  48%|████▊     | 48/100 [00:10<00:11,  4.69it/s]Measuring inference for batch_size=8:  49%|████▉     | 49/100 [00:10<00:10,  4.69it/s]Measuring inference for batch_size=8:  50%|█████     | 50/100 [00:10<00:10,  4.69it/s]Measuring inference for batch_size=8:  51%|█████     | 51/100 [00:10<00:10,  4.69it/s]Measuring inference for batch_size=8:  52%|█████▏    | 52/100 [00:11<00:10,  4.67it/s]Measuring inference for batch_size=8:  53%|█████▎    | 53/100 [00:11<00:10,  4.67it/s]Measuring inference for batch_size=8:  54%|█████▍    | 54/100 [00:11<00:09,  4.68it/s]Measuring inference for batch_size=8:  55%|█████▌    | 55/100 [00:11<00:09,  4.68it/s]Measuring inference for batch_size=8:  56%|█████▌    | 56/100 [00:11<00:09,  4.68it/s]Measuring inference for batch_size=8:  57%|█████▋    | 57/100 [00:12<00:09,  4.69it/s]Measuring inference for batch_size=8:  58%|█████▊    | 58/100 [00:12<00:08,  4.69it/s]Measuring inference for batch_size=8:  59%|█████▉    | 59/100 [00:12<00:08,  4.69it/s]Measuring inference for batch_size=8:  60%|██████    | 60/100 [00:12<00:08,  4.69it/s]Measuring inference for batch_size=8:  61%|██████    | 61/100 [00:13<00:08,  4.69it/s]Measuring inference for batch_size=8:  62%|██████▏   | 62/100 [00:13<00:08,  4.69it/s]Measuring inference for batch_size=8:  63%|██████▎   | 63/100 [00:13<00:07,  4.69it/s]Measuring inference for batch_size=8:  64%|██████▍   | 64/100 [00:13<00:07,  4.69it/s]Measuring inference for batch_size=8:  65%|██████▌   | 65/100 [00:13<00:07,  4.68it/s]Measuring inference for batch_size=8:  66%|██████▌   | 66/100 [00:14<00:07,  4.69it/s]Measuring inference for batch_size=8:  67%|██████▋   | 67/100 [00:14<00:07,  4.69it/s]Measuring inference for batch_size=8:  68%|██████▊   | 68/100 [00:14<00:06,  4.69it/s]Measuring inference for batch_size=8:  69%|██████▉   | 69/100 [00:14<00:06,  4.69it/s]Measuring inference for batch_size=8:  70%|███████   | 70/100 [00:14<00:06,  4.69it/s]Measuring inference for batch_size=8:  71%|███████   | 71/100 [00:15<00:06,  4.69it/s]Measuring inference for batch_size=8:  72%|███████▏  | 72/100 [00:15<00:05,  4.69it/s]Measuring inference for batch_size=8:  73%|███████▎  | 73/100 [00:15<00:05,  4.69it/s]Measuring inference for batch_size=8:  74%|███████▍  | 74/100 [00:15<00:05,  4.69it/s]Measuring inference for batch_size=8:  75%|███████▌  | 75/100 [00:15<00:05,  4.69it/s]Measuring inference for batch_size=8:  76%|███████▌  | 76/100 [00:16<00:05,  4.69it/s]Measuring inference for batch_size=8:  77%|███████▋  | 77/100 [00:16<00:04,  4.69it/s]Measuring inference for batch_size=8:  78%|███████▊  | 78/100 [00:16<00:04,  4.69it/s]Measuring inference for batch_size=8:  79%|███████▉  | 79/100 [00:16<00:04,  4.69it/s]Measuring inference for batch_size=8:  80%|████████  | 80/100 [00:17<00:04,  4.69it/s]Measuring inference for batch_size=8:  81%|████████  | 81/100 [00:17<00:04,  4.69it/s]Measuring inference for batch_size=8:  82%|████████▏ | 82/100 [00:17<00:03,  4.69it/s]Measuring inference for batch_size=8:  83%|████████▎ | 83/100 [00:17<00:03,  4.69it/s]Measuring inference for batch_size=8:  84%|████████▍ | 84/100 [00:17<00:03,  4.69it/s]Measuring inference for batch_size=8:  85%|████████▌ | 85/100 [00:18<00:03,  4.69it/s]Measuring inference for batch_size=8:  86%|████████▌ | 86/100 [00:18<00:02,  4.69it/s]Measuring inference for batch_size=8:  87%|████████▋ | 87/100 [00:18<00:02,  4.70it/s]Measuring inference for batch_size=8:  88%|████████▊ | 88/100 [00:18<00:02,  4.70it/s]Measuring inference for batch_size=8:  89%|████████▉ | 89/100 [00:18<00:02,  4.70it/s]Measuring inference for batch_size=8:  90%|█████████ | 90/100 [00:19<00:02,  4.70it/s]Measuring inference for batch_size=8:  91%|█████████ | 91/100 [00:19<00:01,  4.70it/s]Measuring inference for batch_size=8:  92%|█████████▏| 92/100 [00:19<00:01,  4.70it/s]Measuring inference for batch_size=8:  93%|█████████▎| 93/100 [00:19<00:01,  4.70it/s]Measuring inference for batch_size=8:  94%|█████████▍| 94/100 [00:20<00:01,  4.69it/s]Measuring inference for batch_size=8:  95%|█████████▌| 95/100 [00:20<00:01,  4.69it/s]Measuring inference for batch_size=8:  96%|█████████▌| 96/100 [00:20<00:00,  4.69it/s]Measuring inference for batch_size=8:  97%|█████████▋| 97/100 [00:20<00:00,  4.69it/s]Measuring inference for batch_size=8:  98%|█████████▊| 98/100 [00:20<00:00,  4.69it/s]Measuring inference for batch_size=8:  99%|█████████▉| 99/100 [00:21<00:00,  4.69it/s]Measuring inference for batch_size=8: 100%|██████████| 100/100 [00:21<00:00,  4.69it/s]Measuring inference for batch_size=8: 100%|██████████| 100/100 [00:21<00:00,  4.69it/s]
Measuring energy for batch_size=1:   0%|          | 0/10 [00:00<?, ?it/s]Measuring energy for batch_size=1:  10%|█         | 1/10 [00:00<00:03,  2.82it/s]Measuring energy for batch_size=1:  20%|██        | 2/10 [00:00<00:02,  3.33it/s]Measuring energy for batch_size=1:  30%|███       | 3/10 [00:00<00:01,  3.54it/s]Measuring energy for batch_size=1:  40%|████      | 4/10 [00:01<00:01,  3.64it/s]Measuring energy for batch_size=1:  50%|█████     | 5/10 [00:01<00:01,  3.72it/s]Measuring energy for batch_size=1:  60%|██████    | 6/10 [00:01<00:01,  3.76it/s]Measuring energy for batch_size=1:  70%|███████   | 7/10 [00:01<00:00,  3.79it/s]Measuring energy for batch_size=1:  80%|████████  | 8/10 [00:02<00:00,  3.81it/s]Measuring energy for batch_size=1:  90%|█████████ | 9/10 [00:02<00:00,  3.82it/s]Measuring energy for batch_size=1: 100%|██████████| 10/10 [00:02<00:00,  3.83it/s]Measuring energy for batch_size=1: 100%|██████████| 10/10 [00:02<00:00,  3.71it/s]
Warming up with batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=8:  10%|█         | 1/10 [00:01<00:10,  1.21s/it]Warming up with batch_size=8:  20%|██        | 2/10 [00:02<00:09,  1.21s/it]Warming up with batch_size=8:  30%|███       | 3/10 [00:03<00:08,  1.21s/it]Warming up with batch_size=8:  40%|████      | 4/10 [00:04<00:07,  1.21s/it]Warming up with batch_size=8:  50%|█████     | 5/10 [00:06<00:06,  1.22s/it]Warming up with batch_size=8:  60%|██████    | 6/10 [00:07<00:04,  1.22s/it]Warming up with batch_size=8:  70%|███████   | 7/10 [00:08<00:03,  1.22s/it]Warming up with batch_size=8:  80%|████████  | 8/10 [00:09<00:02,  1.22s/it]Warming up with batch_size=8:  90%|█████████ | 9/10 [00:10<00:01,  1.22s/it]Warming up with batch_size=8: 100%|██████████| 10/10 [00:12<00:00,  1.22s/it]Warming up with batch_size=8: 100%|██████████| 10/10 [00:12<00:00,  1.22s/it]
Measuring inference for batch_size=8:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference for batch_size=8:   1%|          | 1/100 [00:01<02:00,  1.22s/it]Measuring inference for batch_size=8:   2%|▏         | 2/100 [00:02<01:58,  1.21s/it]Measuring inference for batch_size=8:   3%|▎         | 3/100 [00:03<01:57,  1.21s/it]Measuring inference for batch_size=8:   4%|▍         | 4/100 [00:04<01:56,  1.22s/it]Measuring inference for batch_size=8:   5%|▌         | 5/100 [00:06<01:55,  1.22s/it]Measuring inference for batch_size=8:   6%|▌         | 6/100 [00:07<01:54,  1.22s/it]Measuring inference for batch_size=8:   7%|▋         | 7/100 [00:08<01:53,  1.22s/it]Measuring inference for batch_size=8:   8%|▊         | 8/100 [00:09<01:52,  1.22s/it]Measuring inference for batch_size=8:   9%|▉         | 9/100 [00:10<01:50,  1.22s/it]Measuring inference for batch_size=8:  10%|█         | 10/100 [00:12<01:49,  1.22s/it]Measuring inference for batch_size=8:  11%|█         | 11/100 [00:13<01:48,  1.22s/it]Measuring inference for batch_size=8:  12%|█▏        | 12/100 [00:14<01:46,  1.22s/it]Measuring inference for batch_size=8:  13%|█▎        | 13/100 [00:15<01:45,  1.22s/it]Measuring inference for batch_size=8:  14%|█▍        | 14/100 [00:17<01:44,  1.22s/it]Measuring inference for batch_size=8:  15%|█▌        | 15/100 [00:18<01:43,  1.22s/it]Measuring inference for batch_size=8:  16%|█▌        | 16/100 [00:19<01:42,  1.22s/it]Measuring inference for batch_size=8:  17%|█▋        | 17/100 [00:20<01:40,  1.22s/it]Measuring inference for batch_size=8:  18%|█▊        | 18/100 [00:21<01:39,  1.22s/it]Measuring inference for batch_size=8:  19%|█▉        | 19/100 [00:23<01:38,  1.22s/it]Measuring inference for batch_size=8:  20%|██        | 20/100 [00:24<01:37,  1.22s/it]Measuring inference for batch_size=8:  21%|██        | 21/100 [00:25<01:35,  1.21s/it]Measuring inference for batch_size=8:  22%|██▏       | 22/100 [00:26<01:34,  1.21s/it]Measuring inference for batch_size=8:  23%|██▎       | 23/100 [00:27<01:33,  1.21s/it]Measuring inference for batch_size=8:  24%|██▍       | 24/100 [00:29<01:32,  1.21s/it]Measuring inference for batch_size=8:  25%|██▌       | 25/100 [00:30<01:31,  1.21s/it]Measuring inference for batch_size=8:  26%|██▌       | 26/100 [00:31<01:29,  1.22s/it]Measuring inference for batch_size=8:  27%|██▋       | 27/100 [00:32<01:28,  1.21s/it]Measuring inference for batch_size=8:  28%|██▊       | 28/100 [00:34<01:27,  1.22s/it]Measuring inference for batch_size=8:  29%|██▉       | 29/100 [00:35<01:26,  1.22s/it]Measuring inference for batch_size=8:  30%|███       | 30/100 [00:36<01:25,  1.22s/it]Measuring inference for batch_size=8:  31%|███       | 31/100 [00:37<01:23,  1.22s/it]Measuring inference for batch_size=8:  32%|███▏      | 32/100 [00:38<01:22,  1.22s/it]Measuring inference for batch_size=8:  33%|███▎      | 33/100 [00:40<01:21,  1.22s/it]Measuring inference for batch_size=8:  34%|███▍      | 34/100 [00:41<01:20,  1.22s/it]Measuring inference for batch_size=8:  35%|███▌      | 35/100 [00:42<01:19,  1.22s/it]Measuring inference for batch_size=8:  36%|███▌      | 36/100 [00:43<01:17,  1.22s/it]Measuring inference for batch_size=8:  37%|███▋      | 37/100 [00:44<01:16,  1.22s/it]Measuring inference for batch_size=8:  38%|███▊      | 38/100 [00:46<01:15,  1.22s/it]Measuring inference for batch_size=8:  39%|███▉      | 39/100 [00:47<01:14,  1.22s/it]Measuring inference for batch_size=8:  40%|████      | 40/100 [00:48<01:12,  1.22s/it]Measuring inference for batch_size=8:  41%|████      | 41/100 [00:49<01:11,  1.22s/it]Measuring inference for batch_size=8:  42%|████▏     | 42/100 [00:51<01:10,  1.22s/it]Measuring inference for batch_size=8:  43%|████▎     | 43/100 [00:52<01:09,  1.22s/it]Measuring inference for batch_size=8:  44%|████▍     | 44/100 [00:53<01:08,  1.22s/it]Measuring inference for batch_size=8:  45%|████▌     | 45/100 [00:54<01:06,  1.22s/it]Measuring inference for batch_size=8:  46%|████▌     | 46/100 [00:55<01:05,  1.22s/it]Measuring inference for batch_size=8:  47%|████▋     | 47/100 [00:57<01:04,  1.22s/it]Measuring inference for batch_size=8:  48%|████▊     | 48/100 [00:58<01:03,  1.22s/it]Measuring inference for batch_size=8:  49%|████▉     | 49/100 [00:59<01:01,  1.22s/it]Measuring inference for batch_size=8:  50%|█████     | 50/100 [01:00<01:00,  1.22s/it]Measuring inference for batch_size=8:  51%|█████     | 51/100 [01:02<00:59,  1.22s/it]Measuring inference for batch_size=8:  52%|█████▏    | 52/100 [01:03<00:58,  1.22s/it]Measuring inference for batch_size=8:  53%|█████▎    | 53/100 [01:04<00:57,  1.22s/it]Measuring inference for batch_size=8:  54%|█████▍    | 54/100 [01:05<00:55,  1.22s/it]Measuring inference for batch_size=8:  55%|█████▌    | 55/100 [01:06<00:54,  1.22s/it]Measuring inference for batch_size=8:  56%|█████▌    | 56/100 [01:08<00:53,  1.22s/it]Measuring inference for batch_size=8:  57%|█████▋    | 57/100 [01:09<00:52,  1.22s/it]Measuring inference for batch_size=8:  58%|█████▊    | 58/100 [01:10<00:51,  1.22s/it]Measuring inference for batch_size=8:  59%|█████▉    | 59/100 [01:11<00:49,  1.22s/it]Measuring inference for batch_size=8:  60%|██████    | 60/100 [01:12<00:48,  1.22s/it]Measuring inference for batch_size=8:  61%|██████    | 61/100 [01:14<00:47,  1.22s/it]Measuring inference for batch_size=8:  62%|██████▏   | 62/100 [01:15<00:46,  1.21s/it]Measuring inference for batch_size=8:  63%|██████▎   | 63/100 [01:16<00:44,  1.21s/it]Measuring inference for batch_size=8:  64%|██████▍   | 64/100 [01:17<00:43,  1.21s/it]Measuring inference for batch_size=8:  65%|██████▌   | 65/100 [01:19<00:42,  1.21s/it]Measuring inference for batch_size=8:  66%|██████▌   | 66/100 [01:20<00:41,  1.21s/it]Measuring inference for batch_size=8:  67%|██████▋   | 67/100 [01:21<00:40,  1.22s/it]Measuring inference for batch_size=8:  68%|██████▊   | 68/100 [01:22<00:38,  1.22s/it]Measuring inference for batch_size=8:  69%|██████▉   | 69/100 [01:23<00:37,  1.22s/it]Measuring inference for batch_size=8:  70%|███████   | 70/100 [01:25<00:36,  1.22s/it]Measuring inference for batch_size=8:  71%|███████   | 71/100 [01:26<00:35,  1.22s/it]Measuring inference for batch_size=8:  72%|███████▏  | 72/100 [01:27<00:34,  1.22s/it]Measuring inference for batch_size=8:  73%|███████▎  | 73/100 [01:28<00:32,  1.22s/it]Measuring inference for batch_size=8:  74%|███████▍  | 74/100 [01:29<00:31,  1.22s/it]Measuring inference for batch_size=8:  75%|███████▌  | 75/100 [01:31<00:30,  1.22s/it]Measuring inference for batch_size=8:  76%|███████▌  | 76/100 [01:32<00:29,  1.22s/it]Measuring inference for batch_size=8:  77%|███████▋  | 77/100 [01:33<00:27,  1.22s/it]Measuring inference for batch_size=8:  78%|███████▊  | 78/100 [01:34<00:26,  1.22s/it]Measuring inference for batch_size=8:  79%|███████▉  | 79/100 [01:36<00:25,  1.22s/it]Measuring inference for batch_size=8:  80%|████████  | 80/100 [01:37<00:24,  1.22s/it]Measuring inference for batch_size=8:  81%|████████  | 81/100 [01:38<00:23,  1.22s/it]Measuring inference for batch_size=8:  82%|████████▏ | 82/100 [01:39<00:21,  1.22s/it]Measuring inference for batch_size=8:  83%|████████▎ | 83/100 [01:40<00:20,  1.22s/it]Measuring inference for batch_size=8:  84%|████████▍ | 84/100 [01:42<00:19,  1.22s/it]Measuring inference for batch_size=8:  85%|████████▌ | 85/100 [01:43<00:18,  1.22s/it]Measuring inference for batch_size=8:  86%|████████▌ | 86/100 [01:44<00:17,  1.22s/it]Measuring inference for batch_size=8:  87%|████████▋ | 87/100 [01:45<00:15,  1.22s/it]Measuring inference for batch_size=8:  88%|████████▊ | 88/100 [01:47<00:14,  1.22s/it]Measuring inference for batch_size=8:  89%|████████▉ | 89/100 [01:48<00:13,  1.22s/it]Measuring inference for batch_size=8:  90%|█████████ | 90/100 [01:49<00:12,  1.22s/it]Measuring inference for batch_size=8:  91%|█████████ | 91/100 [01:50<00:10,  1.22s/it]Measuring inference for batch_size=8:  92%|█████████▏| 92/100 [01:51<00:09,  1.22s/it]Measuring inference for batch_size=8:  93%|█████████▎| 93/100 [01:53<00:08,  1.22s/it]Measuring inference for batch_size=8:  94%|█████████▍| 94/100 [01:54<00:07,  1.22s/it]Measuring inference for batch_size=8:  95%|█████████▌| 95/100 [01:55<00:06,  1.22s/it]Measuring inference for batch_size=8:  96%|█████████▌| 96/100 [01:56<00:04,  1.22s/it]Measuring inference for batch_size=8:  97%|█████████▋| 97/100 [01:57<00:03,  1.22s/it]Measuring inference for batch_size=8:  98%|█████████▊| 98/100 [01:59<00:02,  1.22s/it]Measuring inference for batch_size=8:  99%|█████████▉| 99/100 [02:00<00:01,  1.22s/it]Measuring inference for batch_size=8: 100%|██████████| 100/100 [02:01<00:00,  1.22s/it]Measuring inference for batch_size=8: 100%|██████████| 100/100 [02:01<00:00,  1.22s/it]
Measuring energy for batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Measuring energy for batch_size=8:  10%|█         | 1/10 [00:01<00:17,  1.98s/it]Measuring energy for batch_size=8:  20%|██        | 2/10 [00:03<00:12,  1.54s/it]Measuring energy for batch_size=8:  30%|███       | 3/10 [00:04<00:09,  1.40s/it]Measuring energy for batch_size=8:  40%|████      | 4/10 [00:05<00:07,  1.33s/it]Measuring energy for batch_size=8:  50%|█████     | 5/10 [00:06<00:06,  1.29s/it]Measuring energy for batch_size=8:  60%|██████    | 6/10 [00:08<00:05,  1.27s/it]Measuring energy for batch_size=8:  70%|███████   | 7/10 [00:09<00:03,  1.26s/it]Measuring energy for batch_size=8:  80%|████████  | 8/10 [00:10<00:02,  1.25s/it]Measuring energy for batch_size=8:  90%|█████████ | 9/10 [00:11<00:01,  1.24s/it]Measuring energy for batch_size=8: 100%|██████████| 10/10 [00:13<00:00,  1.24s/it]Measuring energy for batch_size=8: 100%|██████████| 10/10 [00:13<00:00,  1.31s/it]
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
learner.model.forward:
  device: cuda
  energy:
    batch_size_1:
      joules: 5.4961981007957466
      kWh: 1.5267216946654853e-06
    batch_size_8:
      joules: 39.662070133067765
      kWh: 1.1017241703629935e-05
  flops: 4970008352
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 13.45 GB
      total: 31.17 GB
      used: 17.28 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  params: 3794322
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: 1.634 ms +/- 216.575 us [1.427 ms, 2.232 ms]
          batches_per_second: 621.05 +/- 69.47 [447.97, 700.57]
        metrics:
          batches_per_second_max: 700.5685652246534
          batches_per_second_mean: 621.0491268757461
          batches_per_second_min: 447.96582292000426
          batches_per_second_std: 69.47469889053731
          seconds_per_batch_max: 0.0022323131561279297
          seconds_per_batch_mean: 0.0016342854499816894
          seconds_per_batch_min: 0.0014274120330810547
          seconds_per_batch_std: 0.00021657522918217402
      gpu_to_cpu:
        human_readable:
          batch_latency: 8.156 ms +/- 632.906 us [6.579 ms, 9.761 ms]
          batches_per_second: 123.37 +/- 9.93 [102.45, 152.00]
        metrics:
          batches_per_second_max: 151.99507157093677
          batches_per_second_mean: 123.3741183796301
          batches_per_second_min: 102.45252693031095
          batches_per_second_std: 9.929815725954304
          seconds_per_batch_max: 0.009760618209838867
          seconds_per_batch_mean: 0.008156070709228516
          seconds_per_batch_min: 0.006579160690307617
          seconds_per_batch_std: 0.0006329060561114586
      on_device_inference:
        human_readable:
          batch_latency: 202.375 ms +/- 676.796 us [200.408 ms, 204.228 ms]
          batches_per_second: 4.94 +/- 0.02 [4.90, 4.99]
        metrics:
          batches_per_second_max: 4.989821216980818
          batches_per_second_mean: 4.9413884084244835
          batches_per_second_min: 4.896490052463594
          batches_per_second_std: 0.016501015202298644
          seconds_per_batch_max: 0.20422792434692383
          seconds_per_batch_mean: 0.20237453222274782
          seconds_per_batch_min: 0.2004079818725586
          seconds_per_batch_std: 0.0006767964555348838
      total:
        human_readable:
          batch_latency: 212.165 ms +/- 434.835 us [211.260 ms, 214.591 ms]
          batches_per_second: 4.71 +/- 0.01 [4.66, 4.73]
        metrics:
          batches_per_second_max: 4.733496597411098
          batches_per_second_mean: 4.713334965622937
          batches_per_second_min: 4.660021931846842
          batches_per_second_std: 0.00962882208274921
          seconds_per_batch_max: 0.21459126472473145
          seconds_per_batch_mean: 0.21216488838195802
          seconds_per_batch_min: 0.21126031875610352
          seconds_per_batch_std: 0.0004348354662602866
    batch_size_8:
      cpu_to_gpu:
        human_readable:
          batch_latency: 13.135 ms +/- 264.347 us [12.575 ms, 13.809 ms]
          batches_per_second: 76.16 +/- 1.53 [72.42, 79.52]
        metrics:
          batches_per_second_max: 79.52342490946666
          batches_per_second_mean: 76.16274247611533
          batches_per_second_min: 72.41671990193201
          batches_per_second_std: 1.5250803471163052
          seconds_per_batch_max: 0.013808965682983398
          seconds_per_batch_mean: 0.01313507080078125
          seconds_per_batch_min: 0.012574911117553711
          seconds_per_batch_std: 0.00026434717233116264
      gpu_to_cpu:
        human_readable:
          batch_latency: 147.352 ms +/- 2.790 ms [145.476 ms, 159.596 ms]
          batches_per_second: 6.79 +/- 0.12 [6.27, 6.87]
        metrics:
          batches_per_second_max: 6.873992493895144
          batches_per_second_mean: 6.788743729932688
          batches_per_second_min: 6.265813159644156
          batches_per_second_std: 0.12114598612253721
          seconds_per_batch_max: 0.15959620475769043
          seconds_per_batch_mean: 0.1473524284362793
          seconds_per_batch_min: 0.1454758644104004
          seconds_per_batch_std: 0.0027896652322541384
      on_device_inference:
        human_readable:
          batch_latency: 1.054 s +/- 3.509 ms [1.040 s, 1.061 s]
          batches_per_second: 0.95 +/- 0.00 [0.94, 0.96]
        metrics:
          batches_per_second_max: 0.9617217101997251
          batches_per_second_mean: 0.9485051246681776
          batches_per_second_min: 0.9421280172472853
          batches_per_second_std: 0.003180758854501985
          seconds_per_batch_max: 1.0614268779754639
          seconds_per_batch_mean: 1.054302327632904
          seconds_per_batch_min: 1.039801836013794
          seconds_per_batch_std: 0.0035088857554806023
      total:
        human_readable:
          batch_latency: 1.215 s +/- 1.851 ms [1.211 s, 1.223 s]
          batches_per_second: 0.82 +/- 0.00 [0.82, 0.83]
        metrics:
          batches_per_second_max: 0.8255332264849812
          batches_per_second_mean: 0.8231895724124008
          batches_per_second_min: 0.8179663406089488
          batches_per_second_std: 0.0012524661094444427
          seconds_per_batch_max: 1.2225441932678223
          seconds_per_batch_mean: 1.2147898268699646
          seconds_per_batch_min: 1.2113382816314697
          seconds_per_batch_std: 0.0018508776932981157

==== Benchmarking X3DLearner (l) ====
== Benchmarking learner.infer ==
Warming up with batch_size=2:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=2:  10%|█         | 1/10 [00:00<00:07,  1.26it/s]Warming up with batch_size=2:  20%|██        | 2/10 [00:01<00:06,  1.26it/s]Warming up with batch_size=2:  30%|███       | 3/10 [00:02<00:05,  1.26it/s]Warming up with batch_size=2:  40%|████      | 4/10 [00:03<00:04,  1.26it/s]Warming up with batch_size=2:  50%|█████     | 5/10 [00:03<00:03,  1.26it/s]Warming up with batch_size=2:  60%|██████    | 6/10 [00:04<00:03,  1.26it/s]Warming up with batch_size=2:  70%|███████   | 7/10 [00:05<00:02,  1.26it/s]Warming up with batch_size=2:  80%|████████  | 8/10 [00:06<00:01,  1.26it/s]Warming up with batch_size=2:  90%|█████████ | 9/10 [00:07<00:00,  1.26it/s]Warming up with batch_size=2: 100%|██████████| 10/10 [00:07<00:00,  1.26it/s]Warming up with batch_size=2: 100%|██████████| 10/10 [00:07<00:00,  1.26it/s]
Measuring inference for batch_size=2:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference for batch_size=2:   1%|          | 1/100 [00:00<01:18,  1.27it/s]Measuring inference for batch_size=2:   2%|▏         | 2/100 [00:01<01:17,  1.26it/s]Measuring inference for batch_size=2:   3%|▎         | 3/100 [00:02<01:17,  1.26it/s]Measuring inference for batch_size=2:   4%|▍         | 4/100 [00:03<01:16,  1.26it/s]Measuring inference for batch_size=2:   5%|▌         | 5/100 [00:03<01:15,  1.26it/s]Measuring inference for batch_size=2:   6%|▌         | 6/100 [00:04<01:14,  1.26it/s]Measuring inference for batch_size=2:   7%|▋         | 7/100 [00:05<01:13,  1.26it/s]Measuring inference for batch_size=2:   8%|▊         | 8/100 [00:06<01:12,  1.26it/s]Measuring inference for batch_size=2:   9%|▉         | 9/100 [00:07<01:12,  1.26it/s]Measuring inference for batch_size=2:  10%|█         | 10/100 [00:07<01:11,  1.26it/s]Measuring inference for batch_size=2:  11%|█         | 11/100 [00:08<01:10,  1.26it/s]Measuring inference for batch_size=2:  12%|█▏        | 12/100 [00:09<01:09,  1.26it/s]Measuring inference for batch_size=2:  13%|█▎        | 13/100 [00:10<01:08,  1.26it/s]Measuring inference for batch_size=2:  14%|█▍        | 14/100 [00:11<01:08,  1.26it/s]Measuring inference for batch_size=2:  15%|█▌        | 15/100 [00:11<01:07,  1.26it/s]Measuring inference for batch_size=2:  16%|█▌        | 16/100 [00:12<01:06,  1.26it/s]Measuring inference for batch_size=2:  17%|█▋        | 17/100 [00:13<01:05,  1.26it/s]Measuring inference for batch_size=2:  18%|█▊        | 18/100 [00:14<01:04,  1.26it/s]Measuring inference for batch_size=2:  19%|█▉        | 19/100 [00:15<01:04,  1.26it/s]Measuring inference for batch_size=2:  20%|██        | 20/100 [00:15<01:03,  1.26it/s]Measuring inference for batch_size=2:  21%|██        | 21/100 [00:16<01:02,  1.26it/s]Measuring inference for batch_size=2:  22%|██▏       | 22/100 [00:17<01:01,  1.26it/s]Measuring inference for batch_size=2:  23%|██▎       | 23/100 [00:18<01:01,  1.26it/s]Measuring inference for batch_size=2:  24%|██▍       | 24/100 [00:19<01:00,  1.26it/s]Measuring inference for batch_size=2:  25%|██▌       | 25/100 [00:19<00:59,  1.26it/s]Measuring inference for batch_size=2:  26%|██▌       | 26/100 [00:20<00:58,  1.26it/s]Measuring inference for batch_size=2:  27%|██▋       | 27/100 [00:21<00:57,  1.26it/s]Measuring inference for batch_size=2:  28%|██▊       | 28/100 [00:22<00:57,  1.26it/s]Measuring inference for batch_size=2:  29%|██▉       | 29/100 [00:23<00:56,  1.26it/s]Measuring inference for batch_size=2:  30%|███       | 30/100 [00:23<00:55,  1.26it/s]Measuring inference for batch_size=2:  31%|███       | 31/100 [00:24<00:54,  1.26it/s]Measuring inference for batch_size=2:  32%|███▏      | 32/100 [00:25<00:53,  1.26it/s]Measuring inference for batch_size=2:  33%|███▎      | 33/100 [00:26<00:53,  1.26it/s]Measuring inference for batch_size=2:  34%|███▍      | 34/100 [00:26<00:52,  1.26it/s]Measuring inference for batch_size=2:  35%|███▌      | 35/100 [00:27<00:51,  1.26it/s]Measuring inference for batch_size=2:  36%|███▌      | 36/100 [00:28<00:50,  1.26it/s]Measuring inference for batch_size=2:  37%|███▋      | 37/100 [00:29<00:50,  1.26it/s]Measuring inference for batch_size=2:  38%|███▊      | 38/100 [00:30<00:49,  1.26it/s]Measuring inference for batch_size=2:  39%|███▉      | 39/100 [00:30<00:48,  1.26it/s]Measuring inference for batch_size=2:  40%|████      | 40/100 [00:31<00:47,  1.26it/s]Measuring inference for batch_size=2:  41%|████      | 41/100 [00:32<00:46,  1.26it/s]Measuring inference for batch_size=2:  42%|████▏     | 42/100 [00:33<00:46,  1.26it/s]Measuring inference for batch_size=2:  43%|████▎     | 43/100 [00:34<00:45,  1.26it/s]Measuring inference for batch_size=2:  44%|████▍     | 44/100 [00:34<00:44,  1.26it/s]Measuring inference for batch_size=2:  45%|████▌     | 45/100 [00:35<00:43,  1.26it/s]Measuring inference for batch_size=2:  46%|████▌     | 46/100 [00:36<00:42,  1.26it/s]Measuring inference for batch_size=2:  47%|████▋     | 47/100 [00:37<00:42,  1.26it/s]Measuring inference for batch_size=2:  48%|████▊     | 48/100 [00:38<00:41,  1.26it/s]Measuring inference for batch_size=2:  49%|████▉     | 49/100 [00:38<00:40,  1.26it/s]Measuring inference for batch_size=2:  50%|█████     | 50/100 [00:39<00:39,  1.26it/s]Measuring inference for batch_size=2:  51%|█████     | 51/100 [00:40<00:38,  1.26it/s]Measuring inference for batch_size=2:  52%|█████▏    | 52/100 [00:41<00:38,  1.26it/s]Measuring inference for batch_size=2:  53%|█████▎    | 53/100 [00:42<00:37,  1.26it/s]Measuring inference for batch_size=2:  54%|█████▍    | 54/100 [00:42<00:36,  1.26it/s]Measuring inference for batch_size=2:  55%|█████▌    | 55/100 [00:43<00:35,  1.26it/s]Measuring inference for batch_size=2:  56%|█████▌    | 56/100 [00:44<00:34,  1.26it/s]Measuring inference for batch_size=2:  57%|█████▋    | 57/100 [00:45<00:34,  1.26it/s]Measuring inference for batch_size=2:  58%|█████▊    | 58/100 [00:46<00:33,  1.26it/s]Measuring inference for batch_size=2:  59%|█████▉    | 59/100 [00:46<00:32,  1.26it/s]Measuring inference for batch_size=2:  60%|██████    | 60/100 [00:47<00:31,  1.26it/s]Measuring inference for batch_size=2:  61%|██████    | 61/100 [00:48<00:30,  1.26it/s]Measuring inference for batch_size=2:  62%|██████▏   | 62/100 [00:49<00:30,  1.26it/s]Measuring inference for batch_size=2:  63%|██████▎   | 63/100 [00:49<00:29,  1.26it/s]Measuring inference for batch_size=2:  64%|██████▍   | 64/100 [00:50<00:28,  1.26it/s]Measuring inference for batch_size=2:  65%|██████▌   | 65/100 [00:51<00:27,  1.26it/s]Measuring inference for batch_size=2:  66%|██████▌   | 66/100 [00:52<00:26,  1.26it/s]Measuring inference for batch_size=2:  67%|██████▋   | 67/100 [00:53<00:26,  1.26it/s]Measuring inference for batch_size=2:  68%|██████▊   | 68/100 [00:53<00:25,  1.26it/s]Measuring inference for batch_size=2:  69%|██████▉   | 69/100 [00:54<00:24,  1.26it/s]Measuring inference for batch_size=2:  70%|███████   | 70/100 [00:55<00:23,  1.26it/s]Measuring inference for batch_size=2:  71%|███████   | 71/100 [00:56<00:22,  1.26it/s]Measuring inference for batch_size=2:  72%|███████▏  | 72/100 [00:57<00:22,  1.26it/s]Measuring inference for batch_size=2:  73%|███████▎  | 73/100 [00:57<00:21,  1.26it/s]Measuring inference for batch_size=2:  74%|███████▍  | 74/100 [00:58<00:20,  1.26it/s]Measuring inference for batch_size=2:  75%|███████▌  | 75/100 [00:59<00:19,  1.26it/s]Measuring inference for batch_size=2:  76%|███████▌  | 76/100 [01:00<00:19,  1.26it/s]Measuring inference for batch_size=2:  77%|███████▋  | 77/100 [01:01<00:18,  1.26it/s]Measuring inference for batch_size=2:  78%|███████▊  | 78/100 [01:01<00:17,  1.26it/s]Measuring inference for batch_size=2:  79%|███████▉  | 79/100 [01:02<00:16,  1.26it/s]Measuring inference for batch_size=2:  80%|████████  | 80/100 [01:03<00:15,  1.26it/s]Measuring inference for batch_size=2:  81%|████████  | 81/100 [01:04<00:15,  1.26it/s]Measuring inference for batch_size=2:  82%|████████▏ | 82/100 [01:05<00:14,  1.26it/s]Measuring inference for batch_size=2:  83%|████████▎ | 83/100 [01:05<00:13,  1.26it/s]Measuring inference for batch_size=2:  84%|████████▍ | 84/100 [01:06<00:12,  1.26it/s]Measuring inference for batch_size=2:  85%|████████▌ | 85/100 [01:07<00:11,  1.26it/s]Measuring inference for batch_size=2:  86%|████████▌ | 86/100 [01:08<00:11,  1.26it/s]Measuring inference for batch_size=2:  87%|████████▋ | 87/100 [01:09<00:10,  1.26it/s]Measuring inference for batch_size=2:  88%|████████▊ | 88/100 [01:09<00:09,  1.26it/s]Measuring inference for batch_size=2:  89%|████████▉ | 89/100 [01:10<00:08,  1.26it/s]Measuring inference for batch_size=2:  90%|█████████ | 90/100 [01:11<00:07,  1.26it/s]Measuring inference for batch_size=2:  91%|█████████ | 91/100 [01:12<00:07,  1.26it/s]Measuring inference for batch_size=2:  92%|█████████▏| 92/100 [01:12<00:06,  1.26it/s]Measuring inference for batch_size=2:  93%|█████████▎| 93/100 [01:13<00:05,  1.26it/s]Measuring inference for batch_size=2:  94%|█████████▍| 94/100 [01:14<00:04,  1.26it/s]Measuring inference for batch_size=2:  95%|█████████▌| 95/100 [01:15<00:03,  1.26it/s]Measuring inference for batch_size=2:  96%|█████████▌| 96/100 [01:16<00:03,  1.26it/s]Measuring inference for batch_size=2:  97%|█████████▋| 97/100 [01:16<00:02,  1.26it/s]Measuring inference for batch_size=2:  98%|█████████▊| 98/100 [01:17<00:01,  1.26it/s]Measuring inference for batch_size=2:  99%|█████████▉| 99/100 [01:18<00:00,  1.26it/s]Measuring inference for batch_size=2: 100%|██████████| 100/100 [01:19<00:00,  1.26it/s]Measuring inference for batch_size=2: 100%|██████████| 100/100 [01:19<00:00,  1.26it/s]
Measuring energy for batch_size=1:   0%|          | 0/10 [00:00<?, ?it/s]Measuring energy for batch_size=1:  10%|█         | 1/10 [00:01<00:12,  1.38s/it]Measuring energy for batch_size=1:  20%|██        | 2/10 [00:02<00:08,  1.07s/it]Measuring energy for batch_size=1:  30%|███       | 3/10 [00:03<00:06,  1.04it/s]Measuring energy for batch_size=1:  40%|████      | 4/10 [00:03<00:05,  1.10it/s]Measuring energy for batch_size=1:  50%|█████     | 5/10 [00:04<00:04,  1.12it/s]Measuring energy for batch_size=1:  60%|██████    | 6/10 [00:05<00:03,  1.14it/s]Measuring energy for batch_size=1:  70%|███████   | 7/10 [00:06<00:02,  1.15it/s]Measuring energy for batch_size=1:  80%|████████  | 8/10 [00:07<00:01,  1.16it/s]Measuring energy for batch_size=1:  90%|█████████ | 9/10 [00:08<00:00,  1.17it/s]Measuring energy for batch_size=1: 100%|██████████| 10/10 [00:08<00:00,  1.17it/s]Measuring energy for batch_size=1: 100%|██████████| 10/10 [00:08<00:00,  1.11it/s]
Warming up with batch_size=2:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=2:  10%|█         | 1/10 [00:01<00:10,  1.15s/it]Warming up with batch_size=2:  20%|██        | 2/10 [00:02<00:09,  1.15s/it]Warming up with batch_size=2:  30%|███       | 3/10 [00:03<00:08,  1.15s/it]Warming up with batch_size=2:  40%|████      | 4/10 [00:04<00:06,  1.15s/it]Warming up with batch_size=2:  50%|█████     | 5/10 [00:05<00:05,  1.15s/it]Warming up with batch_size=2:  60%|██████    | 6/10 [00:06<00:04,  1.15s/it]Warming up with batch_size=2:  70%|███████   | 7/10 [00:08<00:03,  1.15s/it]Warming up with batch_size=2:  80%|████████  | 8/10 [00:09<00:02,  1.16s/it]Warming up with batch_size=2:  90%|█████████ | 9/10 [00:10<00:01,  1.15s/it]Warming up with batch_size=2: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it]Warming up with batch_size=2: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it]
Measuring inference for batch_size=2:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference for batch_size=2:   1%|          | 1/100 [00:01<01:54,  1.15s/it]Measuring inference for batch_size=2:   2%|▏         | 2/100 [00:02<01:53,  1.15s/it]Measuring inference for batch_size=2:   3%|▎         | 3/100 [00:03<01:52,  1.15s/it]Measuring inference for batch_size=2:   4%|▍         | 4/100 [00:04<01:50,  1.15s/it]Measuring inference for batch_size=2:   5%|▌         | 5/100 [00:05<01:49,  1.15s/it]Measuring inference for batch_size=2:   6%|▌         | 6/100 [00:06<01:48,  1.15s/it]Measuring inference for batch_size=2:   7%|▋         | 7/100 [00:08<01:47,  1.15s/it]Measuring inference for batch_size=2:   8%|▊         | 8/100 [00:09<01:46,  1.15s/it]Measuring inference for batch_size=2:   9%|▉         | 9/100 [00:10<01:45,  1.15s/it]Measuring inference for batch_size=2:  10%|█         | 10/100 [00:11<01:43,  1.15s/it]Measuring inference for batch_size=2:  11%|█         | 11/100 [00:12<01:42,  1.15s/it]Measuring inference for batch_size=2:  12%|█▏        | 12/100 [00:13<01:41,  1.15s/it]Measuring inference for batch_size=2:  13%|█▎        | 13/100 [00:14<01:40,  1.15s/it]Measuring inference for batch_size=2:  14%|█▍        | 14/100 [00:16<01:39,  1.15s/it]Measuring inference for batch_size=2:  15%|█▌        | 15/100 [00:17<01:37,  1.15s/it]Measuring inference for batch_size=2:  16%|█▌        | 16/100 [00:18<01:36,  1.15s/it]Measuring inference for batch_size=2:  17%|█▋        | 17/100 [00:19<01:35,  1.15s/it]Measuring inference for batch_size=2:  18%|█▊        | 18/100 [00:20<01:34,  1.15s/it]Measuring inference for batch_size=2:  19%|█▉        | 19/100 [00:21<01:33,  1.15s/it]Measuring inference for batch_size=2:  20%|██        | 20/100 [00:23<01:32,  1.15s/it]Measuring inference for batch_size=2:  21%|██        | 21/100 [00:24<01:31,  1.15s/it]Measuring inference for batch_size=2:  22%|██▏       | 22/100 [00:25<01:29,  1.15s/it]Measuring inference for batch_size=2:  23%|██▎       | 23/100 [00:26<01:28,  1.15s/it]Measuring inference for batch_size=2:  24%|██▍       | 24/100 [00:27<01:27,  1.15s/it]Measuring inference for batch_size=2:  25%|██▌       | 25/100 [00:28<01:26,  1.15s/it]Measuring inference for batch_size=2:  26%|██▌       | 26/100 [00:29<01:25,  1.15s/it]Measuring inference for batch_size=2:  27%|██▋       | 27/100 [00:31<01:24,  1.15s/it]Measuring inference for batch_size=2:  28%|██▊       | 28/100 [00:32<01:23,  1.15s/it]Measuring inference for batch_size=2:  29%|██▉       | 29/100 [00:33<01:21,  1.15s/it]Measuring inference for batch_size=2:  30%|███       | 30/100 [00:34<01:20,  1.15s/it]Measuring inference for batch_size=2:  31%|███       | 31/100 [00:35<01:19,  1.15s/it]Measuring inference for batch_size=2:  32%|███▏      | 32/100 [00:36<01:18,  1.15s/it]Measuring inference for batch_size=2:  33%|███▎      | 33/100 [00:38<01:17,  1.15s/it]Measuring inference for batch_size=2:  34%|███▍      | 34/100 [00:39<01:16,  1.15s/it]Measuring inference for batch_size=2:  35%|███▌      | 35/100 [00:40<01:15,  1.15s/it]Measuring inference for batch_size=2:  36%|███▌      | 36/100 [00:41<01:13,  1.15s/it]Measuring inference for batch_size=2:  37%|███▋      | 37/100 [00:42<01:12,  1.15s/it]Measuring inference for batch_size=2:  38%|███▊      | 38/100 [00:43<01:11,  1.15s/it]Measuring inference for batch_size=2:  39%|███▉      | 39/100 [00:44<01:10,  1.15s/it]Measuring inference for batch_size=2:  40%|████      | 40/100 [00:46<01:09,  1.15s/it]Measuring inference for batch_size=2:  41%|████      | 41/100 [00:47<01:07,  1.15s/it]Measuring inference for batch_size=2:  42%|████▏     | 42/100 [00:48<01:06,  1.15s/it]Measuring inference for batch_size=2:  43%|████▎     | 43/100 [00:49<01:05,  1.15s/it]Measuring inference for batch_size=2:  44%|████▍     | 44/100 [00:50<01:04,  1.15s/it]Measuring inference for batch_size=2:  45%|████▌     | 45/100 [00:51<01:03,  1.15s/it]Measuring inference for batch_size=2:  46%|████▌     | 46/100 [00:53<01:02,  1.15s/it]Measuring inference for batch_size=2:  47%|████▋     | 47/100 [00:54<01:01,  1.15s/it]Measuring inference for batch_size=2:  48%|████▊     | 48/100 [00:55<00:59,  1.15s/it]Measuring inference for batch_size=2:  49%|████▉     | 49/100 [00:56<00:58,  1.15s/it]Measuring inference for batch_size=2:  50%|█████     | 50/100 [00:57<00:57,  1.15s/it]Measuring inference for batch_size=2:  51%|█████     | 51/100 [00:58<00:56,  1.15s/it]Measuring inference for batch_size=2:  52%|█████▏    | 52/100 [00:59<00:55,  1.15s/it]Measuring inference for batch_size=2:  53%|█████▎    | 53/100 [01:01<00:54,  1.15s/it]Measuring inference for batch_size=2:  54%|█████▍    | 54/100 [01:02<00:53,  1.15s/it]Measuring inference for batch_size=2:  55%|█████▌    | 55/100 [01:03<00:51,  1.15s/it]Measuring inference for batch_size=2:  56%|█████▌    | 56/100 [01:04<00:50,  1.15s/it]Measuring inference for batch_size=2:  57%|█████▋    | 57/100 [01:05<00:49,  1.16s/it]Measuring inference for batch_size=2:  58%|█████▊    | 58/100 [01:06<00:48,  1.15s/it]Measuring inference for batch_size=2:  59%|█████▉    | 59/100 [01:08<00:47,  1.15s/it]Measuring inference for batch_size=2:  60%|██████    | 60/100 [01:09<00:46,  1.15s/it]Measuring inference for batch_size=2:  61%|██████    | 61/100 [01:10<00:44,  1.15s/it]Measuring inference for batch_size=2:  62%|██████▏   | 62/100 [01:11<00:43,  1.15s/it]Measuring inference for batch_size=2:  63%|██████▎   | 63/100 [01:12<00:42,  1.15s/it]Measuring inference for batch_size=2:  64%|██████▍   | 64/100 [01:13<00:41,  1.15s/it]Measuring inference for batch_size=2:  65%|██████▌   | 65/100 [01:14<00:40,  1.15s/it]Measuring inference for batch_size=2:  66%|██████▌   | 66/100 [01:16<00:39,  1.15s/it]Measuring inference for batch_size=2:  67%|██████▋   | 67/100 [01:17<00:38,  1.16s/it]Measuring inference for batch_size=2:  68%|██████▊   | 68/100 [01:18<00:36,  1.16s/it]Measuring inference for batch_size=2:  69%|██████▉   | 69/100 [01:19<00:35,  1.16s/it]Measuring inference for batch_size=2:  70%|███████   | 70/100 [01:20<00:34,  1.15s/it]Measuring inference for batch_size=2:  71%|███████   | 71/100 [01:21<00:33,  1.15s/it]Measuring inference for batch_size=2:  72%|███████▏  | 72/100 [01:23<00:32,  1.16s/it]Measuring inference for batch_size=2:  73%|███████▎  | 73/100 [01:24<00:31,  1.16s/it]Measuring inference for batch_size=2:  74%|███████▍  | 74/100 [01:25<00:30,  1.15s/it]Measuring inference for batch_size=2:  75%|███████▌  | 75/100 [01:26<00:28,  1.15s/it]Measuring inference for batch_size=2:  76%|███████▌  | 76/100 [01:27<00:27,  1.16s/it]Measuring inference for batch_size=2:  77%|███████▋  | 77/100 [01:28<00:26,  1.15s/it]Measuring inference for batch_size=2:  78%|███████▊  | 78/100 [01:29<00:25,  1.15s/it]Measuring inference for batch_size=2:  79%|███████▉  | 79/100 [01:31<00:24,  1.15s/it]Measuring inference for batch_size=2:  80%|████████  | 80/100 [01:32<00:23,  1.15s/it]Measuring inference for batch_size=2:  81%|████████  | 81/100 [01:33<00:21,  1.15s/it]Measuring inference for batch_size=2:  82%|████████▏ | 82/100 [01:34<00:20,  1.15s/it]Measuring inference for batch_size=2:  83%|████████▎ | 83/100 [01:35<00:19,  1.15s/it]Measuring inference for batch_size=2:  84%|████████▍ | 84/100 [01:36<00:18,  1.15s/it]Measuring inference for batch_size=2:  85%|████████▌ | 85/100 [01:38<00:17,  1.15s/it]Measuring inference for batch_size=2:  86%|████████▌ | 86/100 [01:39<00:16,  1.15s/it]Measuring inference for batch_size=2:  87%|████████▋ | 87/100 [01:40<00:15,  1.15s/it]Measuring inference for batch_size=2:  88%|████████▊ | 88/100 [01:41<00:13,  1.15s/it]Measuring inference for batch_size=2:  89%|████████▉ | 89/100 [01:42<00:12,  1.15s/it]Measuring inference for batch_size=2:  90%|█████████ | 90/100 [01:43<00:11,  1.15s/it]Measuring inference for batch_size=2:  91%|█████████ | 91/100 [01:44<00:10,  1.15s/it]Measuring inference for batch_size=2:  92%|█████████▏| 92/100 [01:46<00:09,  1.15s/it]Measuring inference for batch_size=2:  93%|█████████▎| 93/100 [01:47<00:08,  1.15s/it]Measuring inference for batch_size=2:  94%|█████████▍| 94/100 [01:48<00:06,  1.15s/it]Measuring inference for batch_size=2:  95%|█████████▌| 95/100 [01:49<00:05,  1.15s/it]Measuring inference for batch_size=2:  96%|█████████▌| 96/100 [01:50<00:04,  1.16s/it]Measuring inference for batch_size=2:  97%|█████████▋| 97/100 [01:51<00:03,  1.16s/it]Measuring inference for batch_size=2:  98%|█████████▊| 98/100 [01:53<00:02,  1.15s/it]Measuring inference for batch_size=2:  99%|█████████▉| 99/100 [01:54<00:01,  1.16s/it]Measuring inference for batch_size=2: 100%|██████████| 100/100 [01:55<00:00,  1.16s/it]Measuring inference for batch_size=2: 100%|██████████| 100/100 [01:55<00:00,  1.15s/it]
Measuring energy for batch_size=2:   0%|          | 0/10 [00:00<?, ?it/s]Measuring energy for batch_size=2:  10%|█         | 1/10 [00:01<00:10,  1.20s/it]Measuring energy for batch_size=2:  20%|██        | 2/10 [00:02<00:09,  1.20s/it]Measuring energy for batch_size=2:  30%|███       | 3/10 [00:03<00:08,  1.20s/it]Measuring energy for batch_size=2:  40%|████      | 4/10 [00:04<00:07,  1.20s/it]Measuring energy for batch_size=2:  50%|█████     | 5/10 [00:05<00:05,  1.20s/it]Measuring energy for batch_size=2:  60%|██████    | 6/10 [00:07<00:04,  1.20s/it]Measuring energy for batch_size=2:  70%|███████   | 7/10 [00:08<00:03,  1.20s/it]Measuring energy for batch_size=2:  80%|████████  | 8/10 [00:09<00:02,  1.20s/it]Measuring energy for batch_size=2:  90%|█████████ | 9/10 [00:10<00:01,  1.20s/it]Measuring energy for batch_size=2: 100%|██████████| 10/10 [00:11<00:00,  1.20s/it]Measuring energy for batch_size=2: 100%|██████████| 10/10 [00:11<00:00,  1.20s/it]
learner.infer:
  device: cuda
  energy:
    batch_size_1:
      joules: 23.53651551680088
      kWh: 6.5379209768891334e-06
    batch_size_2:
      joules: 36.869737346838306
      kWh: 1.0241593707455085e-05
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 13.61 GB
      total: 31.17 GB
      used: 17.13 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: 10.371 us +/- 3.843 us [5.960 us, 38.147 us]
          batches_per_second: 105.07 K +/- 27.76 K [26.21 K, 167.77 K]
        metrics:
          batches_per_second_max: 167772.16
          batches_per_second_mean: 105069.74063894352
          batches_per_second_min: 26214.4
          batches_per_second_std: 27756.456053200465
          seconds_per_batch_max: 3.814697265625e-05
          seconds_per_batch_mean: 1.0371208190917969e-05
          seconds_per_batch_min: 5.9604644775390625e-06
          seconds_per_batch_std: 3.843274956463688e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: 388.377 us +/- 104.901 us [215.054 us, 780.582 us]
          batches_per_second: 2.75 K +/- 671.15 [1.28 K, 4.65 K]
        metrics:
          batches_per_second_max: 4650.0044345898
          batches_per_second_mean: 2745.123540817529
          batches_per_second_min: 1281.0946854001222
          batches_per_second_std: 671.1549910235401
          seconds_per_batch_max: 0.0007805824279785156
          seconds_per_batch_mean: 0.00038837671279907227
          seconds_per_batch_min: 0.00021505355834960938
          seconds_per_batch_std: 0.00010490096398880163
      on_device_inference:
        human_readable:
          batch_latency: 791.791 ms +/- 1.868 ms [787.754 ms, 797.936 ms]
          batches_per_second: 1.26 +/- 0.00 [1.25, 1.27]
        metrics:
          batches_per_second_max: 1.26943211708921
          batches_per_second_mean: 1.262965860467009
          batches_per_second_min: 1.2532326517245789
          batches_per_second_std: 0.0029761187205545603
          seconds_per_batch_max: 0.7979364395141602
          seconds_per_batch_mean: 0.7917914414405822
          seconds_per_batch_min: 0.7877538204193115
          seconds_per_batch_std: 0.0018679320105350345
      total:
        human_readable:
          batch_latency: 792.190 ms +/- 1.877 ms [788.152 ms, 798.236 ms]
          batches_per_second: 1.26 +/- 0.00 [1.25, 1.27]
        metrics:
          batches_per_second_max: 1.268790824711737
          batches_per_second_mean: 1.2623302068355473
          batches_per_second_min: 1.2527628826750432
          batches_per_second_std: 0.0029871656020896806
          seconds_per_batch_max: 0.7982356548309326
          seconds_per_batch_mean: 0.7921901893615723
          seconds_per_batch_min: 0.7881519794464111
          seconds_per_batch_std: 0.0018765706844109731
    batch_size_2:
      cpu_to_gpu:
        human_readable:
          batch_latency: 9.844 us +/- 1.811 us [5.960 us, 16.928 us]
          batches_per_second: 105.34 K +/- 21.05 K [59.07 K, 167.77 K]
        metrics:
          batches_per_second_max: 167772.16
          batches_per_second_mean: 105341.62126569555
          batches_per_second_min: 59074.704225352114
          batches_per_second_std: 21046.68280192839
          seconds_per_batch_max: 1.6927719116210938e-05
          seconds_per_batch_mean: 9.844303131103515e-06
          seconds_per_batch_min: 5.9604644775390625e-06
          seconds_per_batch_std: 1.8108185375853335e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: 466.368 us +/- 134.863 us [286.579 us, 866.175 us]
          batches_per_second: 2.30 K +/- 571.90 [1.15 K, 3.49 K]
        metrics:
          batches_per_second_max: 3489.4376039933445
          batches_per_second_mean: 2299.4967330801887
          batches_per_second_min: 1154.5015139003578
          batches_per_second_std: 571.9001619564574
          seconds_per_batch_max: 0.0008661746978759766
          seconds_per_batch_mean: 0.00046636819839477537
          seconds_per_batch_min: 0.0002865791320800781
          seconds_per_batch_std: 0.00013486314246664605
      on_device_inference:
        human_readable:
          batch_latency: 1.152 s +/- 2.347 ms [1.148 s, 1.158 s]
          batches_per_second: 0.87 +/- 0.00 [0.86, 0.87]
        metrics:
          batches_per_second_max: 0.8713597295615098
          batches_per_second_mean: 0.8678678818475973
          batches_per_second_min: 0.8633610882913132
          batches_per_second_std: 0.0017669161830767962
          seconds_per_batch_max: 1.158263921737671
          seconds_per_batch_mean: 1.1522538948059082
          seconds_per_batch_min: 1.1476316452026367
          seconds_per_batch_std: 0.0023470552286395576
      total:
        human_readable:
          batch_latency: 1.153 s +/- 2.343 ms [1.148 s, 1.159 s]
          batches_per_second: 0.87 +/- 0.00 [0.86, 0.87]
        metrics:
          batches_per_second_max: 0.871054992523675
          batches_per_second_mean: 0.8675093367803983
          batches_per_second_min: 0.8629965702796387
          batches_per_second_std: 0.0017628075723035823
          seconds_per_batch_max: 1.1587531566619873
          seconds_per_batch_mean: 1.152730107307434
          seconds_per_batch_min: 1.1480331420898438
          seconds_per_batch_std: 0.0023433591532932944

== Benchmarking model directly ==
Warming up with batch_size=2:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=2:  10%|█         | 1/10 [00:00<00:07,  1.28it/s]Warming up with batch_size=2:  20%|██        | 2/10 [00:01<00:06,  1.28it/s]Warming up with batch_size=2:  30%|███       | 3/10 [00:02<00:05,  1.28it/s]Warming up with batch_size=2:  40%|████      | 4/10 [00:03<00:04,  1.28it/s]Warming up with batch_size=2:  50%|█████     | 5/10 [00:03<00:03,  1.28it/s]Warming up with batch_size=2:  60%|██████    | 6/10 [00:04<00:03,  1.28it/s]Warming up with batch_size=2:  70%|███████   | 7/10 [00:05<00:02,  1.28it/s]Warming up with batch_size=2:  80%|████████  | 8/10 [00:06<00:01,  1.28it/s]Warming up with batch_size=2:  90%|█████████ | 9/10 [00:07<00:00,  1.28it/s]Warming up with batch_size=2: 100%|██████████| 10/10 [00:07<00:00,  1.28it/s]Warming up with batch_size=2: 100%|██████████| 10/10 [00:07<00:00,  1.28it/s]
Measuring inference for batch_size=2:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference for batch_size=2:   1%|          | 1/100 [00:00<01:17,  1.28it/s]Measuring inference for batch_size=2:   2%|▏         | 2/100 [00:01<01:16,  1.28it/s]Measuring inference for batch_size=2:   3%|▎         | 3/100 [00:02<01:15,  1.28it/s]Measuring inference for batch_size=2:   4%|▍         | 4/100 [00:03<01:14,  1.28it/s]Measuring inference for batch_size=2:   5%|▌         | 5/100 [00:03<01:14,  1.28it/s]Measuring inference for batch_size=2:   6%|▌         | 6/100 [00:04<01:13,  1.28it/s]Measuring inference for batch_size=2:   7%|▋         | 7/100 [00:05<01:12,  1.28it/s]Measuring inference for batch_size=2:   8%|▊         | 8/100 [00:06<01:11,  1.28it/s]Measuring inference for batch_size=2:   9%|▉         | 9/100 [00:07<01:11,  1.28it/s]Measuring inference for batch_size=2:  10%|█         | 10/100 [00:07<01:10,  1.28it/s]Measuring inference for batch_size=2:  11%|█         | 11/100 [00:08<01:09,  1.28it/s]Measuring inference for batch_size=2:  12%|█▏        | 12/100 [00:09<01:08,  1.28it/s]Measuring inference for batch_size=2:  13%|█▎        | 13/100 [00:10<01:07,  1.28it/s]Measuring inference for batch_size=2:  14%|█▍        | 14/100 [00:10<01:07,  1.28it/s]Measuring inference for batch_size=2:  15%|█▌        | 15/100 [00:11<01:06,  1.28it/s]Measuring inference for batch_size=2:  16%|█▌        | 16/100 [00:12<01:05,  1.28it/s]Measuring inference for batch_size=2:  17%|█▋        | 17/100 [00:13<01:04,  1.28it/s]Measuring inference for batch_size=2:  18%|█▊        | 18/100 [00:14<01:03,  1.28it/s]Measuring inference for batch_size=2:  19%|█▉        | 19/100 [00:14<01:03,  1.28it/s]Measuring inference for batch_size=2:  20%|██        | 20/100 [00:15<01:02,  1.28it/s]Measuring inference for batch_size=2:  21%|██        | 21/100 [00:16<01:01,  1.28it/s]Measuring inference for batch_size=2:  22%|██▏       | 22/100 [00:17<01:00,  1.28it/s]Measuring inference for batch_size=2:  23%|██▎       | 23/100 [00:17<01:00,  1.28it/s]Measuring inference for batch_size=2:  24%|██▍       | 24/100 [00:18<00:59,  1.28it/s]Measuring inference for batch_size=2:  25%|██▌       | 25/100 [00:19<00:58,  1.28it/s]Measuring inference for batch_size=2:  26%|██▌       | 26/100 [00:20<00:57,  1.28it/s]Measuring inference for batch_size=2:  27%|██▋       | 27/100 [00:21<00:57,  1.28it/s]Measuring inference for batch_size=2:  28%|██▊       | 28/100 [00:21<00:56,  1.28it/s]Measuring inference for batch_size=2:  29%|██▉       | 29/100 [00:22<00:55,  1.28it/s]Measuring inference for batch_size=2:  30%|███       | 30/100 [00:23<00:54,  1.28it/s]Measuring inference for batch_size=2:  31%|███       | 31/100 [00:24<00:53,  1.28it/s]Measuring inference for batch_size=2:  32%|███▏      | 32/100 [00:24<00:53,  1.28it/s]Measuring inference for batch_size=2:  33%|███▎      | 33/100 [00:25<00:52,  1.28it/s]Measuring inference for batch_size=2:  34%|███▍      | 34/100 [00:26<00:51,  1.28it/s]Measuring inference for batch_size=2:  35%|███▌      | 35/100 [00:27<00:50,  1.28it/s]Measuring inference for batch_size=2:  36%|███▌      | 36/100 [00:28<00:49,  1.28it/s]Measuring inference for batch_size=2:  37%|███▋      | 37/100 [00:28<00:49,  1.28it/s]Measuring inference for batch_size=2:  38%|███▊      | 38/100 [00:29<00:48,  1.28it/s]Measuring inference for batch_size=2:  39%|███▉      | 39/100 [00:30<00:47,  1.28it/s]Measuring inference for batch_size=2:  40%|████      | 40/100 [00:31<00:46,  1.28it/s]Measuring inference for batch_size=2:  41%|████      | 41/100 [00:31<00:46,  1.28it/s]Measuring inference for batch_size=2:  42%|████▏     | 42/100 [00:32<00:45,  1.28it/s]Measuring inference for batch_size=2:  43%|████▎     | 43/100 [00:33<00:44,  1.28it/s]Measuring inference for batch_size=2:  44%|████▍     | 44/100 [00:34<00:43,  1.28it/s]Measuring inference for batch_size=2:  45%|████▌     | 45/100 [00:35<00:42,  1.28it/s]Measuring inference for batch_size=2:  46%|████▌     | 46/100 [00:35<00:42,  1.28it/s]Measuring inference for batch_size=2:  47%|████▋     | 47/100 [00:36<00:41,  1.28it/s]Measuring inference for batch_size=2:  48%|████▊     | 48/100 [00:37<00:40,  1.28it/s]Measuring inference for batch_size=2:  49%|████▉     | 49/100 [00:38<00:39,  1.28it/s]Measuring inference for batch_size=2:  50%|█████     | 50/100 [00:39<00:39,  1.28it/s]Measuring inference for batch_size=2:  51%|█████     | 51/100 [00:39<00:38,  1.28it/s]Measuring inference for batch_size=2:  52%|█████▏    | 52/100 [00:40<00:37,  1.28it/s]Measuring inference for batch_size=2:  53%|█████▎    | 53/100 [00:41<00:36,  1.28it/s]Measuring inference for batch_size=2:  54%|█████▍    | 54/100 [00:42<00:35,  1.28it/s]Measuring inference for batch_size=2:  55%|█████▌    | 55/100 [00:42<00:35,  1.28it/s]Measuring inference for batch_size=2:  56%|█████▌    | 56/100 [00:43<00:34,  1.28it/s]Measuring inference for batch_size=2:  57%|█████▋    | 57/100 [00:44<00:33,  1.28it/s]Measuring inference for batch_size=2:  58%|█████▊    | 58/100 [00:45<00:32,  1.28it/s]Measuring inference for batch_size=2:  59%|█████▉    | 59/100 [00:46<00:31,  1.28it/s]Measuring inference for batch_size=2:  60%|██████    | 60/100 [00:46<00:31,  1.28it/s]Measuring inference for batch_size=2:  61%|██████    | 61/100 [00:47<00:30,  1.28it/s]Measuring inference for batch_size=2:  62%|██████▏   | 62/100 [00:48<00:29,  1.28it/s]Measuring inference for batch_size=2:  63%|██████▎   | 63/100 [00:49<00:28,  1.28it/s]Measuring inference for batch_size=2:  64%|██████▍   | 64/100 [00:49<00:28,  1.28it/s]Measuring inference for batch_size=2:  65%|██████▌   | 65/100 [00:50<00:27,  1.28it/s]Measuring inference for batch_size=2:  66%|██████▌   | 66/100 [00:51<00:26,  1.28it/s]Measuring inference for batch_size=2:  67%|██████▋   | 67/100 [00:52<00:25,  1.28it/s]Measuring inference for batch_size=2:  68%|██████▊   | 68/100 [00:53<00:24,  1.28it/s]Measuring inference for batch_size=2:  69%|██████▉   | 69/100 [00:53<00:24,  1.28it/s]Measuring inference for batch_size=2:  70%|███████   | 70/100 [00:54<00:23,  1.28it/s]Measuring inference for batch_size=2:  71%|███████   | 71/100 [00:55<00:22,  1.28it/s]Measuring inference for batch_size=2:  72%|███████▏  | 72/100 [00:56<00:21,  1.28it/s]Measuring inference for batch_size=2:  73%|███████▎  | 73/100 [00:56<00:21,  1.28it/s]Measuring inference for batch_size=2:  74%|███████▍  | 74/100 [00:57<00:20,  1.28it/s]Measuring inference for batch_size=2:  75%|███████▌  | 75/100 [00:58<00:19,  1.28it/s]Measuring inference for batch_size=2:  76%|███████▌  | 76/100 [00:59<00:18,  1.28it/s]Measuring inference for batch_size=2:  77%|███████▋  | 77/100 [01:00<00:17,  1.28it/s]Measuring inference for batch_size=2:  78%|███████▊  | 78/100 [01:00<00:17,  1.28it/s]Measuring inference for batch_size=2:  79%|███████▉  | 79/100 [01:01<00:16,  1.28it/s]Measuring inference for batch_size=2:  80%|████████  | 80/100 [01:02<00:15,  1.28it/s]Measuring inference for batch_size=2:  81%|████████  | 81/100 [01:03<00:14,  1.28it/s]Measuring inference for batch_size=2:  82%|████████▏ | 82/100 [01:03<00:14,  1.28it/s]Measuring inference for batch_size=2:  83%|████████▎ | 83/100 [01:04<00:13,  1.28it/s]Measuring inference for batch_size=2:  84%|████████▍ | 84/100 [01:05<00:12,  1.28it/s]Measuring inference for batch_size=2:  85%|████████▌ | 85/100 [01:06<00:11,  1.28it/s]Measuring inference for batch_size=2:  86%|████████▌ | 86/100 [01:07<00:10,  1.28it/s]Measuring inference for batch_size=2:  87%|████████▋ | 87/100 [01:07<00:10,  1.28it/s]Measuring inference for batch_size=2:  88%|████████▊ | 88/100 [01:08<00:09,  1.28it/s]Measuring inference for batch_size=2:  89%|████████▉ | 89/100 [01:09<00:08,  1.28it/s]Measuring inference for batch_size=2:  90%|█████████ | 90/100 [01:10<00:07,  1.28it/s]Measuring inference for batch_size=2:  91%|█████████ | 91/100 [01:10<00:07,  1.28it/s]Measuring inference for batch_size=2:  92%|█████████▏| 92/100 [01:11<00:06,  1.28it/s]Measuring inference for batch_size=2:  93%|█████████▎| 93/100 [01:12<00:05,  1.28it/s]Measuring inference for batch_size=2:  94%|█████████▍| 94/100 [01:13<00:04,  1.28it/s]Measuring inference for batch_size=2:  95%|█████████▌| 95/100 [01:14<00:03,  1.28it/s]Measuring inference for batch_size=2:  96%|█████████▌| 96/100 [01:14<00:03,  1.28it/s]Measuring inference for batch_size=2:  97%|█████████▋| 97/100 [01:15<00:02,  1.28it/s]Measuring inference for batch_size=2:  98%|█████████▊| 98/100 [01:16<00:01,  1.28it/s]Measuring inference for batch_size=2:  99%|█████████▉| 99/100 [01:17<00:00,  1.28it/s]Measuring inference for batch_size=2: 100%|██████████| 100/100 [01:18<00:00,  1.28it/s]Measuring inference for batch_size=2: 100%|██████████| 100/100 [01:18<00:00,  1.28it/s]
Measuring energy for batch_size=1:   0%|          | 0/10 [00:00<?, ?it/s]Measuring energy for batch_size=1:  10%|█         | 1/10 [00:00<00:08,  1.02it/s]Measuring energy for batch_size=1:  20%|██        | 2/10 [00:01<00:06,  1.15it/s]Measuring energy for batch_size=1:  30%|███       | 3/10 [00:02<00:05,  1.20it/s]Measuring energy for batch_size=1:  40%|████      | 4/10 [00:03<00:04,  1.22it/s]Measuring energy for batch_size=1:  50%|█████     | 5/10 [00:04<00:04,  1.24it/s]Measuring energy for batch_size=1:  60%|██████    | 6/10 [00:04<00:03,  1.25it/s]Measuring energy for batch_size=1:  70%|███████   | 7/10 [00:05<00:02,  1.25it/s]Measuring energy for batch_size=1:  80%|████████  | 8/10 [00:06<00:01,  1.26it/s]Measuring energy for batch_size=1:  90%|█████████ | 9/10 [00:07<00:00,  1.26it/s]Measuring energy for batch_size=1: 100%|██████████| 10/10 [00:08<00:00,  1.26it/s]Measuring energy for batch_size=1: 100%|██████████| 10/10 [00:08<00:00,  1.23it/s]
Warming up with batch_size=2:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=2:  10%|█         | 1/10 [00:01<00:10,  1.14s/it]Warming up with batch_size=2:  20%|██        | 2/10 [00:02<00:09,  1.13s/it]Warming up with batch_size=2:  30%|███       | 3/10 [00:03<00:07,  1.14s/it]Warming up with batch_size=2:  40%|████      | 4/10 [00:04<00:06,  1.14s/it]Warming up with batch_size=2:  50%|█████     | 5/10 [00:05<00:05,  1.14s/it]Warming up with batch_size=2:  60%|██████    | 6/10 [00:06<00:04,  1.13s/it]Warming up with batch_size=2:  70%|███████   | 7/10 [00:07<00:03,  1.13s/it]Warming up with batch_size=2:  80%|████████  | 8/10 [00:09<00:02,  1.13s/it]Warming up with batch_size=2:  90%|█████████ | 9/10 [00:10<00:01,  1.13s/it]Warming up with batch_size=2: 100%|██████████| 10/10 [00:11<00:00,  1.13s/it]Warming up with batch_size=2: 100%|██████████| 10/10 [00:11<00:00,  1.13s/it]
Measuring inference for batch_size=2:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference for batch_size=2:   1%|          | 1/100 [00:01<01:52,  1.13s/it]Measuring inference for batch_size=2:   2%|▏         | 2/100 [00:02<01:51,  1.13s/it]Measuring inference for batch_size=2:   3%|▎         | 3/100 [00:03<01:49,  1.13s/it]Measuring inference for batch_size=2:   4%|▍         | 4/100 [00:04<01:48,  1.13s/it]Measuring inference for batch_size=2:   5%|▌         | 5/100 [00:05<01:47,  1.13s/it]Measuring inference for batch_size=2:   6%|▌         | 6/100 [00:06<01:46,  1.13s/it]Measuring inference for batch_size=2:   7%|▋         | 7/100 [00:07<01:45,  1.13s/it]Measuring inference for batch_size=2:   8%|▊         | 8/100 [00:09<01:44,  1.13s/it]Measuring inference for batch_size=2:   9%|▉         | 9/100 [00:10<01:43,  1.13s/it]Measuring inference for batch_size=2:  10%|█         | 10/100 [00:11<01:42,  1.13s/it]Measuring inference for batch_size=2:  11%|█         | 11/100 [00:12<01:40,  1.13s/it]Measuring inference for batch_size=2:  12%|█▏        | 12/100 [00:13<01:39,  1.13s/it]Measuring inference for batch_size=2:  13%|█▎        | 13/100 [00:14<01:38,  1.13s/it]Measuring inference for batch_size=2:  14%|█▍        | 14/100 [00:15<01:37,  1.13s/it]Measuring inference for batch_size=2:  15%|█▌        | 15/100 [00:17<01:36,  1.13s/it]Measuring inference for batch_size=2:  16%|█▌        | 16/100 [00:18<01:35,  1.13s/it]Measuring inference for batch_size=2:  17%|█▋        | 17/100 [00:19<01:34,  1.13s/it]Measuring inference for batch_size=2:  18%|█▊        | 18/100 [00:20<01:33,  1.13s/it]Measuring inference for batch_size=2:  19%|█▉        | 19/100 [00:21<01:31,  1.13s/it]Measuring inference for batch_size=2:  20%|██        | 20/100 [00:22<01:30,  1.13s/it]Measuring inference for batch_size=2:  21%|██        | 21/100 [00:23<01:29,  1.13s/it]Measuring inference for batch_size=2:  22%|██▏       | 22/100 [00:24<01:28,  1.13s/it]Measuring inference for batch_size=2:  23%|██▎       | 23/100 [00:26<01:27,  1.13s/it]Measuring inference for batch_size=2:  24%|██▍       | 24/100 [00:27<01:26,  1.13s/it]Measuring inference for batch_size=2:  25%|██▌       | 25/100 [00:28<01:25,  1.13s/it]Measuring inference for batch_size=2:  26%|██▌       | 26/100 [00:29<01:23,  1.13s/it]Measuring inference for batch_size=2:  27%|██▋       | 27/100 [00:30<01:22,  1.14s/it]Measuring inference for batch_size=2:  28%|██▊       | 28/100 [00:31<01:21,  1.13s/it]Measuring inference for batch_size=2:  29%|██▉       | 29/100 [00:32<01:20,  1.14s/it]Measuring inference for batch_size=2:  30%|███       | 30/100 [00:34<01:19,  1.14s/it]Measuring inference for batch_size=2:  31%|███       | 31/100 [00:35<01:18,  1.13s/it]Measuring inference for batch_size=2:  32%|███▏      | 32/100 [00:36<01:17,  1.13s/it]Measuring inference for batch_size=2:  33%|███▎      | 33/100 [00:37<01:16,  1.13s/it]Measuring inference for batch_size=2:  34%|███▍      | 34/100 [00:38<01:14,  1.14s/it]Measuring inference for batch_size=2:  35%|███▌      | 35/100 [00:39<01:13,  1.13s/it]Measuring inference for batch_size=2:  36%|███▌      | 36/100 [00:40<01:12,  1.13s/it]Measuring inference for batch_size=2:  37%|███▋      | 37/100 [00:41<01:11,  1.13s/it]Measuring inference for batch_size=2:  38%|███▊      | 38/100 [00:43<01:10,  1.13s/it]Measuring inference for batch_size=2:  39%|███▉      | 39/100 [00:44<01:09,  1.13s/it]Measuring inference for batch_size=2:  40%|████      | 40/100 [00:45<01:08,  1.13s/it]Measuring inference for batch_size=2:  41%|████      | 41/100 [00:46<01:06,  1.13s/it]Measuring inference for batch_size=2:  42%|████▏     | 42/100 [00:47<01:05,  1.13s/it]Measuring inference for batch_size=2:  43%|████▎     | 43/100 [00:48<01:04,  1.14s/it]Measuring inference for batch_size=2:  44%|████▍     | 44/100 [00:49<01:03,  1.14s/it]Measuring inference for batch_size=2:  45%|████▌     | 45/100 [00:51<01:02,  1.14s/it]Measuring inference for batch_size=2:  46%|████▌     | 46/100 [00:52<01:01,  1.14s/it]Measuring inference for batch_size=2:  47%|████▋     | 47/100 [00:53<01:00,  1.13s/it]Measuring inference for batch_size=2:  48%|████▊     | 48/100 [00:54<00:59,  1.14s/it]Measuring inference for batch_size=2:  49%|████▉     | 49/100 [00:55<00:57,  1.14s/it]Measuring inference for batch_size=2:  50%|█████     | 50/100 [00:56<00:56,  1.13s/it]Measuring inference for batch_size=2:  51%|█████     | 51/100 [00:57<00:55,  1.14s/it]Measuring inference for batch_size=2:  52%|█████▏    | 52/100 [00:59<00:54,  1.14s/it]Measuring inference for batch_size=2:  53%|█████▎    | 53/100 [01:00<00:53,  1.14s/it]Measuring inference for batch_size=2:  54%|█████▍    | 54/100 [01:01<00:52,  1.14s/it]Measuring inference for batch_size=2:  55%|█████▌    | 55/100 [01:02<00:51,  1.14s/it]Measuring inference for batch_size=2:  56%|█████▌    | 56/100 [01:03<00:50,  1.14s/it]Measuring inference for batch_size=2:  57%|█████▋    | 57/100 [01:04<00:48,  1.14s/it]Measuring inference for batch_size=2:  58%|█████▊    | 58/100 [01:05<00:47,  1.14s/it]Measuring inference for batch_size=2:  59%|█████▉    | 59/100 [01:06<00:46,  1.14s/it]Measuring inference for batch_size=2:  60%|██████    | 60/100 [01:08<00:45,  1.14s/it]Measuring inference for batch_size=2:  61%|██████    | 61/100 [01:09<00:44,  1.14s/it]Measuring inference for batch_size=2:  62%|██████▏   | 62/100 [01:10<00:43,  1.14s/it]Measuring inference for batch_size=2:  63%|██████▎   | 63/100 [01:11<00:42,  1.14s/it]Measuring inference for batch_size=2:  64%|██████▍   | 64/100 [01:12<00:40,  1.14s/it]Measuring inference for batch_size=2:  65%|██████▌   | 65/100 [01:13<00:39,  1.14s/it]Measuring inference for batch_size=2:  66%|██████▌   | 66/100 [01:14<00:38,  1.14s/it]Measuring inference for batch_size=2:  67%|██████▋   | 67/100 [01:16<00:37,  1.14s/it]Measuring inference for batch_size=2:  68%|██████▊   | 68/100 [01:17<00:36,  1.14s/it]Measuring inference for batch_size=2:  69%|██████▉   | 69/100 [01:18<00:35,  1.14s/it]Measuring inference for batch_size=2:  70%|███████   | 70/100 [01:19<00:34,  1.14s/it]Measuring inference for batch_size=2:  71%|███████   | 71/100 [01:20<00:32,  1.14s/it]Measuring inference for batch_size=2:  72%|███████▏  | 72/100 [01:21<00:31,  1.14s/it]Measuring inference for batch_size=2:  73%|███████▎  | 73/100 [01:22<00:30,  1.14s/it]Measuring inference for batch_size=2:  74%|███████▍  | 74/100 [01:23<00:29,  1.13s/it]Measuring inference for batch_size=2:  75%|███████▌  | 75/100 [01:25<00:28,  1.13s/it]Measuring inference for batch_size=2:  76%|███████▌  | 76/100 [01:26<00:27,  1.13s/it]Measuring inference for batch_size=2:  77%|███████▋  | 77/100 [01:27<00:26,  1.13s/it]Measuring inference for batch_size=2:  78%|███████▊  | 78/100 [01:28<00:24,  1.13s/it]Measuring inference for batch_size=2:  79%|███████▉  | 79/100 [01:29<00:23,  1.14s/it]Measuring inference for batch_size=2:  80%|████████  | 80/100 [01:30<00:22,  1.13s/it]Measuring inference for batch_size=2:  81%|████████  | 81/100 [01:31<00:21,  1.14s/it]Measuring inference for batch_size=2:  82%|████████▏ | 82/100 [01:33<00:20,  1.14s/it]Measuring inference for batch_size=2:  83%|████████▎ | 83/100 [01:34<00:19,  1.14s/it]Measuring inference for batch_size=2:  84%|████████▍ | 84/100 [01:35<00:18,  1.14s/it]Measuring inference for batch_size=2:  85%|████████▌ | 85/100 [01:36<00:17,  1.14s/it]Measuring inference for batch_size=2:  86%|████████▌ | 86/100 [01:37<00:15,  1.14s/it]Measuring inference for batch_size=2:  87%|████████▋ | 87/100 [01:38<00:14,  1.14s/it]Measuring inference for batch_size=2:  88%|████████▊ | 88/100 [01:39<00:13,  1.13s/it]Measuring inference for batch_size=2:  89%|████████▉ | 89/100 [01:41<00:12,  1.13s/it]Measuring inference for batch_size=2:  90%|█████████ | 90/100 [01:42<00:11,  1.13s/it]Measuring inference for batch_size=2:  91%|█████████ | 91/100 [01:43<00:10,  1.13s/it]Measuring inference for batch_size=2:  92%|█████████▏| 92/100 [01:44<00:09,  1.14s/it]Measuring inference for batch_size=2:  93%|█████████▎| 93/100 [01:45<00:07,  1.13s/it]Measuring inference for batch_size=2:  94%|█████████▍| 94/100 [01:46<00:06,  1.13s/it]Measuring inference for batch_size=2:  95%|█████████▌| 95/100 [01:47<00:05,  1.13s/it]Measuring inference for batch_size=2:  96%|█████████▌| 96/100 [01:48<00:04,  1.13s/it]Measuring inference for batch_size=2:  97%|█████████▋| 97/100 [01:50<00:03,  1.13s/it]Measuring inference for batch_size=2:  98%|█████████▊| 98/100 [01:51<00:02,  1.14s/it]Measuring inference for batch_size=2:  99%|█████████▉| 99/100 [01:52<00:01,  1.14s/it]Measuring inference for batch_size=2: 100%|██████████| 100/100 [01:53<00:00,  1.14s/it]Measuring inference for batch_size=2: 100%|██████████| 100/100 [01:53<00:00,  1.14s/it]
Measuring energy for batch_size=2:   0%|          | 0/10 [00:00<?, ?it/s]Measuring energy for batch_size=2:  10%|█         | 1/10 [00:04<00:39,  4.34s/it]Measuring energy for batch_size=2:  20%|██        | 2/10 [00:05<00:19,  2.46s/it]Measuring energy for batch_size=2:  30%|███       | 3/10 [00:06<00:13,  1.86s/it]Measuring energy for batch_size=2:  40%|████      | 4/10 [00:07<00:09,  1.58s/it]Measuring energy for batch_size=2:  50%|█████     | 5/10 [00:08<00:07,  1.44s/it]Measuring energy for batch_size=2:  60%|██████    | 6/10 [00:10<00:05,  1.35s/it]Measuring energy for batch_size=2:  70%|███████   | 7/10 [00:11<00:03,  1.28s/it]Measuring energy for batch_size=2:  80%|████████  | 8/10 [00:12<00:02,  1.24s/it]Measuring energy for batch_size=2:  90%|█████████ | 9/10 [00:13<00:01,  1.21s/it]Measuring energy for batch_size=2: 100%|██████████| 10/10 [00:14<00:00,  1.19s/it]Measuring energy for batch_size=2: 100%|██████████| 10/10 [00:14<00:00,  1.47s/it]
learner.model.forward:
  device: cuda
  energy:
    batch_size_1:
      joules: 22.97210638529301
      kWh: 6.381140662581392e-06
    batch_size_2:
      joules: 38.60659488108157
      kWh: 1.072405413363377e-05
  flops: 19166052038
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 13.07 GB
      total: 31.17 GB
      used: 17.67 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  params: 6153432
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: 3.064 ms +/- 201.768 us [2.744 ms, 3.910 ms]
          batches_per_second: 327.54 +/- 18.41 [255.78, 364.41]
        metrics:
          batches_per_second_max: 364.405212858384
          batches_per_second_mean: 327.5399736043166
          batches_per_second_min: 255.78143676058056
          batches_per_second_std: 18.41169715985179
          seconds_per_batch_max: 0.003909587860107422
          seconds_per_batch_mean: 0.0030643510818481445
          seconds_per_batch_min: 0.0027441978454589844
          seconds_per_batch_std: 0.00020176776078231052
      gpu_to_cpu:
        human_readable:
          batch_latency: 38.593 ms +/- 468.229 us [37.871 ms, 41.240 ms]
          batches_per_second: 25.92 +/- 0.30 [24.25, 26.41]
        metrics:
          batches_per_second_max: 26.40517740676387
          batches_per_second_mean: 25.915423295508845
          batches_per_second_min: 24.248316211200592
          batches_per_second_std: 0.30147560416136715
          seconds_per_batch_max: 0.04123997688293457
          seconds_per_batch_mean: 0.03859250545501709
          seconds_per_batch_min: 0.037871360778808594
          seconds_per_batch_std: 0.0004682285770227378
      on_device_inference:
        human_readable:
          batch_latency: 737.422 ms +/- 1.014 ms [735.043 ms, 740.975 ms]
          batches_per_second: 1.36 +/- 0.00 [1.35, 1.36]
        metrics:
          batches_per_second_max: 1.3604649767287826
          batches_per_second_mean: 1.3560789401170459
          batches_per_second_min: 1.3495738081961521
          batches_per_second_std: 0.0018642439367425544
          seconds_per_batch_max: 0.7409746646881104
          seconds_per_batch_mean: 0.7374215912818909
          seconds_per_batch_min: 0.7350428104400635
          seconds_per_batch_std: 0.0010137781209895718
      total:
        human_readable:
          batch_latency: 779.078 ms +/- 1.033 ms [776.764 ms, 782.469 ms]
          batches_per_second: 1.28 +/- 0.00 [1.28, 1.29]
        metrics:
          batches_per_second_max: 1.2873924488272501
          batches_per_second_mean: 1.2835700427851606
          batches_per_second_min: 1.2780062353896144
          batches_per_second_std: 0.0017011846876041727
          seconds_per_batch_max: 0.7824687957763672
          seconds_per_batch_mean: 0.7790784478187561
          seconds_per_batch_min: 0.776763916015625
          seconds_per_batch_std: 0.0010325766486110868
    batch_size_2:
      cpu_to_gpu:
        human_readable:
          batch_latency: 5.867 ms +/- 501.443 us [5.347 ms, 7.728 ms]
          batches_per_second: 171.50 +/- 12.51 [129.40, 187.03]
        metrics:
          batches_per_second_max: 187.0286274859538
          batches_per_second_mean: 171.49991297653258
          batches_per_second_min: 129.39791448139692
          batches_per_second_std: 12.513087110530277
          seconds_per_batch_max: 0.007728099822998047
          seconds_per_batch_mean: 0.005867326259613037
          seconds_per_batch_min: 0.005346775054931641
          seconds_per_batch_std: 0.0005014426304825627
      gpu_to_cpu:
        human_readable:
          batch_latency: 40.859 ms +/- 632.289 us [39.776 ms, 43.863 ms]
          batches_per_second: 24.48 +/- 0.36 [22.80, 25.14]
        metrics:
          batches_per_second_max: 25.141034939549606
          batches_per_second_mean: 24.47987675138398
          batches_per_second_min: 22.798104100534854
          batches_per_second_std: 0.35769707567832754
          seconds_per_batch_max: 0.04386329650878906
          seconds_per_batch_mean: 0.040859115123748777
          seconds_per_batch_min: 0.03977560997009277
          seconds_per_batch_std: 0.0006322886594721905
      on_device_inference:
        human_readable:
          batch_latency: 1.087 s +/- 1.719 ms [1.081 s, 1.095 s]
          batches_per_second: 0.92 +/- 0.00 [0.91, 0.92]
        metrics:
          batches_per_second_max: 0.924817806016657
          batches_per_second_mean: 0.9197811982333967
          batches_per_second_min: 0.9136203426764609
          batches_per_second_std: 0.0014540168273176192
          seconds_per_batch_max: 1.0945465564727783
          seconds_per_batch_mean: 1.0872178101539611
          seconds_per_batch_min: 1.081294059753418
          seconds_per_batch_std: 0.0017194991196300866
      total:
        human_readable:
          batch_latency: 1.134 s +/- 1.794 ms [1.130 s, 1.142 s]
          batches_per_second: 0.88 +/- 0.00 [0.88, 0.88]
        metrics:
          batches_per_second_max: 0.8848707146532535
          batches_per_second_mean: 0.8818797740930233
          batches_per_second_min: 0.8757699493618659
          batches_per_second_std: 0.0013934063043118357
          seconds_per_batch_max: 1.1418523788452148
          seconds_per_batch_mean: 1.133944251537323
          seconds_per_batch_min: 1.1301085948944092
          seconds_per_batch_std: 0.0017944456825598131

