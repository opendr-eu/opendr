INFO:benchmark:==== Benchmarking CoX3DLearner (xs) ====
WARNING:opendr.perception.activity_recognition.cox3d.algorithm.utils:Padding along the temporal dimension only affects the computation in `forward3d`. In `forward` it is omitted.
INFO:benchmark:== Benchmarking learner.infer ==
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'

Warming up with batch_size=128:   0%|          | 0/10 [00:00<?, ?it/s]
Warming up with batch_size=128:  10%|█         | 1/10 [00:00<00:01,  8.14it/s]
Warming up with batch_size=128:  20%|██        | 2/10 [00:00<00:00,  8.83it/s]
Warming up with batch_size=128:  30%|███       | 3/10 [00:00<00:00,  9.04it/s]
Warming up with batch_size=128:  40%|████      | 4/10 [00:00<00:00,  9.21it/s]
Warming up with batch_size=128:  50%|█████     | 5/10 [00:00<00:00,  9.22it/s]
Warming up with batch_size=128:  60%|██████    | 6/10 [00:00<00:00,  9.27it/s]
Warming up with batch_size=128:  70%|███████   | 7/10 [00:00<00:00,  8.76it/s]
Warming up with batch_size=128:  80%|████████  | 8/10 [00:00<00:00,  7.43it/s]
Warming up with batch_size=128:  90%|█████████ | 9/10 [00:01<00:00,  7.54it/s]
Warming up with batch_size=128: 100%|██████████| 10/10 [00:01<00:00,  7.82it/s]
Warming up with batch_size=128: 100%|██████████| 10/10 [00:01<00:00,  8.27it/s]

Measuring inference with batch_size=128:   0%|          | 0/100 [00:00<?, ?it/s]
Measuring inference with batch_size=128:   1%|          | 1/100 [00:00<00:12,  7.82it/s]
Measuring inference with batch_size=128:   2%|▏         | 2/100 [00:00<00:11,  8.21it/s]
Measuring inference with batch_size=128:   3%|▎         | 3/100 [00:00<00:11,  8.34it/s]
Measuring inference with batch_size=128:   4%|▍         | 4/100 [00:00<00:11,  8.39it/s]
Measuring inference with batch_size=128:   5%|▌         | 5/100 [00:00<00:11,  8.36it/s]
Measuring inference with batch_size=128:   6%|▌         | 6/100 [00:00<00:11,  8.32it/s]
Measuring inference with batch_size=128:   7%|▋         | 7/100 [00:00<00:11,  8.41it/s]
Measuring inference with batch_size=128:   8%|▊         | 8/100 [00:00<00:10,  8.44it/s]
Measuring inference with batch_size=128:   9%|▉         | 9/100 [00:01<00:10,  8.32it/s]
Measuring inference with batch_size=128:  10%|█         | 10/100 [00:01<00:10,  8.32it/s]
Measuring inference with batch_size=128:  11%|█         | 11/100 [00:01<00:10,  8.41it/s]
Measuring inference with batch_size=128:  12%|█▏        | 12/100 [00:01<00:10,  8.50it/s]
Measuring inference with batch_size=128:  13%|█▎        | 13/100 [00:01<00:10,  8.51it/s]
Measuring inference with batch_size=128:  14%|█▍        | 14/100 [00:01<00:10,  8.49it/s]
Measuring inference with batch_size=128:  15%|█▌        | 15/100 [00:01<00:09,  8.54it/s]
Measuring inference with batch_size=128:  16%|█▌        | 16/100 [00:01<00:09,  8.54it/s]
Measuring inference with batch_size=128:  17%|█▋        | 17/100 [00:02<00:09,  8.45it/s]
Measuring inference with batch_size=128:  18%|█▊        | 18/100 [00:02<00:09,  8.45it/s]
Measuring inference with batch_size=128:  19%|█▉        | 19/100 [00:02<00:09,  8.47it/s]
Measuring inference with batch_size=128:  20%|██        | 20/100 [00:02<00:09,  8.50it/s]
Measuring inference with batch_size=128:  21%|██        | 21/100 [00:02<00:09,  8.33it/s]
Measuring inference with batch_size=128:  22%|██▏       | 22/100 [00:02<00:09,  8.27it/s]
Measuring inference with batch_size=128:  23%|██▎       | 23/100 [00:02<00:09,  8.24it/s]
Measuring inference with batch_size=128:  24%|██▍       | 24/100 [00:02<00:09,  8.16it/s]
Measuring inference with batch_size=128:  25%|██▌       | 25/100 [00:03<00:09,  7.89it/s]
Measuring inference with batch_size=128:  26%|██▌       | 26/100 [00:03<00:10,  7.39it/s]
Measuring inference with batch_size=128:  27%|██▋       | 27/100 [00:03<00:10,  6.98it/s]
Measuring inference with batch_size=128:  28%|██▊       | 28/100 [00:03<00:10,  6.72it/s]
Measuring inference with batch_size=128:  29%|██▉       | 29/100 [00:03<00:10,  6.52it/s]
Measuring inference with batch_size=128:  30%|███       | 30/100 [00:03<00:10,  6.39it/s]
Measuring inference with batch_size=128:  31%|███       | 31/100 [00:03<00:10,  6.79it/s]
Measuring inference with batch_size=128:  32%|███▏      | 32/100 [00:04<00:09,  7.31it/s]
Measuring inference with batch_size=128:  33%|███▎      | 33/100 [00:04<00:08,  7.69it/s]
Measuring inference with batch_size=128:  34%|███▍      | 34/100 [00:04<00:08,  7.78it/s]
Measuring inference with batch_size=128:  35%|███▌      | 35/100 [00:04<00:08,  7.91it/s]
Measuring inference with batch_size=128:  36%|███▌      | 36/100 [00:04<00:07,  8.10it/s]
Measuring inference with batch_size=128:  37%|███▋      | 37/100 [00:04<00:07,  7.95it/s]
Measuring inference with batch_size=128:  38%|███▊      | 38/100 [00:04<00:07,  8.15it/s]
Measuring inference with batch_size=128:  39%|███▉      | 39/100 [00:04<00:07,  8.31it/s]
Measuring inference with batch_size=128:  40%|████      | 40/100 [00:05<00:07,  8.40it/s]
Measuring inference with batch_size=128:  41%|████      | 41/100 [00:05<00:07,  8.38it/s]
Measuring inference with batch_size=128:  42%|████▏     | 42/100 [00:05<00:06,  8.48it/s]
Measuring inference with batch_size=128:  43%|████▎     | 43/100 [00:05<00:06,  8.48it/s]
Measuring inference with batch_size=128:  44%|████▍     | 44/100 [00:05<00:06,  8.50it/s]
Measuring inference with batch_size=128:  45%|████▌     | 45/100 [00:05<00:06,  8.52it/s]
Measuring inference with batch_size=128:  46%|████▌     | 46/100 [00:05<00:06,  8.52it/s]
Measuring inference with batch_size=128:  47%|████▋     | 47/100 [00:05<00:06,  8.53it/s]
Measuring inference with batch_size=128:  48%|████▊     | 48/100 [00:05<00:06,  8.56it/s]
Measuring inference with batch_size=128:  49%|████▉     | 49/100 [00:06<00:05,  8.60it/s]
Measuring inference with batch_size=128:  50%|█████     | 50/100 [00:06<00:05,  8.60it/s]
Measuring inference with batch_size=128:  51%|█████     | 51/100 [00:06<00:05,  8.59it/s]
Measuring inference with batch_size=128:  52%|█████▏    | 52/100 [00:06<00:05,  8.61it/s]
Measuring inference with batch_size=128:  53%|█████▎    | 53/100 [00:06<00:05,  8.61it/s]
Measuring inference with batch_size=128:  54%|█████▍    | 54/100 [00:06<00:05,  8.61it/s]
Measuring inference with batch_size=128:  55%|█████▌    | 55/100 [00:06<00:05,  8.58it/s]
Measuring inference with batch_size=128:  56%|█████▌    | 56/100 [00:06<00:05,  8.56it/s]
Measuring inference with batch_size=128:  57%|█████▋    | 57/100 [00:06<00:05,  8.56it/s]
Measuring inference with batch_size=128:  58%|█████▊    | 58/100 [00:07<00:04,  8.58it/s]
Measuring inference with batch_size=128:  59%|█████▉    | 59/100 [00:07<00:04,  8.55it/s]
Measuring inference with batch_size=128:  60%|██████    | 60/100 [00:07<00:04,  8.40it/s]
Measuring inference with batch_size=128:  61%|██████    | 61/100 [00:07<00:04,  8.48it/s]
Measuring inference with batch_size=128:  62%|██████▏   | 62/100 [00:07<00:04,  8.49it/s]
Measuring inference with batch_size=128:  63%|██████▎   | 63/100 [00:07<00:04,  8.43it/s]
Measuring inference with batch_size=128:  64%|██████▍   | 64/100 [00:07<00:04,  8.48it/s]
Measuring inference with batch_size=128:  65%|██████▌   | 65/100 [00:07<00:04,  8.56it/s]
Measuring inference with batch_size=128:  66%|██████▌   | 66/100 [00:08<00:04,  8.42it/s]
Measuring inference with batch_size=128:  67%|██████▋   | 67/100 [00:08<00:03,  8.50it/s]
Measuring inference with batch_size=128:  68%|██████▊   | 68/100 [00:08<00:03,  8.55it/s]
Measuring inference with batch_size=128:  69%|██████▉   | 69/100 [00:08<00:03,  8.45it/s]
Measuring inference with batch_size=128:  70%|███████   | 70/100 [00:08<00:03,  8.48it/s]
Measuring inference with batch_size=128:  71%|███████   | 71/100 [00:08<00:03,  8.18it/s]
Measuring inference with batch_size=128:  72%|███████▏  | 72/100 [00:08<00:03,  8.26it/s]
Measuring inference with batch_size=128:  73%|███████▎  | 73/100 [00:08<00:03,  8.20it/s]
Measuring inference with batch_size=128:  74%|███████▍  | 74/100 [00:09<00:03,  8.02it/s]
Measuring inference with batch_size=128:  75%|███████▌  | 75/100 [00:09<00:03,  8.19it/s]
Measuring inference with batch_size=128:  76%|███████▌  | 76/100 [00:09<00:02,  8.35it/s]
Measuring inference with batch_size=128:  77%|███████▋  | 77/100 [00:09<00:02,  8.42it/s]
Measuring inference with batch_size=128:  78%|███████▊  | 78/100 [00:09<00:02,  8.03it/s]
Measuring inference with batch_size=128:  79%|███████▉  | 79/100 [00:09<00:02,  7.90it/s]
Measuring inference with batch_size=128:  80%|████████  | 80/100 [00:09<00:02,  7.48it/s]
Measuring inference with batch_size=128:  81%|████████  | 81/100 [00:09<00:02,  7.15it/s]
Measuring inference with batch_size=128:  82%|████████▏ | 82/100 [00:10<00:02,  7.48it/s]
Measuring inference with batch_size=128:  83%|████████▎ | 83/100 [00:10<00:02,  6.89it/s]
Measuring inference with batch_size=128:  84%|████████▍ | 84/100 [00:10<00:02,  6.78it/s]
Measuring inference with batch_size=128:  85%|████████▌ | 85/100 [00:10<00:02,  6.89it/s]
Measuring inference with batch_size=128:  86%|████████▌ | 86/100 [00:10<00:01,  7.17it/s]
Measuring inference with batch_size=128:  87%|████████▋ | 87/100 [00:10<00:01,  7.52it/s]
Measuring inference with batch_size=128:  88%|████████▊ | 88/100 [00:10<00:01,  7.81it/s]
Measuring inference with batch_size=128:  89%|████████▉ | 89/100 [00:11<00:01,  7.55it/s]
Measuring inference with batch_size=128:  90%|█████████ | 90/100 [00:11<00:01,  7.84it/s]
Measuring inference with batch_size=128:  91%|█████████ | 91/100 [00:11<00:01,  7.92it/s]
Measuring inference with batch_size=128:  92%|█████████▏| 92/100 [00:11<00:01,  7.93it/s]
Measuring inference with batch_size=128:  93%|█████████▎| 93/100 [00:11<00:00,  7.95it/s]
Measuring inference with batch_size=128:  94%|█████████▍| 94/100 [00:11<00:00,  8.14it/s]
Measuring inference with batch_size=128:  95%|█████████▌| 95/100 [00:11<00:00,  8.18it/s]
Measuring inference with batch_size=128:  96%|█████████▌| 96/100 [00:11<00:00,  8.26it/s]
Measuring inference with batch_size=128:  97%|█████████▋| 97/100 [00:12<00:00,  7.86it/s]
Measuring inference with batch_size=128:  98%|█████████▊| 98/100 [00:12<00:00,  7.09it/s]
Measuring inference with batch_size=128:  99%|█████████▉| 99/100 [00:12<00:00,  7.00it/s]
Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:12<00:00,  7.02it/s]
Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:12<00:00,  8.01it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.

Warming up with batch_size=128:   0%|          | 0/10 [00:00<?, ?it/s]
Warming up with batch_size=128:  10%|█         | 1/10 [00:00<00:06,  1.45it/s]
Warming up with batch_size=128:  20%|██        | 2/10 [00:01<00:05,  1.51it/s]
Warming up with batch_size=128:  30%|███       | 3/10 [00:01<00:04,  1.51it/s]
Warming up with batch_size=128:  40%|████      | 4/10 [00:02<00:03,  1.53it/s]
Warming up with batch_size=128:  50%|█████     | 5/10 [00:03<00:03,  1.54it/s]
Warming up with batch_size=128:  60%|██████    | 6/10 [00:03<00:02,  1.57it/s]
Warming up with batch_size=128:  70%|███████   | 7/10 [00:04<00:01,  1.57it/s]
Warming up with batch_size=128:  80%|████████  | 8/10 [00:05<00:01,  1.56it/s]
Warming up with batch_size=128:  90%|█████████ | 9/10 [00:05<00:00,  1.55it/s]
Warming up with batch_size=128: 100%|██████████| 10/10 [00:06<00:00,  1.57it/s]
Warming up with batch_size=128: 100%|██████████| 10/10 [00:06<00:00,  1.55it/s]

Measuring inference with batch_size=128:   0%|          | 0/100 [00:00<?, ?it/s]
Measuring inference with batch_size=128:   1%|          | 1/100 [00:00<01:01,  1.60it/s]
Measuring inference with batch_size=128:   2%|▏         | 2/100 [00:01<01:01,  1.59it/s]
Measuring inference with batch_size=128:   3%|▎         | 3/100 [00:01<01:00,  1.61it/s]
Measuring inference with batch_size=128:   4%|▍         | 4/100 [00:02<00:59,  1.62it/s]
Measuring inference with batch_size=128:   5%|▌         | 5/100 [00:03<00:58,  1.62it/s]
Measuring inference with batch_size=128:   6%|▌         | 6/100 [00:03<00:58,  1.60it/s]
Measuring inference with batch_size=128:   7%|▋         | 7/100 [00:04<00:58,  1.58it/s]
Measuring inference with batch_size=128:   8%|▊         | 8/100 [00:05<00:57,  1.59it/s]
Measuring inference with batch_size=128:   9%|▉         | 9/100 [00:05<00:57,  1.59it/s]
Measuring inference with batch_size=128:  10%|█         | 10/100 [00:06<00:56,  1.60it/s]
Measuring inference with batch_size=128:  11%|█         | 11/100 [00:06<00:55,  1.60it/s]
Measuring inference with batch_size=128:  12%|█▏        | 12/100 [00:07<00:55,  1.58it/s]
Measuring inference with batch_size=128:  13%|█▎        | 13/100 [00:08<00:54,  1.59it/s]
Measuring inference with batch_size=128:  14%|█▍        | 14/100 [00:08<00:53,  1.60it/s]
Measuring inference with batch_size=128:  15%|█▌        | 15/100 [00:09<00:54,  1.57it/s]
Measuring inference with batch_size=128:  16%|█▌        | 16/100 [00:10<00:54,  1.55it/s]
Measuring inference with batch_size=128:  17%|█▋        | 17/100 [00:10<00:53,  1.55it/s]
Measuring inference with batch_size=128:  18%|█▊        | 18/100 [00:11<00:52,  1.57it/s]
Measuring inference with batch_size=128:  19%|█▉        | 19/100 [00:11<00:51,  1.59it/s]
Measuring inference with batch_size=128:  20%|██        | 20/100 [00:12<00:50,  1.57it/s]
Measuring inference with batch_size=128:  21%|██        | 21/100 [00:13<00:51,  1.54it/s]
Measuring inference with batch_size=128:  22%|██▏       | 22/100 [00:13<00:49,  1.57it/s]
Measuring inference with batch_size=128:  23%|██▎       | 23/100 [00:14<00:49,  1.55it/s]
Measuring inference with batch_size=128:  24%|██▍       | 24/100 [00:15<00:48,  1.57it/s]
Measuring inference with batch_size=128:  25%|██▌       | 25/100 [00:15<00:47,  1.58it/s]
Measuring inference with batch_size=128:  26%|██▌       | 26/100 [00:16<00:46,  1.60it/s]
Measuring inference with batch_size=128:  27%|██▋       | 27/100 [00:17<00:45,  1.60it/s]
Measuring inference with batch_size=128:  28%|██▊       | 28/100 [00:17<00:44,  1.61it/s]
Measuring inference with batch_size=128:  29%|██▉       | 29/100 [00:18<00:43,  1.61it/s]
Measuring inference with batch_size=128:  30%|███       | 30/100 [00:18<00:43,  1.62it/s]
Measuring inference with batch_size=128:  31%|███       | 31/100 [00:19<00:42,  1.63it/s]
Measuring inference with batch_size=128:  32%|███▏      | 32/100 [00:20<00:41,  1.63it/s]
Measuring inference with batch_size=128:  33%|███▎      | 33/100 [00:20<00:41,  1.63it/s]
Measuring inference with batch_size=128:  34%|███▍      | 34/100 [00:21<00:40,  1.64it/s]
Measuring inference with batch_size=128:  35%|███▌      | 35/100 [00:21<00:40,  1.60it/s]
Measuring inference with batch_size=128:  36%|███▌      | 36/100 [00:23<00:54,  1.17it/s]
Measuring inference with batch_size=128:  37%|███▋      | 37/100 [00:24<00:49,  1.27it/s]
Measuring inference with batch_size=128:  38%|███▊      | 38/100 [00:24<00:46,  1.33it/s]
Measuring inference with batch_size=128:  39%|███▉      | 39/100 [00:25<00:44,  1.38it/s]
Measuring inference with batch_size=128:  40%|████      | 40/100 [00:26<00:42,  1.40it/s]
Measuring inference with batch_size=128:  41%|████      | 41/100 [00:26<00:42,  1.40it/s]
Measuring inference with batch_size=128:  42%|████▏     | 42/100 [00:27<00:41,  1.41it/s]
Measuring inference with batch_size=128:  43%|████▎     | 43/100 [00:29<00:55,  1.02it/s]
Measuring inference with batch_size=128:  44%|████▍     | 44/100 [00:29<00:49,  1.14it/s]
Measuring inference with batch_size=128:  45%|████▌     | 45/100 [00:30<00:44,  1.23it/s]
Measuring inference with batch_size=128:  46%|████▌     | 46/100 [00:30<00:41,  1.31it/s]
Measuring inference with batch_size=128:  47%|████▋     | 47/100 [00:31<00:38,  1.37it/s]
Measuring inference with batch_size=128:  48%|████▊     | 48/100 [00:32<00:36,  1.41it/s]
Measuring inference with batch_size=128:  49%|████▉     | 49/100 [00:32<00:35,  1.45it/s]
Measuring inference with batch_size=128:  50%|█████     | 50/100 [00:33<00:34,  1.47it/s]
Measuring inference with batch_size=128:  51%|█████     | 51/100 [00:34<00:33,  1.47it/s]
Measuring inference with batch_size=128:  52%|█████▏    | 52/100 [00:34<00:32,  1.48it/s]
Measuring inference with batch_size=128:  53%|█████▎    | 53/100 [00:36<00:42,  1.11it/s]
Measuring inference with batch_size=128:  54%|█████▍    | 54/100 [00:37<00:37,  1.22it/s]
Measuring inference with batch_size=128:  55%|█████▌    | 55/100 [00:37<00:34,  1.30it/s]
Measuring inference with batch_size=128:  56%|█████▌    | 56/100 [00:38<00:32,  1.36it/s]
Measuring inference with batch_size=128:  57%|█████▋    | 57/100 [00:38<00:30,  1.41it/s]
Measuring inference with batch_size=128:  58%|█████▊    | 58/100 [00:39<00:29,  1.45it/s]
Measuring inference with batch_size=128:  59%|█████▉    | 59/100 [00:40<00:28,  1.45it/s]
Measuring inference with batch_size=128:  60%|██████    | 60/100 [00:40<00:27,  1.48it/s]
Measuring inference with batch_size=128:  61%|██████    | 61/100 [00:41<00:26,  1.49it/s]
Measuring inference with batch_size=128:  62%|██████▏   | 62/100 [00:42<00:25,  1.50it/s]
Measuring inference with batch_size=128:  63%|██████▎   | 63/100 [00:42<00:24,  1.50it/s]
Measuring inference with batch_size=128:  64%|██████▍   | 64/100 [00:43<00:23,  1.50it/s]
Measuring inference with batch_size=128:  65%|██████▌   | 65/100 [00:44<00:24,  1.41it/s]
Measuring inference with batch_size=128:  66%|██████▌   | 66/100 [00:45<00:23,  1.42it/s]
Measuring inference with batch_size=128:  67%|██████▋   | 67/100 [00:45<00:22,  1.44it/s]
Measuring inference with batch_size=128:  68%|██████▊   | 68/100 [00:46<00:21,  1.46it/s]
Measuring inference with batch_size=128:  69%|██████▉   | 69/100 [00:48<00:29,  1.04it/s]
Measuring inference with batch_size=128:  70%|███████   | 70/100 [00:48<00:26,  1.14it/s]
Measuring inference with batch_size=128:  71%|███████   | 71/100 [00:49<00:23,  1.24it/s]
Measuring inference with batch_size=128:  72%|███████▏  | 72/100 [00:50<00:21,  1.31it/s]
Measuring inference with batch_size=128:  73%|███████▎  | 73/100 [00:50<00:19,  1.37it/s]
Measuring inference with batch_size=128:  74%|███████▍  | 74/100 [00:51<00:18,  1.41it/s]
Measuring inference with batch_size=128:  75%|███████▌  | 75/100 [00:51<00:17,  1.44it/s]
Measuring inference with batch_size=128:  76%|███████▌  | 76/100 [00:52<00:16,  1.46it/s]
Measuring inference with batch_size=128:  77%|███████▋  | 77/100 [00:53<00:15,  1.47it/s]
Measuring inference with batch_size=128:  78%|███████▊  | 78/100 [00:53<00:14,  1.50it/s]
Measuring inference with batch_size=128:  79%|███████▉  | 79/100 [00:54<00:14,  1.49it/s]
Measuring inference with batch_size=128:  80%|████████  | 80/100 [00:55<00:13,  1.51it/s]
Measuring inference with batch_size=128:  81%|████████  | 81/100 [00:55<00:12,  1.52it/s]
Measuring inference with batch_size=128:  82%|████████▏ | 82/100 [00:56<00:11,  1.53it/s]
Measuring inference with batch_size=128:  83%|████████▎ | 83/100 [00:57<00:11,  1.53it/s]
Measuring inference with batch_size=128:  84%|████████▍ | 84/100 [00:57<00:10,  1.51it/s]
Measuring inference with batch_size=128:  85%|████████▌ | 85/100 [00:58<00:09,  1.52it/s]
Measuring inference with batch_size=128:  86%|████████▌ | 86/100 [00:59<00:09,  1.51it/s]
Measuring inference with batch_size=128:  87%|████████▋ | 87/100 [00:59<00:08,  1.51it/s]
Measuring inference with batch_size=128:  88%|████████▊ | 88/100 [01:00<00:07,  1.52it/s]
Measuring inference with batch_size=128:  89%|████████▉ | 89/100 [01:01<00:07,  1.52it/s]
Measuring inference with batch_size=128:  90%|█████████ | 90/100 [01:01<00:06,  1.53it/s]
Measuring inference with batch_size=128:  91%|█████████ | 91/100 [01:02<00:05,  1.53it/s]
Measuring inference with batch_size=128:  92%|█████████▏| 92/100 [01:03<00:07,  1.13it/s]
Measuring inference with batch_size=128:  93%|█████████▎| 93/100 [01:04<00:05,  1.23it/s]
Measuring inference with batch_size=128:  94%|█████████▍| 94/100 [01:05<00:04,  1.28it/s]
Measuring inference with batch_size=128:  95%|█████████▌| 95/100 [01:05<00:03,  1.34it/s]
Measuring inference with batch_size=128:  96%|█████████▌| 96/100 [01:06<00:02,  1.39it/s]
Measuring inference with batch_size=128:  97%|█████████▋| 97/100 [01:07<00:02,  1.43it/s]
Measuring inference with batch_size=128:  98%|█████████▊| 98/100 [01:07<00:01,  1.43it/s]
Measuring inference with batch_size=128:  99%|█████████▉| 99/100 [01:08<00:00,  1.46it/s]
Measuring inference with batch_size=128: 100%|██████████| 100/100 [01:09<00:00,  1.49it/s]
Measuring inference with batch_size=128: 100%|██████████| 100/100 [01:09<00:00,  1.44it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
INFO:benchmark:learner.infer:
  device: cuda
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 52
        total: 104
      frequency: 2.10 GHz
      model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
    gpus:
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    memory:
      available: 159.41 GB
      total: 187.56 GB
      used: 26.48 GB
    system:
      node: charybdis
      release: 4.15.0-167-generic
      system: Linux
  max_inference_memory: 7554420736
  post_inference_memory: 7505677824
  pre_inference_memory: 15445504
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "2.706 \xB5s +/- 0.419 \xB5s [1.907 \xB5s, 4.053 \xB5s]"
          batches_per_second: 378.00 K +/- 56.15 K [246.72 K, 524.29 K]
        metrics:
          batches_per_second_max: 524288.0
          batches_per_second_mean: 377996.9175587811
          batches_per_second_min: 246723.76470588235
          batches_per_second_std: 56147.686414104595
          seconds_per_batch_max: 4.0531158447265625e-06
          seconds_per_batch_mean: 2.7060508728027343e-06
          seconds_per_batch_min: 1.9073486328125e-06
          seconds_per_batch_std: 4.189320419026751e-07
      gpu_to_cpu:
        human_readable:
          batch_latency: "79.725 \xB5s +/- 8.450 \xB5s [70.572 \xB5s, 112.534 \xB5\
            s]"
          batches_per_second: 12.67 K +/- 1.18 K [8.89 K, 14.17 K]
        metrics:
          batches_per_second_max: 14169.945945945947
          batches_per_second_mean: 12667.237156376848
          batches_per_second_min: 8886.237288135593
          batches_per_second_std: 1177.7148949778828
          seconds_per_batch_max: 0.0001125335693359375
          seconds_per_batch_mean: 7.972478866577148e-05
          seconds_per_batch_min: 7.05718994140625e-05
          seconds_per_batch_std: 8.449636180710533e-06
      on_device_inference:
        human_readable:
          batch_latency: 124.541 ms +/- 14.044 ms [112.133 ms, 172.671 ms]
          batches_per_second: 8.12 +/- 0.77 [5.79, 8.92]
        metrics:
          batches_per_second_max: 8.91799820972574
          batches_per_second_mean: 8.115351198630936
          batches_per_second_min: 5.791357777516966
          batches_per_second_std: 0.7656384077452133
          seconds_per_batch_max: 0.17267107963562012
          seconds_per_batch_mean: 0.12454111099243165
          seconds_per_batch_min: 0.11213278770446777
          seconds_per_batch_std: 0.0140438826756024
      total:
        human_readable:
          batch_latency: 124.624 ms +/- 14.048 ms [112.211 ms, 172.761 ms]
          batches_per_second: 8.11 +/- 0.76 [5.79, 8.91]
        metrics:
          batches_per_second_max: 8.911745270891895
          batches_per_second_mean: 8.109919516947311
          batches_per_second_min: 5.788336670747006
          batches_per_second_std: 0.7648586815868101
          seconds_per_batch_max: 0.1727612018585205
          seconds_per_batch_mean: 0.12462354183197022
          seconds_per_batch_min: 0.11221146583557129
          seconds_per_batch_std: 0.014047691255336205
    batch_size_128:
      cpu_to_gpu:
        human_readable:
          batch_latency: "3.283 \xB5s +/- 0.623 \xB5s [2.384 \xB5s, 6.199 \xB5s]"
          batches_per_second: 313.67 K +/- 49.74 K [161.32 K, 419.43 K]
        metrics:
          batches_per_second_max: 419430.4
          batches_per_second_mean: 313673.18438642053
          batches_per_second_min: 161319.38461538462
          batches_per_second_std: 49744.6232129401
          seconds_per_batch_max: 6.198883056640625e-06
          seconds_per_batch_mean: 3.2830238342285155e-06
          seconds_per_batch_min: 2.384185791015625e-06
          seconds_per_batch_std: 6.234132971738978e-07
      gpu_to_cpu:
        human_readable:
          batch_latency: "2.898 ms +/- 381.882 \xB5s [2.456 ms, 4.496 ms]"
          batches_per_second: 349.83 +/- 36.73 [222.44, 407.10]
        metrics:
          batches_per_second_max: 407.09540910414444
          batches_per_second_mean: 349.82807715332706
          batches_per_second_min: 222.4386932541366
          batches_per_second_std: 36.730746156918066
          seconds_per_batch_max: 0.0044956207275390625
          seconds_per_batch_mean: 0.002898101806640625
          seconds_per_batch_min: 0.0024564266204833984
          seconds_per_batch_std: 0.0003818823333553559
      on_device_inference:
        human_readable:
          batch_latency: 689.241 ms +/- 185.734 ms [603.859 ms, 1.594 s]
          batches_per_second: 1.50 +/- 0.20 [0.63, 1.66]
        metrics:
          batches_per_second_max: 1.6560152149682819
          batches_per_second_mean: 1.5040262835611755
          batches_per_second_min: 0.6274980098504362
          batches_per_second_std: 0.2005531185446535
          seconds_per_batch_max: 1.5936305522918701
          seconds_per_batch_mean: 0.6892413973808289
          seconds_per_batch_min: 0.6038591861724854
          seconds_per_batch_std: 0.18573440600218086
      total:
        human_readable:
          batch_latency: 692.143 ms +/- 185.839 ms [607.186 ms, 1.596 s]
          batches_per_second: 1.50 +/- 0.20 [0.63, 1.65]
        metrics:
          batches_per_second_max: 1.646940932743399
          batches_per_second_mean: 1.4974278321313594
          batches_per_second_min: 0.6263921055822814
          batches_per_second_std: 0.19931525935243047
          seconds_per_batch_max: 1.5964441299438477
          seconds_per_batch_mean: 0.6921427822113038
          seconds_per_batch_min: 0.6071863174438477
          seconds_per_batch_std: 0.18583866112801062

INFO:benchmark:== Benchmarking model directly ==

Warming up with batch_size=128:   0%|          | 0/10 [00:00<?, ?it/s]
Warming up with batch_size=128:  10%|█         | 1/10 [00:00<00:01,  8.25it/s]
Warming up with batch_size=128:  20%|██        | 2/10 [00:00<00:00,  8.59it/s]
Warming up with batch_size=128:  30%|███       | 3/10 [00:00<00:00,  8.76it/s]
Warming up with batch_size=128:  40%|████      | 4/10 [00:00<00:00,  8.87it/s]
Warming up with batch_size=128:  50%|█████     | 5/10 [00:00<00:00,  6.94it/s]
Warming up with batch_size=128:  60%|██████    | 6/10 [00:00<00:00,  7.53it/s]
Warming up with batch_size=128:  70%|███████   | 7/10 [00:00<00:00,  7.93it/s]
Warming up with batch_size=128:  80%|████████  | 8/10 [00:00<00:00,  8.21it/s]
Warming up with batch_size=128:  90%|█████████ | 9/10 [00:01<00:00,  8.39it/s]
Warming up with batch_size=128: 100%|██████████| 10/10 [00:01<00:00,  8.59it/s]
Warming up with batch_size=128: 100%|██████████| 10/10 [00:01<00:00,  8.23it/s]

Measuring inference with batch_size=128:   0%|          | 0/100 [00:00<?, ?it/s]
Measuring inference with batch_size=128:   1%|          | 1/100 [00:00<00:11,  8.97it/s]
Measuring inference with batch_size=128:   2%|▏         | 2/100 [00:00<00:10,  8.92it/s]
Measuring inference with batch_size=128:   3%|▎         | 3/100 [00:00<00:10,  8.94it/s]
Measuring inference with batch_size=128:   4%|▍         | 4/100 [00:00<00:10,  8.89it/s]
Measuring inference with batch_size=128:   5%|▌         | 5/100 [00:00<00:10,  8.86it/s]
Measuring inference with batch_size=128:   6%|▌         | 6/100 [00:00<00:10,  8.86it/s]
Measuring inference with batch_size=128:   7%|▋         | 7/100 [00:00<00:10,  8.85it/s]
Measuring inference with batch_size=128:   8%|▊         | 8/100 [00:00<00:10,  8.83it/s]
Measuring inference with batch_size=128:   9%|▉         | 9/100 [00:01<00:10,  8.85it/s]
Measuring inference with batch_size=128:  10%|█         | 10/100 [00:01<00:10,  8.89it/s]
Measuring inference with batch_size=128:  11%|█         | 11/100 [00:01<00:10,  8.85it/s]
Measuring inference with batch_size=128:  12%|█▏        | 12/100 [00:01<00:09,  8.83it/s]
Measuring inference with batch_size=128:  13%|█▎        | 13/100 [00:01<00:09,  8.82it/s]
Measuring inference with batch_size=128:  14%|█▍        | 14/100 [00:01<00:09,  8.80it/s]
Measuring inference with batch_size=128:  15%|█▌        | 15/100 [00:01<00:09,  8.80it/s]
Measuring inference with batch_size=128:  16%|█▌        | 16/100 [00:01<00:09,  8.82it/s]
Measuring inference with batch_size=128:  17%|█▋        | 17/100 [00:01<00:09,  8.82it/s]
Measuring inference with batch_size=128:  18%|█▊        | 18/100 [00:02<00:09,  8.77it/s]
Measuring inference with batch_size=128:  19%|█▉        | 19/100 [00:02<00:09,  8.78it/s]
Measuring inference with batch_size=128:  20%|██        | 20/100 [00:02<00:09,  8.77it/s]
Measuring inference with batch_size=128:  21%|██        | 21/100 [00:02<00:09,  8.77it/s]
Measuring inference with batch_size=128:  22%|██▏       | 22/100 [00:02<00:08,  8.83it/s]
Measuring inference with batch_size=128:  23%|██▎       | 23/100 [00:02<00:08,  8.82it/s]
Measuring inference with batch_size=128:  24%|██▍       | 24/100 [00:02<00:08,  8.79it/s]
Measuring inference with batch_size=128:  25%|██▌       | 25/100 [00:02<00:08,  8.77it/s]
Measuring inference with batch_size=128:  26%|██▌       | 26/100 [00:02<00:08,  8.78it/s]
Measuring inference with batch_size=128:  27%|██▋       | 27/100 [00:03<00:08,  8.80it/s]
Measuring inference with batch_size=128:  28%|██▊       | 28/100 [00:03<00:08,  8.84it/s]
Measuring inference with batch_size=128:  29%|██▉       | 29/100 [00:03<00:08,  8.84it/s]
Measuring inference with batch_size=128:  30%|███       | 30/100 [00:03<00:07,  8.84it/s]
Measuring inference with batch_size=128:  31%|███       | 31/100 [00:03<00:07,  8.87it/s]
Measuring inference with batch_size=128:  32%|███▏      | 32/100 [00:03<00:07,  8.90it/s]
Measuring inference with batch_size=128:  33%|███▎      | 33/100 [00:03<00:07,  8.93it/s]
Measuring inference with batch_size=128:  34%|███▍      | 34/100 [00:03<00:07,  8.78it/s]
Measuring inference with batch_size=128:  35%|███▌      | 35/100 [00:03<00:08,  8.09it/s]
Measuring inference with batch_size=128:  36%|███▌      | 36/100 [00:04<00:08,  7.68it/s]
Measuring inference with batch_size=128:  37%|███▋      | 37/100 [00:04<00:08,  7.41it/s]
Measuring inference with batch_size=128:  38%|███▊      | 38/100 [00:04<00:08,  7.22it/s]
Measuring inference with batch_size=128:  39%|███▉      | 39/100 [00:04<00:08,  7.50it/s]
Measuring inference with batch_size=128:  40%|████      | 40/100 [00:04<00:07,  7.88it/s]
Measuring inference with batch_size=128:  41%|████      | 41/100 [00:04<00:07,  8.14it/s]
Measuring inference with batch_size=128:  42%|████▏     | 42/100 [00:04<00:07,  8.28it/s]
Measuring inference with batch_size=128:  43%|████▎     | 43/100 [00:05<00:06,  8.44it/s]
Measuring inference with batch_size=128:  44%|████▍     | 44/100 [00:05<00:06,  8.47it/s]
Measuring inference with batch_size=128:  45%|████▌     | 45/100 [00:05<00:06,  8.40it/s]
Measuring inference with batch_size=128:  46%|████▌     | 46/100 [00:05<00:06,  8.45it/s]
Measuring inference with batch_size=128:  47%|████▋     | 47/100 [00:05<00:06,  8.50it/s]
Measuring inference with batch_size=128:  48%|████▊     | 48/100 [00:05<00:06,  8.54it/s]
Measuring inference with batch_size=128:  49%|████▉     | 49/100 [00:05<00:05,  8.56it/s]
Measuring inference with batch_size=128:  50%|█████     | 50/100 [00:05<00:05,  8.56it/s]
Measuring inference with batch_size=128:  51%|█████     | 51/100 [00:05<00:05,  8.55it/s]
Measuring inference with batch_size=128:  52%|█████▏    | 52/100 [00:06<00:05,  8.52it/s]
Measuring inference with batch_size=128:  53%|█████▎    | 53/100 [00:06<00:05,  8.52it/s]
Measuring inference with batch_size=128:  54%|█████▍    | 54/100 [00:06<00:05,  8.56it/s]
Measuring inference with batch_size=128:  55%|█████▌    | 55/100 [00:06<00:05,  8.58it/s]
Measuring inference with batch_size=128:  56%|█████▌    | 56/100 [00:06<00:05,  8.57it/s]
Measuring inference with batch_size=128:  57%|█████▋    | 57/100 [00:06<00:05,  8.57it/s]
Measuring inference with batch_size=128:  58%|█████▊    | 58/100 [00:06<00:04,  8.60it/s]
Measuring inference with batch_size=128:  59%|█████▉    | 59/100 [00:06<00:04,  8.59it/s]
Measuring inference with batch_size=128:  60%|██████    | 60/100 [00:06<00:04,  8.60it/s]
Measuring inference with batch_size=128:  61%|██████    | 61/100 [00:07<00:04,  8.57it/s]
Measuring inference with batch_size=128:  62%|██████▏   | 62/100 [00:07<00:04,  8.57it/s]
Measuring inference with batch_size=128:  63%|██████▎   | 63/100 [00:07<00:04,  8.54it/s]
Measuring inference with batch_size=128:  64%|██████▍   | 64/100 [00:07<00:04,  8.48it/s]
Measuring inference with batch_size=128:  65%|██████▌   | 65/100 [00:07<00:04,  8.49it/s]
Measuring inference with batch_size=128:  66%|██████▌   | 66/100 [00:07<00:03,  8.54it/s]
Measuring inference with batch_size=128:  67%|██████▋   | 67/100 [00:07<00:03,  8.57it/s]
Measuring inference with batch_size=128:  68%|██████▊   | 68/100 [00:07<00:03,  8.67it/s]
Measuring inference with batch_size=128:  69%|██████▉   | 69/100 [00:08<00:03,  8.70it/s]
Measuring inference with batch_size=128:  70%|███████   | 70/100 [00:08<00:03,  8.71it/s]
Measuring inference with batch_size=128:  71%|███████   | 71/100 [00:08<00:03,  8.73it/s]
Measuring inference with batch_size=128:  72%|███████▏  | 72/100 [00:08<00:03,  8.77it/s]
Measuring inference with batch_size=128:  73%|███████▎  | 73/100 [00:08<00:03,  8.76it/s]
Measuring inference with batch_size=128:  74%|███████▍  | 74/100 [00:08<00:02,  8.82it/s]
Measuring inference with batch_size=128:  75%|███████▌  | 75/100 [00:08<00:02,  8.80it/s]
Measuring inference with batch_size=128:  76%|███████▌  | 76/100 [00:08<00:02,  8.81it/s]
Measuring inference with batch_size=128:  77%|███████▋  | 77/100 [00:08<00:02,  8.82it/s]
Measuring inference with batch_size=128:  78%|███████▊  | 78/100 [00:09<00:02,  8.81it/s]
Measuring inference with batch_size=128:  79%|███████▉  | 79/100 [00:09<00:02,  8.83it/s]
Measuring inference with batch_size=128:  80%|████████  | 80/100 [00:09<00:02,  8.82it/s]
Measuring inference with batch_size=128:  81%|████████  | 81/100 [00:09<00:02,  8.81it/s]
Measuring inference with batch_size=128:  82%|████████▏ | 82/100 [00:09<00:02,  8.79it/s]
Measuring inference with batch_size=128:  83%|████████▎ | 83/100 [00:09<00:01,  8.83it/s]
Measuring inference with batch_size=128:  84%|████████▍ | 84/100 [00:09<00:01,  8.83it/s]
Measuring inference with batch_size=128:  85%|████████▌ | 85/100 [00:09<00:01,  8.81it/s]
Measuring inference with batch_size=128:  86%|████████▌ | 86/100 [00:09<00:01,  8.80it/s]
Measuring inference with batch_size=128:  87%|████████▋ | 87/100 [00:10<00:01,  8.80it/s]
Measuring inference with batch_size=128:  88%|████████▊ | 88/100 [00:10<00:01,  8.83it/s]
Measuring inference with batch_size=128:  89%|████████▉ | 89/100 [00:10<00:01,  8.83it/s]
Measuring inference with batch_size=128:  90%|█████████ | 90/100 [00:10<00:01,  8.81it/s]
Measuring inference with batch_size=128:  91%|█████████ | 91/100 [00:10<00:01,  8.75it/s]
Measuring inference with batch_size=128:  92%|█████████▏| 92/100 [00:10<00:00,  8.67it/s]
Measuring inference with batch_size=128:  93%|█████████▎| 93/100 [00:10<00:00,  8.62it/s]
Measuring inference with batch_size=128:  94%|█████████▍| 94/100 [00:10<00:00,  8.59it/s]
Measuring inference with batch_size=128:  95%|█████████▌| 95/100 [00:11<00:00,  8.56it/s]
Measuring inference with batch_size=128:  96%|█████████▌| 96/100 [00:11<00:00,  8.43it/s]
Measuring inference with batch_size=128:  97%|█████████▋| 97/100 [00:11<00:00,  8.40it/s]
Measuring inference with batch_size=128:  98%|█████████▊| 98/100 [00:11<00:00,  8.42it/s]
Measuring inference with batch_size=128:  99%|█████████▉| 99/100 [00:11<00:00,  8.44it/s]
Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:11<00:00,  8.33it/s]
Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:11<00:00,  8.61it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.

Warming up with batch_size=128:   0%|          | 0/10 [00:00<?, ?it/s]
Warming up with batch_size=128:  10%|█         | 1/10 [00:00<00:05,  1.51it/s]
Warming up with batch_size=128:  20%|██        | 2/10 [00:01<00:05,  1.56it/s]
Warming up with batch_size=128:  30%|███       | 3/10 [00:01<00:04,  1.58it/s]
Warming up with batch_size=128:  40%|████      | 4/10 [00:02<00:03,  1.58it/s]
Warming up with batch_size=128:  50%|█████     | 5/10 [00:03<00:03,  1.59it/s]
Warming up with batch_size=128:  60%|██████    | 6/10 [00:03<00:02,  1.59it/s]
Warming up with batch_size=128:  70%|███████   | 7/10 [00:04<00:01,  1.59it/s]
Warming up with batch_size=128:  80%|████████  | 8/10 [00:05<00:01,  1.59it/s]
Warming up with batch_size=128:  90%|█████████ | 9/10 [00:05<00:00,  1.59it/s]
Warming up with batch_size=128: 100%|██████████| 10/10 [00:06<00:00,  1.59it/s]
Warming up with batch_size=128: 100%|██████████| 10/10 [00:06<00:00,  1.59it/s]

Measuring inference with batch_size=128:   0%|          | 0/100 [00:00<?, ?it/s]
Measuring inference with batch_size=128:   1%|          | 1/100 [00:00<01:02,  1.59it/s]
Measuring inference with batch_size=128:   2%|▏         | 2/100 [00:01<01:01,  1.60it/s]
Measuring inference with batch_size=128:   3%|▎         | 3/100 [00:01<01:00,  1.59it/s]
Measuring inference with batch_size=128:   4%|▍         | 4/100 [00:02<01:00,  1.59it/s]
Measuring inference with batch_size=128:   5%|▌         | 5/100 [00:03<00:59,  1.60it/s]
Measuring inference with batch_size=128:   6%|▌         | 6/100 [00:03<00:58,  1.60it/s]
Measuring inference with batch_size=128:   7%|▋         | 7/100 [00:04<00:58,  1.59it/s]
Measuring inference with batch_size=128:   8%|▊         | 8/100 [00:05<00:57,  1.60it/s]
Measuring inference with batch_size=128:   9%|▉         | 9/100 [00:05<00:57,  1.59it/s]
Measuring inference with batch_size=128:  10%|█         | 10/100 [00:06<00:56,  1.59it/s]
Measuring inference with batch_size=128:  11%|█         | 11/100 [00:06<00:55,  1.59it/s]
Measuring inference with batch_size=128:  12%|█▏        | 12/100 [00:07<00:55,  1.59it/s]
Measuring inference with batch_size=128:  13%|█▎        | 13/100 [00:08<00:54,  1.59it/s]
Measuring inference with batch_size=128:  14%|█▍        | 14/100 [00:08<00:53,  1.59it/s]
Measuring inference with batch_size=128:  15%|█▌        | 15/100 [00:09<00:53,  1.60it/s]
Measuring inference with batch_size=128:  16%|█▌        | 16/100 [00:10<00:52,  1.59it/s]
Measuring inference with batch_size=128:  17%|█▋        | 17/100 [00:10<00:52,  1.59it/s]
Measuring inference with batch_size=128:  18%|█▊        | 18/100 [00:11<00:51,  1.59it/s]
Measuring inference with batch_size=128:  19%|█▉        | 19/100 [00:11<00:50,  1.59it/s]
Measuring inference with batch_size=128:  20%|██        | 20/100 [00:12<00:50,  1.59it/s]
Measuring inference with batch_size=128:  21%|██        | 21/100 [00:13<00:49,  1.59it/s]
Measuring inference with batch_size=128:  22%|██▏       | 22/100 [00:13<00:48,  1.59it/s]
Measuring inference with batch_size=128:  23%|██▎       | 23/100 [00:14<00:48,  1.59it/s]
Measuring inference with batch_size=128:  24%|██▍       | 24/100 [00:15<00:47,  1.59it/s]
Measuring inference with batch_size=128:  25%|██▌       | 25/100 [00:15<00:47,  1.59it/s]
Measuring inference with batch_size=128:  26%|██▌       | 26/100 [00:16<00:46,  1.59it/s]
Measuring inference with batch_size=128:  27%|██▋       | 27/100 [00:16<00:45,  1.59it/s]
Measuring inference with batch_size=128:  28%|██▊       | 28/100 [00:17<00:45,  1.59it/s]
Measuring inference with batch_size=128:  29%|██▉       | 29/100 [00:18<00:44,  1.59it/s]
Measuring inference with batch_size=128:  30%|███       | 30/100 [00:18<00:43,  1.59it/s]
Measuring inference with batch_size=128:  31%|███       | 31/100 [00:19<00:43,  1.59it/s]
Measuring inference with batch_size=128:  32%|███▏      | 32/100 [00:20<00:42,  1.59it/s]
Measuring inference with batch_size=128:  33%|███▎      | 33/100 [00:20<00:41,  1.60it/s]
Measuring inference with batch_size=128:  34%|███▍      | 34/100 [00:21<00:41,  1.60it/s]
Measuring inference with batch_size=128:  35%|███▌      | 35/100 [00:21<00:40,  1.60it/s]
Measuring inference with batch_size=128:  36%|███▌      | 36/100 [00:22<00:40,  1.60it/s]
Measuring inference with batch_size=128:  37%|███▋      | 37/100 [00:23<00:39,  1.60it/s]
Measuring inference with batch_size=128:  38%|███▊      | 38/100 [00:23<00:38,  1.60it/s]
Measuring inference with batch_size=128:  39%|███▉      | 39/100 [00:24<00:38,  1.59it/s]
Measuring inference with batch_size=128:  40%|████      | 40/100 [00:25<00:37,  1.60it/s]
Measuring inference with batch_size=128:  41%|████      | 41/100 [00:25<00:36,  1.60it/s]
Measuring inference with batch_size=128:  42%|████▏     | 42/100 [00:26<00:36,  1.60it/s]
Measuring inference with batch_size=128:  43%|████▎     | 43/100 [00:26<00:35,  1.60it/s]
Measuring inference with batch_size=128:  44%|████▍     | 44/100 [00:27<00:35,  1.59it/s]
Measuring inference with batch_size=128:  45%|████▌     | 45/100 [00:28<00:34,  1.59it/s]
Measuring inference with batch_size=128:  46%|████▌     | 46/100 [00:28<00:33,  1.59it/s]
Measuring inference with batch_size=128:  47%|████▋     | 47/100 [00:29<00:33,  1.59it/s]
Measuring inference with batch_size=128:  48%|████▊     | 48/100 [00:30<00:32,  1.59it/s]
Measuring inference with batch_size=128:  49%|████▉     | 49/100 [00:30<00:32,  1.59it/s]
Measuring inference with batch_size=128:  50%|█████     | 50/100 [00:31<00:31,  1.59it/s]
Measuring inference with batch_size=128:  51%|█████     | 51/100 [00:31<00:30,  1.59it/s]
Measuring inference with batch_size=128:  52%|█████▏    | 52/100 [00:32<00:30,  1.59it/s]
Measuring inference with batch_size=128:  53%|█████▎    | 53/100 [00:33<00:29,  1.59it/s]
Measuring inference with batch_size=128:  54%|█████▍    | 54/100 [00:33<00:28,  1.63it/s]
Measuring inference with batch_size=128:  55%|█████▌    | 55/100 [00:34<00:27,  1.65it/s]
Measuring inference with batch_size=128:  56%|█████▌    | 56/100 [00:35<00:26,  1.67it/s]
Measuring inference with batch_size=128:  57%|█████▋    | 57/100 [00:35<00:26,  1.65it/s]
Measuring inference with batch_size=128:  58%|█████▊    | 58/100 [00:36<00:25,  1.65it/s]
Measuring inference with batch_size=128:  59%|█████▉    | 59/100 [00:36<00:24,  1.67it/s]
Measuring inference with batch_size=128:  60%|██████    | 60/100 [00:37<00:23,  1.68it/s]
Measuring inference with batch_size=128:  61%|██████    | 61/100 [00:37<00:23,  1.69it/s]
Measuring inference with batch_size=128:  62%|██████▏   | 62/100 [00:38<00:22,  1.69it/s]
Measuring inference with batch_size=128:  63%|██████▎   | 63/100 [00:39<00:21,  1.70it/s]
Measuring inference with batch_size=128:  64%|██████▍   | 64/100 [00:39<00:21,  1.70it/s]
Measuring inference with batch_size=128:  65%|██████▌   | 65/100 [00:40<00:20,  1.70it/s]
Measuring inference with batch_size=128:  66%|██████▌   | 66/100 [00:40<00:19,  1.70it/s]
Measuring inference with batch_size=128:  67%|██████▋   | 67/100 [00:41<00:19,  1.70it/s]
Measuring inference with batch_size=128:  68%|██████▊   | 68/100 [00:42<00:18,  1.70it/s]
Measuring inference with batch_size=128:  69%|██████▉   | 69/100 [00:42<00:18,  1.71it/s]
Measuring inference with batch_size=128:  70%|███████   | 70/100 [00:43<00:17,  1.70it/s]
Measuring inference with batch_size=128:  71%|███████   | 71/100 [00:43<00:17,  1.71it/s]
Measuring inference with batch_size=128:  72%|███████▏  | 72/100 [00:44<00:16,  1.71it/s]
Measuring inference with batch_size=128:  73%|███████▎  | 73/100 [00:45<00:15,  1.71it/s]
Measuring inference with batch_size=128:  74%|███████▍  | 74/100 [00:45<00:15,  1.71it/s]
Measuring inference with batch_size=128:  75%|███████▌  | 75/100 [00:46<00:14,  1.71it/s]
Measuring inference with batch_size=128:  76%|███████▌  | 76/100 [00:46<00:14,  1.71it/s]
Measuring inference with batch_size=128:  77%|███████▋  | 77/100 [00:47<00:13,  1.70it/s]
Measuring inference with batch_size=128:  78%|███████▊  | 78/100 [00:47<00:12,  1.70it/s]
Measuring inference with batch_size=128:  79%|███████▉  | 79/100 [00:48<00:12,  1.70it/s]
Measuring inference with batch_size=128:  80%|████████  | 80/100 [00:49<00:11,  1.70it/s]
Measuring inference with batch_size=128:  81%|████████  | 81/100 [00:49<00:11,  1.70it/s]
Measuring inference with batch_size=128:  82%|████████▏ | 82/100 [00:50<00:10,  1.70it/s]
Measuring inference with batch_size=128:  83%|████████▎ | 83/100 [00:50<00:09,  1.70it/s]
Measuring inference with batch_size=128:  84%|████████▍ | 84/100 [00:51<00:09,  1.70it/s]
Measuring inference with batch_size=128:  85%|████████▌ | 85/100 [00:52<00:08,  1.67it/s]
Measuring inference with batch_size=128:  86%|████████▌ | 86/100 [00:52<00:08,  1.65it/s]
Measuring inference with batch_size=128:  87%|████████▋ | 87/100 [00:53<00:07,  1.64it/s]
Measuring inference with batch_size=128:  88%|████████▊ | 88/100 [00:53<00:07,  1.64it/s]
Measuring inference with batch_size=128:  89%|████████▉ | 89/100 [00:54<00:06,  1.64it/s]
Measuring inference with batch_size=128:  90%|█████████ | 90/100 [00:55<00:06,  1.64it/s]
Measuring inference with batch_size=128:  91%|█████████ | 91/100 [00:55<00:05,  1.65it/s]
Measuring inference with batch_size=128:  92%|█████████▏| 92/100 [00:56<00:04,  1.65it/s]
Measuring inference with batch_size=128:  93%|█████████▎| 93/100 [00:57<00:04,  1.64it/s]
Measuring inference with batch_size=128:  94%|█████████▍| 94/100 [00:57<00:03,  1.64it/s]
Measuring inference with batch_size=128:  95%|█████████▌| 95/100 [00:58<00:03,  1.63it/s]
Measuring inference with batch_size=128:  96%|█████████▌| 96/100 [00:58<00:02,  1.62it/s]
Measuring inference with batch_size=128:  97%|█████████▋| 97/100 [00:59<00:01,  1.61it/s]
Measuring inference with batch_size=128:  98%|█████████▊| 98/100 [01:00<00:01,  1.61it/s]
Measuring inference with batch_size=128:  99%|█████████▉| 99/100 [01:00<00:00,  1.60it/s]
Measuring inference with batch_size=128: 100%|██████████| 100/100 [01:01<00:00,  1.60it/s]
Measuring inference with batch_size=128: 100%|██████████| 100/100 [01:01<00:00,  1.63it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
INFO:benchmark:learner.model.forward:
  device: cuda
  flops: 138228272
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 52
        total: 104
      frequency: 2.10 GHz
      model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
    gpus:
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    memory:
      available: 156.81 GB
      total: 187.56 GB
      used: 29.07 GB
    system:
      node: charybdis
      release: 4.15.0-167-generic
      system: Linux
  max_inference_memory: 7563021824
  params: 3794322
  post_inference_memory: 7513394688
  pre_inference_memory: 73787904
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "125.034 \xB5s +/- 13.688 \xB5s [111.818 \xB5s, 223.637 \xB5\
            s]"
          batches_per_second: 8.07 K +/- 645.81 [4.47 K, 8.94 K]
        metrics:
          batches_per_second_max: 8943.078891257996
          batches_per_second_mean: 8066.70711033075
          batches_per_second_min: 4471.539445628998
          batches_per_second_std: 645.8054121001605
          seconds_per_batch_max: 0.00022363662719726562
          seconds_per_batch_mean: 0.0001250338554382324
          seconds_per_batch_min: 0.00011181831359863281
          seconds_per_batch_std: 1.3688434954088712e-05
      gpu_to_cpu:
        human_readable:
          batch_latency: "61.245 \xB5s +/- 4.781 \xB5s [52.452 \xB5s, 82.016 \xB5\
            s]"
          batches_per_second: 16.42 K +/- 1.16 K [12.19 K, 19.07 K]
        metrics:
          batches_per_second_max: 19065.01818181818
          batches_per_second_mean: 16417.994652322977
          batches_per_second_min: 12192.744186046511
          batches_per_second_std: 1163.1536294848177
          seconds_per_batch_max: 8.20159912109375e-05
          seconds_per_batch_mean: 6.124496459960937e-05
          seconds_per_batch_min: 5.245208740234375e-05
          seconds_per_batch_std: 4.781024358383226e-06
      on_device_inference:
        human_readable:
          batch_latency: 115.768 ms +/- 6.614 ms [110.383 ms, 146.639 ms]
          batches_per_second: 8.66 +/- 0.41 [6.82, 9.06]
        metrics:
          batches_per_second_max: 9.0593632554322
          batches_per_second_mean: 8.661449868632921
          batches_per_second_min: 6.819474252416072
          batches_per_second_std: 0.4124811468889861
          seconds_per_batch_max: 0.1466388702392578
          seconds_per_batch_mean: 0.11576798439025879
          seconds_per_batch_min: 0.1103830337524414
          seconds_per_batch_std: 0.006613776115910474
      total:
        human_readable:
          batch_latency: 115.954 ms +/- 6.619 ms [110.561 ms, 146.834 ms]
          batches_per_second: 8.65 +/- 0.41 [6.81, 9.04]
        metrics:
          batches_per_second_max: 9.044750370907083
          batches_per_second_mean: 8.647504321393157
          batches_per_second_min: 6.810394435171173
          batches_per_second_std: 0.4116111734363795
          seconds_per_batch_max: 0.1468343734741211
          seconds_per_batch_mean: 0.11595426321029663
          seconds_per_batch_min: 0.11056137084960938
          seconds_per_batch_std: 0.006618973751974307
    batch_size_128:
      cpu_to_gpu:
        human_readable:
          batch_latency: 6.032 ms +/- 1.249 ms [4.813 ms, 16.861 ms]
          batches_per_second: 169.68 +/- 21.75 [59.31, 207.76]
        metrics:
          batches_per_second_max: 207.7622349910838
          batches_per_second_mean: 169.68115631079763
          batches_per_second_min: 59.308597285067876
          batches_per_second_std: 21.75055150010724
          seconds_per_batch_max: 0.0168609619140625
          seconds_per_batch_mean: 0.006031897068023682
          seconds_per_batch_min: 0.004813194274902344
          seconds_per_batch_std: 0.001249068544647338
      gpu_to_cpu:
        human_readable:
          batch_latency: "26.604 ms +/- 9.163 ms [194.311 \xB5s, 35.420 ms]"
          batches_per_second: 101.66 +/- 510.93 [28.23, 5.15 K]
        metrics:
          batches_per_second_max: 5146.38527607362
          batches_per_second_mean: 101.65792216034635
          batches_per_second_min: 28.232683997253673
          batches_per_second_std: 510.9315211222515
          seconds_per_batch_max: 0.03541994094848633
          seconds_per_batch_mean: 0.026603851318359375
          seconds_per_batch_min: 0.00019431114196777344
          seconds_per_batch_std: 0.009162910424642451
      on_device_inference:
        human_readable:
          batch_latency: 580.911 ms +/- 10.361 ms [561.369 ms, 592.905 ms]
          batches_per_second: 1.72 +/- 0.03 [1.69, 1.78]
        metrics:
          batches_per_second_max: 1.7813581680437145
          batches_per_second_mean: 1.721989948245981
          batches_per_second_min: 1.6866100348838378
          batches_per_second_std: 0.03122301732633047
          seconds_per_batch_max: 0.5929052829742432
          seconds_per_batch_mean: 0.5809113454818725
          seconds_per_batch_min: 0.5613694190979004
          seconds_per_batch_std: 0.010361358656650667
      total:
        human_readable:
          batch_latency: 613.547 ms +/- 18.375 ms [584.290 ms, 631.588 ms]
          batches_per_second: 1.63 +/- 0.05 [1.58, 1.71]
        metrics:
          batches_per_second_max: 1.7114781096433618
          batches_per_second_mean: 1.6313616954491992
          batches_per_second_min: 1.583310086207515
          batches_per_second_std: 0.04992080251218949
          seconds_per_batch_max: 0.6315882205963135
          seconds_per_batch_mean: 0.6135470938682556
          seconds_per_batch_min: 0.5842902660369873
          seconds_per_batch_std: 0.018374735086809476

INFO:benchmark:==== Benchmarking CoX3DLearner (s) ====
INFO:benchmark:== Benchmarking learner.infer ==
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
Traceback (most recent call last):
  File "benchmark_cox3d.py", line 109, in <module>
    benchmark_cox3d()
  File "benchmark_cox3d.py", line 92, in benchmark_cox3d
    results1 = benchmark(
  File "/opt/anaconda/envs/opendr/lib/python3.8/site-packages/pytorch_benchmark/benchmark.py", line 327, in benchmark
    pre_mem, post_mem, max_mem = measure_allocated_memory(
  File "/opt/anaconda/envs/opendr/lib/python3.8/site-packages/pytorch_benchmark/benchmark.py", line 84, in measure_allocated_memory
    model(transfer_to_device_fn(sample, model_device)),
  File "/home/lh/projects/opendr/src/opendr/perception/activity_recognition/cox3d/cox3d_learner.py", line 140, in infer
    results = self.model.forward(batch)
  File "/home/lh/projects/opendr/src/opendr/perception/activity_recognition/cox3d/algorithm/cox3d.py", line 1067, in forward
    x = module(x)
  File "/opt/anaconda/envs/opendr/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/lh/projects/opendr/src/opendr/perception/activity_recognition/cox3d/algorithm/cox3d.py", line 494, in forward
    x = m(x)
  File "/opt/anaconda/envs/opendr/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/lh/projects/opendr/src/opendr/perception/activity_recognition/cox3d/algorithm/cox3d.py", line 301, in forward
    output = delayed_x + f_x
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 9.43 GiB already allocated; 13.44 MiB free; 9.65 GiB reserved in total by PyTorch)
