WARNING:opendr.perception.activity_recognition.cox3d.algorithm.utils:Padding along the temporal dimension only affects the computation in `forward3d`. In `forward` it is omitted.
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
==== Benchmarking CoX3DLearner (xs) ====
== Benchmarking learner.infer ==
Machine info:
  cpu:
    architecture: x86_64
    cores:
      physical: 52
      total: 104
    frequency: 2.10 GHz
    model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
  gpus:
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  memory:
    available: 177.76 GB
    total: 187.56 GB
    used: 8.43 GB
  system:
    node: charybdis
    release: 4.15.0-167-generic
    system: Linux

Model device: cuda:0
Allocated GPU memory prior to inference: 15445504 (14.73 MB)
Allocated GPU memory after to inference: 7505677824 (6.99 GB)
Max allocated GPU memory during inference: 7554420736 (7.04 GB)
Warming up with batch_size=128:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=128:  20%|██        | 2/10 [00:00<00:00, 11.28it/s]Warming up with batch_size=128:  40%|████      | 4/10 [00:00<00:00, 11.63it/s]Warming up with batch_size=128:  60%|██████    | 6/10 [00:00<00:00, 11.81it/s]Warming up with batch_size=128:  80%|████████  | 8/10 [00:00<00:00, 11.76it/s]Warming up with batch_size=128: 100%|██████████| 10/10 [00:00<00:00, 11.89it/s]Warming up with batch_size=128: 100%|██████████| 10/10 [00:00<00:00, 11.79it/s]
Measuring inference with batch_size=128:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=128:   2%|▏         | 2/100 [00:00<00:10,  9.72it/s]Measuring inference with batch_size=128:   3%|▎         | 3/100 [00:00<00:11,  8.09it/s]Measuring inference with batch_size=128:   4%|▍         | 4/100 [00:00<00:11,  8.56it/s]Measuring inference with batch_size=128:   5%|▌         | 5/100 [00:00<00:10,  8.92it/s]Measuring inference with batch_size=128:   6%|▌         | 6/100 [00:00<00:10,  9.07it/s]Measuring inference with batch_size=128:   7%|▋         | 7/100 [00:00<00:10,  9.19it/s]Measuring inference with batch_size=128:   8%|▊         | 8/100 [00:00<00:09,  9.29it/s]Measuring inference with batch_size=128:   9%|▉         | 9/100 [00:00<00:09,  9.35it/s]Measuring inference with batch_size=128:  10%|█         | 10/100 [00:01<00:09,  9.42it/s]Measuring inference with batch_size=128:  11%|█         | 11/100 [00:01<00:09,  9.43it/s]Measuring inference with batch_size=128:  12%|█▏        | 12/100 [00:01<00:09,  9.44it/s]Measuring inference with batch_size=128:  13%|█▎        | 13/100 [00:01<00:09,  9.46it/s]Measuring inference with batch_size=128:  14%|█▍        | 14/100 [00:01<00:09,  9.45it/s]Measuring inference with batch_size=128:  15%|█▌        | 15/100 [00:01<00:08,  9.52it/s]Measuring inference with batch_size=128:  16%|█▌        | 16/100 [00:01<00:08,  9.64it/s]Measuring inference with batch_size=128:  17%|█▋        | 17/100 [00:01<00:08,  9.68it/s]Measuring inference with batch_size=128:  18%|█▊        | 18/100 [00:01<00:08,  9.73it/s]Measuring inference with batch_size=128:  19%|█▉        | 19/100 [00:02<00:08,  9.74it/s]Measuring inference with batch_size=128:  20%|██        | 20/100 [00:02<00:08,  9.80it/s]Measuring inference with batch_size=128:  21%|██        | 21/100 [00:02<00:08,  9.83it/s]Measuring inference with batch_size=128:  22%|██▏       | 22/100 [00:02<00:07,  9.88it/s]Measuring inference with batch_size=128:  23%|██▎       | 23/100 [00:02<00:07,  9.90it/s]Measuring inference with batch_size=128:  24%|██▍       | 24/100 [00:02<00:07,  9.90it/s]Measuring inference with batch_size=128:  25%|██▌       | 25/100 [00:02<00:07,  9.92it/s]Measuring inference with batch_size=128:  26%|██▌       | 26/100 [00:02<00:07,  9.86it/s]Measuring inference with batch_size=128:  27%|██▋       | 27/100 [00:02<00:07,  9.81it/s]Measuring inference with batch_size=128:  28%|██▊       | 28/100 [00:02<00:07,  9.84it/s]Measuring inference with batch_size=128:  29%|██▉       | 29/100 [00:03<00:07,  9.84it/s]Measuring inference with batch_size=128:  30%|███       | 30/100 [00:03<00:07,  9.87it/s]Measuring inference with batch_size=128:  31%|███       | 31/100 [00:03<00:06,  9.87it/s]Measuring inference with batch_size=128:  32%|███▏      | 32/100 [00:03<00:06,  9.87it/s]Measuring inference with batch_size=128:  33%|███▎      | 33/100 [00:03<00:06,  9.90it/s]Measuring inference with batch_size=128:  34%|███▍      | 34/100 [00:03<00:06,  9.91it/s]Measuring inference with batch_size=128:  35%|███▌      | 35/100 [00:03<00:06,  9.90it/s]Measuring inference with batch_size=128:  36%|███▌      | 36/100 [00:03<00:06,  9.88it/s]Measuring inference with batch_size=128:  37%|███▋      | 37/100 [00:03<00:06,  9.87it/s]Measuring inference with batch_size=128:  38%|███▊      | 38/100 [00:03<00:06,  9.87it/s]Measuring inference with batch_size=128:  39%|███▉      | 39/100 [00:04<00:06,  9.87it/s]Measuring inference with batch_size=128:  40%|████      | 40/100 [00:04<00:06,  9.90it/s]Measuring inference with batch_size=128:  41%|████      | 41/100 [00:04<00:05,  9.90it/s]Measuring inference with batch_size=128:  42%|████▏     | 42/100 [00:04<00:05,  9.84it/s]Measuring inference with batch_size=128:  43%|████▎     | 43/100 [00:04<00:05,  9.82it/s]Measuring inference with batch_size=128:  44%|████▍     | 44/100 [00:04<00:05,  9.77it/s]Measuring inference with batch_size=128:  45%|████▌     | 45/100 [00:04<00:05,  9.78it/s]Measuring inference with batch_size=128:  46%|████▌     | 46/100 [00:04<00:05,  9.80it/s]Measuring inference with batch_size=128:  47%|████▋     | 47/100 [00:04<00:05,  9.80it/s]Measuring inference with batch_size=128:  48%|████▊     | 48/100 [00:04<00:05,  9.83it/s]Measuring inference with batch_size=128:  49%|████▉     | 49/100 [00:05<00:05,  9.86it/s]Measuring inference with batch_size=128:  50%|█████     | 50/100 [00:05<00:05,  9.85it/s]Measuring inference with batch_size=128:  51%|█████     | 51/100 [00:05<00:04,  9.88it/s]Measuring inference with batch_size=128:  52%|█████▏    | 52/100 [00:05<00:04,  9.89it/s]Measuring inference with batch_size=128:  53%|█████▎    | 53/100 [00:05<00:04,  9.88it/s]Measuring inference with batch_size=128:  54%|█████▍    | 54/100 [00:05<00:04,  9.81it/s]Measuring inference with batch_size=128:  55%|█████▌    | 55/100 [00:05<00:04,  9.85it/s]Measuring inference with batch_size=128:  56%|█████▌    | 56/100 [00:05<00:04,  9.86it/s]Measuring inference with batch_size=128:  57%|█████▋    | 57/100 [00:05<00:04,  9.81it/s]Measuring inference with batch_size=128:  58%|█████▊    | 58/100 [00:05<00:04,  9.84it/s]Measuring inference with batch_size=128:  59%|█████▉    | 59/100 [00:06<00:04,  9.86it/s]Measuring inference with batch_size=128:  60%|██████    | 60/100 [00:06<00:04,  9.88it/s]Measuring inference with batch_size=128:  61%|██████    | 61/100 [00:06<00:03,  9.87it/s]Measuring inference with batch_size=128:  62%|██████▏   | 62/100 [00:06<00:03,  9.89it/s]Measuring inference with batch_size=128:  63%|██████▎   | 63/100 [00:06<00:03,  9.90it/s]Measuring inference with batch_size=128:  64%|██████▍   | 64/100 [00:06<00:03,  9.90it/s]Measuring inference with batch_size=128:  65%|██████▌   | 65/100 [00:06<00:03,  9.89it/s]Measuring inference with batch_size=128:  66%|██████▌   | 66/100 [00:06<00:03,  9.92it/s]Measuring inference with batch_size=128:  67%|██████▋   | 67/100 [00:06<00:03,  9.88it/s]Measuring inference with batch_size=128:  68%|██████▊   | 68/100 [00:06<00:03,  9.89it/s]Measuring inference with batch_size=128:  69%|██████▉   | 69/100 [00:07<00:03,  9.90it/s]Measuring inference with batch_size=128:  70%|███████   | 70/100 [00:07<00:03,  9.90it/s]Measuring inference with batch_size=128:  71%|███████   | 71/100 [00:07<00:02,  9.92it/s]Measuring inference with batch_size=128:  72%|███████▏  | 72/100 [00:07<00:02,  9.87it/s]Measuring inference with batch_size=128:  73%|███████▎  | 73/100 [00:07<00:02,  9.82it/s]Measuring inference with batch_size=128:  74%|███████▍  | 74/100 [00:07<00:02,  9.81it/s]Measuring inference with batch_size=128:  75%|███████▌  | 75/100 [00:07<00:02,  9.74it/s]Measuring inference with batch_size=128:  76%|███████▌  | 76/100 [00:07<00:02,  9.72it/s]Measuring inference with batch_size=128:  77%|███████▋  | 77/100 [00:07<00:02,  9.58it/s]Measuring inference with batch_size=128:  78%|███████▊  | 78/100 [00:08<00:02,  9.63it/s]Measuring inference with batch_size=128:  79%|███████▉  | 79/100 [00:08<00:02,  9.61it/s]Measuring inference with batch_size=128:  80%|████████  | 80/100 [00:08<00:02,  9.70it/s]Measuring inference with batch_size=128:  81%|████████  | 81/100 [00:08<00:01,  9.66it/s]Measuring inference with batch_size=128:  82%|████████▏ | 82/100 [00:08<00:01,  9.72it/s]Measuring inference with batch_size=128:  83%|████████▎ | 83/100 [00:08<00:01,  9.67it/s]Measuring inference with batch_size=128:  84%|████████▍ | 84/100 [00:08<00:01,  9.72it/s]Measuring inference with batch_size=128:  85%|████████▌ | 85/100 [00:08<00:01,  9.69it/s]Measuring inference with batch_size=128:  86%|████████▌ | 86/100 [00:08<00:01,  9.74it/s]Measuring inference with batch_size=128:  87%|████████▋ | 87/100 [00:08<00:01,  9.63it/s]Measuring inference with batch_size=128:  88%|████████▊ | 88/100 [00:09<00:01,  9.67it/s]Measuring inference with batch_size=128:  89%|████████▉ | 89/100 [00:09<00:01,  9.64it/s]Measuring inference with batch_size=128:  90%|█████████ | 90/100 [00:09<00:01,  9.70it/s]Measuring inference with batch_size=128:  91%|█████████ | 91/100 [00:09<00:00,  9.76it/s]Measuring inference with batch_size=128:  92%|█████████▏| 92/100 [00:09<00:00,  9.80it/s]Measuring inference with batch_size=128:  93%|█████████▎| 93/100 [00:09<00:00,  9.84it/s]Measuring inference with batch_size=128:  94%|█████████▍| 94/100 [00:09<00:00,  9.85it/s]Measuring inference with batch_size=128:  95%|█████████▌| 95/100 [00:09<00:00,  9.87it/s]Measuring inference with batch_size=128:  96%|█████████▌| 96/100 [00:09<00:00,  9.81it/s]Measuring inference with batch_size=128:  97%|█████████▋| 97/100 [00:09<00:00,  9.81it/s]Measuring inference with batch_size=128:  98%|█████████▊| 98/100 [00:10<00:00,  9.84it/s]Measuring inference with batch_size=128:  99%|█████████▉| 99/100 [00:10<00:00,  9.87it/s]Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:10<00:00,  9.88it/s]Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:10<00:00,  9.74it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "1.307 \xB5s +/- 0.289 \xB5s [0.715 \xB5s, 3.338 \xB5s]"
      batches_per_second: 794.12 K +/- 146.65 K [299.59 K, 1.40 M]
    metrics:
      batches_per_second_max: 1398101.3333333333
      batches_per_second_mean: 794121.5573333335
      batches_per_second_min: 299593.14285714284
      batches_per_second_std: 146650.9709366633
      seconds_per_batch_max: 3.337860107421875e-06
      seconds_per_batch_mean: 1.3065338134765626e-06
      seconds_per_batch_min: 7.152557373046875e-07
      seconds_per_batch_std: 2.89027833188853e-07
  gpu_to_cpu:
    human_readable:
      batch_latency: "43.390 \xB5s +/- 17.783 \xB5s [36.716 \xB5s, 211.239 \xB5s]"
      batches_per_second: 24.09 K +/- 2.96 K [4.73 K, 27.24 K]
    metrics:
      batches_per_second_max: 27235.74025974026
      batches_per_second_mean: 24086.074630407467
      batches_per_second_min: 4733.977426636568
      batches_per_second_std: 2956.665363667741
      seconds_per_batch_max: 0.00021123886108398438
      seconds_per_batch_mean: 4.338979721069336e-05
      seconds_per_batch_min: 3.6716461181640625e-05
      seconds_per_batch_std: 1.7783261121302897e-05
  on_device_inference:
    human_readable:
      batch_latency: 102.563 ms +/- 6.041 ms [81.821 ms, 152.060 ms]
      batches_per_second: 9.78 +/- 0.47 [6.58, 12.22]
    metrics:
      batches_per_second_max: 12.221735279033988
      batches_per_second_mean: 9.776888924758277
      batches_per_second_min: 6.57636037222575
      batches_per_second_std: 0.4669649139711797
      seconds_per_batch_max: 0.15205979347229004
      seconds_per_batch_mean: 0.10256254196166992
      seconds_per_batch_min: 0.08182144165039062
      seconds_per_batch_std: 0.006040980724733972
  total:
    human_readable:
      batch_latency: 102.607 ms +/- 6.049 ms [81.860 ms, 152.113 ms]
      batches_per_second: 9.77 +/- 0.47 [6.57, 12.22]
    metrics:
      batches_per_second_max: 12.216004310533137
      batches_per_second_mean: 9.772675815927698
      batches_per_second_min: 6.574041159229479
      batches_per_second_std: 0.46713925699963643
      seconds_per_batch_max: 0.1521134376525879
      seconds_per_batch_mean: 0.1026072382926941
      seconds_per_batch_min: 0.08185982704162598
      seconds_per_batch_std: 0.006048631372661362

Warming up with batch_size=128:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=128:  10%|█         | 1/10 [00:00<00:05,  1.55it/s]Warming up with batch_size=128:  20%|██        | 2/10 [00:01<00:04,  1.63it/s]Warming up with batch_size=128:  30%|███       | 3/10 [00:01<00:04,  1.64it/s]Warming up with batch_size=128:  40%|████      | 4/10 [00:02<00:03,  1.65it/s]Warming up with batch_size=128:  50%|█████     | 5/10 [00:03<00:03,  1.66it/s]Warming up with batch_size=128:  60%|██████    | 6/10 [00:03<00:02,  1.66it/s]Warming up with batch_size=128:  70%|███████   | 7/10 [00:04<00:01,  1.67it/s]Warming up with batch_size=128:  80%|████████  | 8/10 [00:04<00:01,  1.67it/s]Warming up with batch_size=128:  90%|█████████ | 9/10 [00:05<00:00,  1.68it/s]Warming up with batch_size=128: 100%|██████████| 10/10 [00:06<00:00,  1.68it/s]Warming up with batch_size=128: 100%|██████████| 10/10 [00:06<00:00,  1.66it/s]
Measuring inference with batch_size=128:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=128:   1%|          | 1/100 [00:00<00:59,  1.67it/s]Measuring inference with batch_size=128:   2%|▏         | 2/100 [00:01<00:58,  1.67it/s]Measuring inference with batch_size=128:   3%|▎         | 3/100 [00:01<00:57,  1.68it/s]Measuring inference with batch_size=128:   4%|▍         | 4/100 [00:02<00:57,  1.68it/s]Measuring inference with batch_size=128:   5%|▌         | 5/100 [00:02<00:56,  1.68it/s]Measuring inference with batch_size=128:   6%|▌         | 6/100 [00:03<00:55,  1.68it/s]Measuring inference with batch_size=128:   7%|▋         | 7/100 [00:04<00:55,  1.68it/s]Measuring inference with batch_size=128:   8%|▊         | 8/100 [00:04<00:54,  1.68it/s]Measuring inference with batch_size=128:   9%|▉         | 9/100 [00:05<00:54,  1.68it/s]Measuring inference with batch_size=128:  10%|█         | 10/100 [00:05<00:53,  1.68it/s]Measuring inference with batch_size=128:  11%|█         | 11/100 [00:06<00:52,  1.68it/s]Measuring inference with batch_size=128:  12%|█▏        | 12/100 [00:07<00:52,  1.68it/s]Measuring inference with batch_size=128:  13%|█▎        | 13/100 [00:07<00:51,  1.68it/s]Measuring inference with batch_size=128:  14%|█▍        | 14/100 [00:08<00:51,  1.68it/s]Measuring inference with batch_size=128:  15%|█▌        | 15/100 [00:08<00:50,  1.67it/s]Measuring inference with batch_size=128:  16%|█▌        | 16/100 [00:09<00:50,  1.68it/s]Measuring inference with batch_size=128:  17%|█▋        | 17/100 [00:10<00:49,  1.68it/s]Measuring inference with batch_size=128:  18%|█▊        | 18/100 [00:10<00:48,  1.68it/s]Measuring inference with batch_size=128:  19%|█▉        | 19/100 [00:11<00:48,  1.68it/s]Measuring inference with batch_size=128:  20%|██        | 20/100 [00:11<00:47,  1.68it/s]Measuring inference with batch_size=128:  21%|██        | 21/100 [00:12<00:46,  1.68it/s]Measuring inference with batch_size=128:  22%|██▏       | 22/100 [00:13<00:46,  1.68it/s]Measuring inference with batch_size=128:  23%|██▎       | 23/100 [00:13<00:45,  1.68it/s]Measuring inference with batch_size=128:  24%|██▍       | 24/100 [00:14<00:45,  1.68it/s]Measuring inference with batch_size=128:  25%|██▌       | 25/100 [00:14<00:44,  1.69it/s]Measuring inference with batch_size=128:  26%|██▌       | 26/100 [00:15<00:43,  1.68it/s]Measuring inference with batch_size=128:  27%|██▋       | 27/100 [00:16<00:43,  1.68it/s]Measuring inference with batch_size=128:  28%|██▊       | 28/100 [00:16<00:42,  1.68it/s]Measuring inference with batch_size=128:  29%|██▉       | 29/100 [00:17<00:42,  1.68it/s]Measuring inference with batch_size=128:  30%|███       | 30/100 [00:17<00:41,  1.68it/s]Measuring inference with batch_size=128:  31%|███       | 31/100 [00:18<00:41,  1.68it/s]Measuring inference with batch_size=128:  32%|███▏      | 32/100 [00:19<00:40,  1.68it/s]Measuring inference with batch_size=128:  33%|███▎      | 33/100 [00:19<00:39,  1.68it/s]Measuring inference with batch_size=128:  34%|███▍      | 34/100 [00:20<00:39,  1.68it/s]Measuring inference with batch_size=128:  35%|███▌      | 35/100 [00:20<00:39,  1.67it/s]Measuring inference with batch_size=128:  36%|███▌      | 36/100 [00:21<00:38,  1.67it/s]Measuring inference with batch_size=128:  37%|███▋      | 37/100 [00:22<00:37,  1.67it/s]Measuring inference with batch_size=128:  38%|███▊      | 38/100 [00:22<00:37,  1.68it/s]Measuring inference with batch_size=128:  39%|███▉      | 39/100 [00:23<00:36,  1.68it/s]Measuring inference with batch_size=128:  40%|████      | 40/100 [00:23<00:35,  1.67it/s]Measuring inference with batch_size=128:  41%|████      | 41/100 [00:24<00:35,  1.67it/s]Measuring inference with batch_size=128:  42%|████▏     | 42/100 [00:25<00:34,  1.67it/s]Measuring inference with batch_size=128:  43%|████▎     | 43/100 [00:25<00:34,  1.67it/s]Measuring inference with batch_size=128:  44%|████▍     | 44/100 [00:26<00:33,  1.67it/s]Measuring inference with batch_size=128:  45%|████▌     | 45/100 [00:26<00:32,  1.67it/s]Measuring inference with batch_size=128:  46%|████▌     | 46/100 [00:27<00:32,  1.67it/s]Measuring inference with batch_size=128:  47%|████▋     | 47/100 [00:28<00:31,  1.67it/s]Measuring inference with batch_size=128:  48%|████▊     | 48/100 [00:28<00:31,  1.67it/s]Measuring inference with batch_size=128:  49%|████▉     | 49/100 [00:29<00:30,  1.67it/s]Measuring inference with batch_size=128:  50%|█████     | 50/100 [00:29<00:29,  1.67it/s]Measuring inference with batch_size=128:  51%|█████     | 51/100 [00:30<00:29,  1.67it/s]Measuring inference with batch_size=128:  52%|█████▏    | 52/100 [00:30<00:28,  1.67it/s]Measuring inference with batch_size=128:  53%|█████▎    | 53/100 [00:31<00:29,  1.60it/s]Measuring inference with batch_size=128:  54%|█████▍    | 54/100 [00:32<00:28,  1.62it/s]Measuring inference with batch_size=128:  55%|█████▌    | 55/100 [00:32<00:27,  1.63it/s]Measuring inference with batch_size=128:  56%|█████▌    | 56/100 [00:33<00:26,  1.64it/s]Measuring inference with batch_size=128:  57%|█████▋    | 57/100 [00:34<00:26,  1.65it/s]Measuring inference with batch_size=128:  58%|█████▊    | 58/100 [00:34<00:25,  1.66it/s]Measuring inference with batch_size=128:  59%|█████▉    | 59/100 [00:35<00:24,  1.67it/s]Measuring inference with batch_size=128:  60%|██████    | 60/100 [00:35<00:23,  1.67it/s]Measuring inference with batch_size=128:  61%|██████    | 61/100 [00:36<00:23,  1.67it/s]Measuring inference with batch_size=128:  62%|██████▏   | 62/100 [00:37<00:22,  1.67it/s]Measuring inference with batch_size=128:  63%|██████▎   | 63/100 [00:37<00:22,  1.67it/s]Measuring inference with batch_size=128:  64%|██████▍   | 64/100 [00:38<00:21,  1.67it/s]Measuring inference with batch_size=128:  65%|██████▌   | 65/100 [00:38<00:20,  1.67it/s]Measuring inference with batch_size=128:  66%|██████▌   | 66/100 [00:39<00:20,  1.67it/s]Measuring inference with batch_size=128:  67%|██████▋   | 67/100 [00:40<00:19,  1.67it/s]Measuring inference with batch_size=128:  68%|██████▊   | 68/100 [00:40<00:19,  1.67it/s]Measuring inference with batch_size=128:  69%|██████▉   | 69/100 [00:41<00:18,  1.67it/s]Measuring inference with batch_size=128:  70%|███████   | 70/100 [00:41<00:17,  1.67it/s]Measuring inference with batch_size=128:  71%|███████   | 71/100 [00:42<00:17,  1.67it/s]Measuring inference with batch_size=128:  72%|███████▏  | 72/100 [00:43<00:16,  1.67it/s]Measuring inference with batch_size=128:  73%|███████▎  | 73/100 [00:43<00:16,  1.67it/s]Measuring inference with batch_size=128:  74%|███████▍  | 74/100 [00:44<00:15,  1.67it/s]Measuring inference with batch_size=128:  75%|███████▌  | 75/100 [00:44<00:14,  1.67it/s]Measuring inference with batch_size=128:  76%|███████▌  | 76/100 [00:45<00:14,  1.67it/s]Measuring inference with batch_size=128:  77%|███████▋  | 77/100 [00:46<00:13,  1.67it/s]Measuring inference with batch_size=128:  78%|███████▊  | 78/100 [00:46<00:13,  1.67it/s]Measuring inference with batch_size=128:  79%|███████▉  | 79/100 [00:47<00:12,  1.67it/s]Measuring inference with batch_size=128:  80%|████████  | 80/100 [00:47<00:11,  1.67it/s]Measuring inference with batch_size=128:  81%|████████  | 81/100 [00:48<00:11,  1.67it/s]Measuring inference with batch_size=128:  82%|████████▏ | 82/100 [00:49<00:10,  1.68it/s]Measuring inference with batch_size=128:  83%|████████▎ | 83/100 [00:49<00:10,  1.67it/s]Measuring inference with batch_size=128:  84%|████████▍ | 84/100 [00:50<00:09,  1.68it/s]Measuring inference with batch_size=128:  85%|████████▌ | 85/100 [00:50<00:08,  1.68it/s]Measuring inference with batch_size=128:  86%|████████▌ | 86/100 [00:51<00:08,  1.68it/s]Measuring inference with batch_size=128:  87%|████████▋ | 87/100 [00:52<00:07,  1.67it/s]Measuring inference with batch_size=128:  88%|████████▊ | 88/100 [00:52<00:07,  1.67it/s]Measuring inference with batch_size=128:  89%|████████▉ | 89/100 [00:53<00:06,  1.67it/s]Measuring inference with batch_size=128:  90%|█████████ | 90/100 [00:53<00:05,  1.67it/s]Measuring inference with batch_size=128:  91%|█████████ | 91/100 [00:54<00:05,  1.68it/s]Measuring inference with batch_size=128:  92%|█████████▏| 92/100 [00:54<00:04,  1.68it/s]Measuring inference with batch_size=128:  93%|█████████▎| 93/100 [00:55<00:04,  1.68it/s]Measuring inference with batch_size=128:  94%|█████████▍| 94/100 [00:56<00:03,  1.67it/s]Measuring inference with batch_size=128:  95%|█████████▌| 95/100 [00:56<00:02,  1.68it/s]Measuring inference with batch_size=128:  96%|█████████▌| 96/100 [00:57<00:02,  1.68it/s]Measuring inference with batch_size=128:  97%|█████████▋| 97/100 [00:57<00:01,  1.68it/s]Measuring inference with batch_size=128:  98%|█████████▊| 98/100 [00:58<00:01,  1.68it/s]Measuring inference with batch_size=128:  99%|█████████▉| 99/100 [00:59<00:00,  1.68it/s]Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:59<00:00,  1.67it/s]Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:59<00:00,  1.67it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Timing results (batch_size=128):
  cpu_to_gpu:
    human_readable:
      batch_latency: "2.000 \xB5s +/- 0.286 \xB5s [1.431 \xB5s, 3.099 \xB5s]"
      batches_per_second: 509.98 K +/- 72.06 K [322.64 K, 699.05 K]
    metrics:
      batches_per_second_max: 699050.6666666666
      batches_per_second_mean: 509976.08406171604
      batches_per_second_min: 322638.76923076925
      batches_per_second_std: 72057.14981495068
      seconds_per_batch_max: 3.0994415283203125e-06
      seconds_per_batch_mean: 2.0003318786621095e-06
      seconds_per_batch_min: 1.430511474609375e-06
      seconds_per_batch_std: 2.858936025516413e-07
  gpu_to_cpu:
    human_readable:
      batch_latency: "2.098 ms +/- 101.474 \xB5s [2.014 ms, 2.674 ms]"
      batches_per_second: 477.53 +/- 19.54 [373.96, 496.54]
    metrics:
      batches_per_second_max: 496.54362495560554
      batches_per_second_mean: 477.52970530533224
      batches_per_second_min: 373.9572039942939
      batches_per_second_std: 19.542101996781955
      seconds_per_batch_max: 0.002674102783203125
      seconds_per_batch_mean: 0.002098245620727539
      seconds_per_batch_min: 0.0020139217376708984
      seconds_per_batch_std: 0.00010147385956055478
  on_device_inference:
    human_readable:
      batch_latency: 595.472 ms +/- 9.099 ms [589.046 ms, 681.406 ms]
      batches_per_second: 1.68 +/- 0.02 [1.47, 1.70]
    metrics:
      batches_per_second_max: 1.6976609675966157
      batches_per_second_mean: 1.6796877122364904
      batches_per_second_min: 1.4675543198917573
      batches_per_second_std: 0.022786145673077185
      seconds_per_batch_max: 0.681405782699585
      seconds_per_batch_mean: 0.5954720878601074
      seconds_per_batch_min: 0.5890457630157471
      seconds_per_batch_std: 0.009098570621502532
  total:
    human_readable:
      batch_latency: 597.572 ms +/- 9.109 ms [591.126 ms, 683.536 ms]
      batches_per_second: 1.67 +/- 0.02 [1.46, 1.69]
    metrics:
      batches_per_second_max: 1.6916868129785
      batches_per_second_mean: 1.6737828180230405
      batches_per_second_min: 1.4629816432481
      batches_per_second_std: 0.022666292114437655
      seconds_per_batch_max: 0.6835355758666992
      seconds_per_batch_mean: 0.5975723338127136
      seconds_per_batch_min: 0.5911259651184082
      seconds_per_batch_std: 0.009109465072100234

learner.infer:
  device: cuda
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 52
        total: 104
      frequency: 2.10 GHz
      model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
    gpus:
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    memory:
      available: 177.76 GB
      total: 187.56 GB
      used: 8.43 GB
    system:
      node: charybdis
      release: 4.15.0-167-generic
      system: Linux
  max_inference_memory: 7554420736
  post_inference_memory: 7505677824
  pre_inference_memory: 15445504
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "1.307 \xB5s +/- 0.289 \xB5s [0.715 \xB5s, 3.338 \xB5s]"
          batches_per_second: 794.12 K +/- 146.65 K [299.59 K, 1.40 M]
        metrics:
          batches_per_second_max: 1398101.3333333333
          batches_per_second_mean: 794121.5573333335
          batches_per_second_min: 299593.14285714284
          batches_per_second_std: 146650.9709366633
          seconds_per_batch_max: 3.337860107421875e-06
          seconds_per_batch_mean: 1.3065338134765626e-06
          seconds_per_batch_min: 7.152557373046875e-07
          seconds_per_batch_std: 2.89027833188853e-07
      gpu_to_cpu:
        human_readable:
          batch_latency: "43.390 \xB5s +/- 17.783 \xB5s [36.716 \xB5s, 211.239 \xB5\
            s]"
          batches_per_second: 24.09 K +/- 2.96 K [4.73 K, 27.24 K]
        metrics:
          batches_per_second_max: 27235.74025974026
          batches_per_second_mean: 24086.074630407467
          batches_per_second_min: 4733.977426636568
          batches_per_second_std: 2956.665363667741
          seconds_per_batch_max: 0.00021123886108398438
          seconds_per_batch_mean: 4.338979721069336e-05
          seconds_per_batch_min: 3.6716461181640625e-05
          seconds_per_batch_std: 1.7783261121302897e-05
      on_device_inference:
        human_readable:
          batch_latency: 102.563 ms +/- 6.041 ms [81.821 ms, 152.060 ms]
          batches_per_second: 9.78 +/- 0.47 [6.58, 12.22]
        metrics:
          batches_per_second_max: 12.221735279033988
          batches_per_second_mean: 9.776888924758277
          batches_per_second_min: 6.57636037222575
          batches_per_second_std: 0.4669649139711797
          seconds_per_batch_max: 0.15205979347229004
          seconds_per_batch_mean: 0.10256254196166992
          seconds_per_batch_min: 0.08182144165039062
          seconds_per_batch_std: 0.006040980724733972
      total:
        human_readable:
          batch_latency: 102.607 ms +/- 6.049 ms [81.860 ms, 152.113 ms]
          batches_per_second: 9.77 +/- 0.47 [6.57, 12.22]
        metrics:
          batches_per_second_max: 12.216004310533137
          batches_per_second_mean: 9.772675815927698
          batches_per_second_min: 6.574041159229479
          batches_per_second_std: 0.46713925699963643
          seconds_per_batch_max: 0.1521134376525879
          seconds_per_batch_mean: 0.1026072382926941
          seconds_per_batch_min: 0.08185982704162598
          seconds_per_batch_std: 0.006048631372661362
    batch_size_128:
      cpu_to_gpu:
        human_readable:
          batch_latency: "2.000 \xB5s +/- 0.286 \xB5s [1.431 \xB5s, 3.099 \xB5s]"
          batches_per_second: 509.98 K +/- 72.06 K [322.64 K, 699.05 K]
        metrics:
          batches_per_second_max: 699050.6666666666
          batches_per_second_mean: 509976.08406171604
          batches_per_second_min: 322638.76923076925
          batches_per_second_std: 72057.14981495068
          seconds_per_batch_max: 3.0994415283203125e-06
          seconds_per_batch_mean: 2.0003318786621095e-06
          seconds_per_batch_min: 1.430511474609375e-06
          seconds_per_batch_std: 2.858936025516413e-07
      gpu_to_cpu:
        human_readable:
          batch_latency: "2.098 ms +/- 101.474 \xB5s [2.014 ms, 2.674 ms]"
          batches_per_second: 477.53 +/- 19.54 [373.96, 496.54]
        metrics:
          batches_per_second_max: 496.54362495560554
          batches_per_second_mean: 477.52970530533224
          batches_per_second_min: 373.9572039942939
          batches_per_second_std: 19.542101996781955
          seconds_per_batch_max: 0.002674102783203125
          seconds_per_batch_mean: 0.002098245620727539
          seconds_per_batch_min: 0.0020139217376708984
          seconds_per_batch_std: 0.00010147385956055478
      on_device_inference:
        human_readable:
          batch_latency: 595.472 ms +/- 9.099 ms [589.046 ms, 681.406 ms]
          batches_per_second: 1.68 +/- 0.02 [1.47, 1.70]
        metrics:
          batches_per_second_max: 1.6976609675966157
          batches_per_second_mean: 1.6796877122364904
          batches_per_second_min: 1.4675543198917573
          batches_per_second_std: 0.022786145673077185
          seconds_per_batch_max: 0.681405782699585
          seconds_per_batch_mean: 0.5954720878601074
          seconds_per_batch_min: 0.5890457630157471
          seconds_per_batch_std: 0.009098570621502532
      total:
        human_readable:
          batch_latency: 597.572 ms +/- 9.109 ms [591.126 ms, 683.536 ms]
          batches_per_second: 1.67 +/- 0.02 [1.46, 1.69]
        metrics:
          batches_per_second_max: 1.6916868129785
          batches_per_second_mean: 1.6737828180230405
          batches_per_second_min: 1.4629816432481
          batches_per_second_std: 0.022666292114437655
          seconds_per_batch_max: 0.6835355758666992
          seconds_per_batch_mean: 0.5975723338127136
          seconds_per_batch_min: 0.5911259651184082
          seconds_per_batch_std: 0.009109465072100234

== Benchmarking model directly ==
Machine info:
  cpu:
    architecture: x86_64
    cores:
      physical: 52
      total: 104
    frequency: 2.10 GHz
    model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
  gpus:
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  memory:
    available: 177.86 GB
    total: 187.56 GB
    used: 8.32 GB
  system:
    node: charybdis
    release: 4.15.0-167-generic
    system: Linux

Model device: cuda:0
Model parameters: 3794322 (3.79 M)
Model FLOPs: 138228272 (138.23 M)
Allocated GPU memory prior to inference: 73787904 (70.37 MB)
Allocated GPU memory after to inference: 7513394688 (7.00 GB)
Max allocated GPU memory during inference: 7563021824 (7.04 GB)
Warming up with batch_size=128:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=128:  10%|█         | 1/10 [00:00<00:00,  9.46it/s]Warming up with batch_size=128:  30%|███       | 3/10 [00:00<00:00,  9.90it/s]Warming up with batch_size=128:  40%|████      | 4/10 [00:00<00:00,  9.84it/s]Warming up with batch_size=128:  60%|██████    | 6/10 [00:00<00:00,  9.89it/s]Warming up with batch_size=128:  80%|████████  | 8/10 [00:00<00:00,  9.97it/s]Warming up with batch_size=128: 100%|██████████| 10/10 [00:01<00:00, 10.04it/s]Warming up with batch_size=128: 100%|██████████| 10/10 [00:01<00:00,  9.96it/s]
Measuring inference with batch_size=128:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=128:   2%|▏         | 2/100 [00:00<00:09, 10.16it/s]Measuring inference with batch_size=128:   4%|▍         | 4/100 [00:00<00:09, 10.19it/s]Measuring inference with batch_size=128:   6%|▌         | 6/100 [00:00<00:09, 10.16it/s]Measuring inference with batch_size=128:   8%|▊         | 8/100 [00:00<00:09, 10.17it/s]Measuring inference with batch_size=128:  10%|█         | 10/100 [00:00<00:08, 10.21it/s]Measuring inference with batch_size=128:  12%|█▏        | 12/100 [00:01<00:08, 10.17it/s]Measuring inference with batch_size=128:  14%|█▍        | 14/100 [00:01<00:08, 10.18it/s]Measuring inference with batch_size=128:  16%|█▌        | 16/100 [00:01<00:08, 10.24it/s]Measuring inference with batch_size=128:  18%|█▊        | 18/100 [00:01<00:08, 10.18it/s]Measuring inference with batch_size=128:  20%|██        | 20/100 [00:01<00:07, 10.20it/s]Measuring inference with batch_size=128:  22%|██▏       | 22/100 [00:02<00:07, 10.19it/s]Measuring inference with batch_size=128:  24%|██▍       | 24/100 [00:02<00:07, 10.22it/s]Measuring inference with batch_size=128:  26%|██▌       | 26/100 [00:02<00:07, 10.21it/s]Measuring inference with batch_size=128:  28%|██▊       | 28/100 [00:02<00:07, 10.25it/s]Measuring inference with batch_size=128:  30%|███       | 30/100 [00:02<00:06, 10.22it/s]Measuring inference with batch_size=128:  32%|███▏      | 32/100 [00:03<00:06, 10.26it/s]Measuring inference with batch_size=128:  34%|███▍      | 34/100 [00:03<00:06, 10.22it/s]Measuring inference with batch_size=128:  36%|███▌      | 36/100 [00:03<00:06, 10.24it/s]Measuring inference with batch_size=128:  38%|███▊      | 38/100 [00:03<00:06, 10.18it/s]Measuring inference with batch_size=128:  40%|████      | 40/100 [00:03<00:05, 10.23it/s]Measuring inference with batch_size=128:  42%|████▏     | 42/100 [00:04<00:05, 10.23it/s]Measuring inference with batch_size=128:  44%|████▍     | 44/100 [00:04<00:05, 10.26it/s]Measuring inference with batch_size=128:  46%|████▌     | 46/100 [00:04<00:05, 10.25it/s]Measuring inference with batch_size=128:  48%|████▊     | 48/100 [00:04<00:05, 10.28it/s]Measuring inference with batch_size=128:  50%|█████     | 50/100 [00:04<00:04, 10.25it/s]Measuring inference with batch_size=128:  52%|█████▏    | 52/100 [00:05<00:04, 10.26it/s]Measuring inference with batch_size=128:  54%|█████▍    | 54/100 [00:05<00:04, 10.24it/s]Measuring inference with batch_size=128:  56%|█████▌    | 56/100 [00:05<00:04, 10.28it/s]Measuring inference with batch_size=128:  58%|█████▊    | 58/100 [00:05<00:04, 10.31it/s]Measuring inference with batch_size=128:  60%|██████    | 60/100 [00:05<00:03, 10.32it/s]Measuring inference with batch_size=128:  62%|██████▏   | 62/100 [00:06<00:03, 10.33it/s]Measuring inference with batch_size=128:  64%|██████▍   | 64/100 [00:06<00:03, 10.28it/s]Measuring inference with batch_size=128:  66%|██████▌   | 66/100 [00:06<00:03, 10.29it/s]Measuring inference with batch_size=128:  68%|██████▊   | 68/100 [00:06<00:03, 10.34it/s]Measuring inference with batch_size=128:  70%|███████   | 70/100 [00:06<00:02, 10.32it/s]Measuring inference with batch_size=128:  72%|███████▏  | 72/100 [00:07<00:02, 10.28it/s]Measuring inference with batch_size=128:  74%|███████▍  | 74/100 [00:07<00:02, 10.30it/s]Measuring inference with batch_size=128:  76%|███████▌  | 76/100 [00:07<00:02, 10.29it/s]Measuring inference with batch_size=128:  78%|███████▊  | 78/100 [00:07<00:02, 10.31it/s]Measuring inference with batch_size=128:  80%|████████  | 80/100 [00:07<00:01, 10.28it/s]Measuring inference with batch_size=128:  82%|████████▏ | 82/100 [00:07<00:01, 10.31it/s]Measuring inference with batch_size=128:  84%|████████▍ | 84/100 [00:08<00:01, 10.32it/s]Measuring inference with batch_size=128:  86%|████████▌ | 86/100 [00:08<00:01, 10.30it/s]Measuring inference with batch_size=128:  88%|████████▊ | 88/100 [00:08<00:01, 10.27it/s]Measuring inference with batch_size=128:  90%|█████████ | 90/100 [00:08<00:00, 10.28it/s]Measuring inference with batch_size=128:  92%|█████████▏| 92/100 [00:08<00:00, 10.27it/s]Measuring inference with batch_size=128:  94%|█████████▍| 94/100 [00:09<00:00, 10.29it/s]Measuring inference with batch_size=128:  96%|█████████▌| 96/100 [00:09<00:00, 10.26it/s]Measuring inference with batch_size=128:  98%|█████████▊| 98/100 [00:09<00:00, 10.28it/s]Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:09<00:00, 10.33it/s]Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:09<00:00, 10.26it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "76.745 \xB5s +/- 18.257 \xB5s [67.234 \xB5s, 221.968 \xB5s]"
      batches_per_second: 13.36 K +/- 1.49 K [4.51 K, 14.87 K]
    metrics:
      batches_per_second_max: 14873.418439716312
      batches_per_second_mean: 13356.94391242652
      batches_per_second_min: 4505.160042964554
      batches_per_second_std: 1489.155380543866
      seconds_per_batch_max: 0.0002219676971435547
      seconds_per_batch_mean: 7.674455642700195e-05
      seconds_per_batch_min: 6.723403930664062e-05
      seconds_per_batch_std: 1.8256820614076345e-05
  gpu_to_cpu:
    human_readable:
      batch_latency: "32.532 \xB5s +/- 2.845 \xB5s [30.756 \xB5s, 49.114 \xB5s]"
      batches_per_second: 30.91 K +/- 2.02 K [20.36 K, 32.51 K]
    metrics:
      batches_per_second_max: 32513.98449612403
      batches_per_second_mean: 30913.01785090153
      batches_per_second_min: 20360.699029126212
      batches_per_second_std: 2015.5554326843856
      seconds_per_batch_max: 4.9114227294921875e-05
      seconds_per_batch_mean: 3.25322151184082e-05
      seconds_per_batch_min: 3.075599670410156e-05
      seconds_per_batch_std: 2.844559467877033e-06
  on_device_inference:
    human_readable:
      batch_latency: 97.301 ms +/- 1.379 ms [95.017 ms, 101.924 ms]
      batches_per_second: 10.28 +/- 0.14 [9.81, 10.52]
    metrics:
      batches_per_second_max: 10.524463715313063
      batches_per_second_mean: 10.279465534040865
      batches_per_second_min: 9.811260377217257
      batches_per_second_std: 0.14311666225294406
      seconds_per_batch_max: 0.10192370414733887
      seconds_per_batch_mean: 0.09730051517486572
      seconds_per_batch_min: 0.0950167179107666
      seconds_per_batch_std: 0.0013786977510435075
  total:
    human_readable:
      batch_latency: 97.410 ms +/- 1.379 ms [95.126 ms, 102.027 ms]
      batches_per_second: 10.27 +/- 0.14 [9.80, 10.51]
    metrics:
      batches_per_second_max: 10.512382608957184
      batches_per_second_mean: 10.267930099632158
      batches_per_second_min: 9.801332922053037
      batches_per_second_std: 0.1428307832526714
      seconds_per_batch_max: 0.10202693939208984
      seconds_per_batch_mean: 0.09740979194641114
      seconds_per_batch_min: 0.09512591361999512
      seconds_per_batch_std: 0.0013789548555267707

Warming up with batch_size=128:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=128:  10%|█         | 1/10 [00:00<00:05,  1.65it/s]Warming up with batch_size=128:  20%|██        | 2/10 [00:01<00:04,  1.70it/s]Warming up with batch_size=128:  30%|███       | 3/10 [00:01<00:04,  1.72it/s]Warming up with batch_size=128:  40%|████      | 4/10 [00:02<00:03,  1.72it/s]Warming up with batch_size=128:  50%|█████     | 5/10 [00:02<00:02,  1.72it/s]Warming up with batch_size=128:  60%|██████    | 6/10 [00:03<00:02,  1.72it/s]Warming up with batch_size=128:  70%|███████   | 7/10 [00:04<00:01,  1.72it/s]Warming up with batch_size=128:  80%|████████  | 8/10 [00:04<00:01,  1.72it/s]Warming up with batch_size=128:  90%|█████████ | 9/10 [00:05<00:00,  1.73it/s]Warming up with batch_size=128: 100%|██████████| 10/10 [00:05<00:00,  1.73it/s]Warming up with batch_size=128: 100%|██████████| 10/10 [00:05<00:00,  1.72it/s]
Measuring inference with batch_size=128:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=128:   1%|          | 1/100 [00:00<00:57,  1.73it/s]Measuring inference with batch_size=128:   2%|▏         | 2/100 [00:01<00:56,  1.73it/s]Measuring inference with batch_size=128:   3%|▎         | 3/100 [00:01<00:55,  1.73it/s]Measuring inference with batch_size=128:   4%|▍         | 4/100 [00:02<00:55,  1.73it/s]Measuring inference with batch_size=128:   5%|▌         | 5/100 [00:02<00:54,  1.73it/s]Measuring inference with batch_size=128:   6%|▌         | 6/100 [00:03<00:54,  1.73it/s]Measuring inference with batch_size=128:   7%|▋         | 7/100 [00:04<00:53,  1.73it/s]Measuring inference with batch_size=128:   8%|▊         | 8/100 [00:04<00:53,  1.73it/s]Measuring inference with batch_size=128:   9%|▉         | 9/100 [00:05<00:52,  1.73it/s]Measuring inference with batch_size=128:  10%|█         | 10/100 [00:05<00:51,  1.73it/s]Measuring inference with batch_size=128:  11%|█         | 11/100 [00:06<00:51,  1.73it/s]Measuring inference with batch_size=128:  12%|█▏        | 12/100 [00:06<00:50,  1.73it/s]Measuring inference with batch_size=128:  13%|█▎        | 13/100 [00:07<00:50,  1.73it/s]Measuring inference with batch_size=128:  14%|█▍        | 14/100 [00:08<00:49,  1.73it/s]Measuring inference with batch_size=128:  15%|█▌        | 15/100 [00:08<00:48,  1.74it/s]Measuring inference with batch_size=128:  16%|█▌        | 16/100 [00:09<00:48,  1.73it/s]Measuring inference with batch_size=128:  17%|█▋        | 17/100 [00:09<00:47,  1.74it/s]Measuring inference with batch_size=128:  18%|█▊        | 18/100 [00:10<00:47,  1.74it/s]Measuring inference with batch_size=128:  19%|█▉        | 19/100 [00:10<00:46,  1.74it/s]Measuring inference with batch_size=128:  20%|██        | 20/100 [00:11<00:46,  1.73it/s]Measuring inference with batch_size=128:  21%|██        | 21/100 [00:12<00:45,  1.73it/s]Measuring inference with batch_size=128:  22%|██▏       | 22/100 [00:12<00:44,  1.73it/s]Measuring inference with batch_size=128:  23%|██▎       | 23/100 [00:13<00:44,  1.74it/s]Measuring inference with batch_size=128:  24%|██▍       | 24/100 [00:13<00:44,  1.72it/s]Measuring inference with batch_size=128:  25%|██▌       | 25/100 [00:14<00:43,  1.73it/s]Measuring inference with batch_size=128:  26%|██▌       | 26/100 [00:15<00:42,  1.73it/s]Measuring inference with batch_size=128:  27%|██▋       | 27/100 [00:15<00:42,  1.73it/s]Measuring inference with batch_size=128:  28%|██▊       | 28/100 [00:16<00:41,  1.73it/s]Measuring inference with batch_size=128:  29%|██▉       | 29/100 [00:16<00:41,  1.73it/s]Measuring inference with batch_size=128:  30%|███       | 30/100 [00:17<00:40,  1.73it/s]Measuring inference with batch_size=128:  31%|███       | 31/100 [00:17<00:39,  1.73it/s]Measuring inference with batch_size=128:  32%|███▏      | 32/100 [00:18<00:39,  1.73it/s]Measuring inference with batch_size=128:  33%|███▎      | 33/100 [00:19<00:38,  1.73it/s]Measuring inference with batch_size=128:  34%|███▍      | 34/100 [00:19<00:38,  1.73it/s]Measuring inference with batch_size=128:  35%|███▌      | 35/100 [00:20<00:37,  1.73it/s]Measuring inference with batch_size=128:  36%|███▌      | 36/100 [00:20<00:36,  1.73it/s]Measuring inference with batch_size=128:  37%|███▋      | 37/100 [00:21<00:36,  1.73it/s]Measuring inference with batch_size=128:  38%|███▊      | 38/100 [00:21<00:35,  1.73it/s]Measuring inference with batch_size=128:  39%|███▉      | 39/100 [00:22<00:35,  1.73it/s]Measuring inference with batch_size=128:  40%|████      | 40/100 [00:23<00:34,  1.73it/s]Measuring inference with batch_size=128:  41%|████      | 41/100 [00:23<00:34,  1.73it/s]Measuring inference with batch_size=128:  42%|████▏     | 42/100 [00:24<00:33,  1.73it/s]Measuring inference with batch_size=128:  43%|████▎     | 43/100 [00:24<00:32,  1.73it/s]Measuring inference with batch_size=128:  44%|████▍     | 44/100 [00:25<00:32,  1.73it/s]Measuring inference with batch_size=128:  45%|████▌     | 45/100 [00:25<00:31,  1.73it/s]Measuring inference with batch_size=128:  46%|████▌     | 46/100 [00:26<00:31,  1.73it/s]Measuring inference with batch_size=128:  47%|████▋     | 47/100 [00:27<00:30,  1.73it/s]Measuring inference with batch_size=128:  48%|████▊     | 48/100 [00:27<00:30,  1.73it/s]Measuring inference with batch_size=128:  49%|████▉     | 49/100 [00:28<00:29,  1.73it/s]Measuring inference with batch_size=128:  50%|█████     | 50/100 [00:28<00:28,  1.73it/s]Measuring inference with batch_size=128:  51%|█████     | 51/100 [00:29<00:28,  1.73it/s]Measuring inference with batch_size=128:  52%|█████▏    | 52/100 [00:30<00:27,  1.73it/s]Measuring inference with batch_size=128:  53%|█████▎    | 53/100 [00:30<00:27,  1.73it/s]Measuring inference with batch_size=128:  54%|█████▍    | 54/100 [00:31<00:26,  1.73it/s]Measuring inference with batch_size=128:  55%|█████▌    | 55/100 [00:31<00:26,  1.73it/s]Measuring inference with batch_size=128:  56%|█████▌    | 56/100 [00:32<00:25,  1.73it/s]Measuring inference with batch_size=128:  57%|█████▋    | 57/100 [00:32<00:24,  1.73it/s]Measuring inference with batch_size=128:  58%|█████▊    | 58/100 [00:33<00:24,  1.73it/s]Measuring inference with batch_size=128:  59%|█████▉    | 59/100 [00:34<00:23,  1.73it/s]Measuring inference with batch_size=128:  60%|██████    | 60/100 [00:34<00:23,  1.73it/s]Measuring inference with batch_size=128:  61%|██████    | 61/100 [00:35<00:22,  1.73it/s]Measuring inference with batch_size=128:  62%|██████▏   | 62/100 [00:35<00:21,  1.73it/s]Measuring inference with batch_size=128:  63%|██████▎   | 63/100 [00:36<00:21,  1.73it/s]Measuring inference with batch_size=128:  64%|██████▍   | 64/100 [00:36<00:20,  1.73it/s]Measuring inference with batch_size=128:  65%|██████▌   | 65/100 [00:37<00:20,  1.73it/s]Measuring inference with batch_size=128:  66%|██████▌   | 66/100 [00:38<00:19,  1.73it/s]Measuring inference with batch_size=128:  67%|██████▋   | 67/100 [00:38<00:19,  1.73it/s]Measuring inference with batch_size=128:  68%|██████▊   | 68/100 [00:39<00:18,  1.73it/s]Measuring inference with batch_size=128:  69%|██████▉   | 69/100 [00:39<00:17,  1.73it/s]Measuring inference with batch_size=128:  70%|███████   | 70/100 [00:40<00:17,  1.73it/s]Measuring inference with batch_size=128:  71%|███████   | 71/100 [00:41<00:16,  1.73it/s]Measuring inference with batch_size=128:  72%|███████▏  | 72/100 [00:41<00:16,  1.73it/s]Measuring inference with batch_size=128:  73%|███████▎  | 73/100 [00:42<00:15,  1.73it/s]Measuring inference with batch_size=128:  74%|███████▍  | 74/100 [00:42<00:15,  1.73it/s]Measuring inference with batch_size=128:  75%|███████▌  | 75/100 [00:43<00:14,  1.73it/s]Measuring inference with batch_size=128:  76%|███████▌  | 76/100 [00:43<00:13,  1.73it/s]Measuring inference with batch_size=128:  77%|███████▋  | 77/100 [00:44<00:13,  1.73it/s]Measuring inference with batch_size=128:  78%|███████▊  | 78/100 [00:45<00:12,  1.73it/s]Measuring inference with batch_size=128:  79%|███████▉  | 79/100 [00:45<00:12,  1.73it/s]Measuring inference with batch_size=128:  80%|████████  | 80/100 [00:46<00:11,  1.72it/s]Measuring inference with batch_size=128:  81%|████████  | 81/100 [00:46<00:11,  1.72it/s]Measuring inference with batch_size=128:  82%|████████▏ | 82/100 [00:47<00:10,  1.72it/s]Measuring inference with batch_size=128:  83%|████████▎ | 83/100 [00:47<00:09,  1.72it/s]Measuring inference with batch_size=128:  84%|████████▍ | 84/100 [00:48<00:09,  1.73it/s]Measuring inference with batch_size=128:  85%|████████▌ | 85/100 [00:49<00:08,  1.72it/s]Measuring inference with batch_size=128:  86%|████████▌ | 86/100 [00:49<00:08,  1.73it/s]Measuring inference with batch_size=128:  87%|████████▋ | 87/100 [00:50<00:07,  1.73it/s]Measuring inference with batch_size=128:  88%|████████▊ | 88/100 [00:50<00:06,  1.73it/s]Measuring inference with batch_size=128:  89%|████████▉ | 89/100 [00:51<00:06,  1.73it/s]Measuring inference with batch_size=128:  90%|█████████ | 90/100 [00:52<00:05,  1.73it/s]Measuring inference with batch_size=128:  91%|█████████ | 91/100 [00:52<00:05,  1.73it/s]Measuring inference with batch_size=128:  92%|█████████▏| 92/100 [00:53<00:04,  1.73it/s]Measuring inference with batch_size=128:  93%|█████████▎| 93/100 [00:53<00:04,  1.73it/s]Measuring inference with batch_size=128:  94%|█████████▍| 94/100 [00:54<00:03,  1.73it/s]Measuring inference with batch_size=128:  95%|█████████▌| 95/100 [00:54<00:02,  1.73it/s]Measuring inference with batch_size=128:  96%|█████████▌| 96/100 [00:55<00:02,  1.73it/s]Measuring inference with batch_size=128:  97%|█████████▋| 97/100 [00:56<00:01,  1.73it/s]Measuring inference with batch_size=128:  98%|█████████▊| 98/100 [00:56<00:01,  1.73it/s]Measuring inference with batch_size=128:  99%|█████████▉| 99/100 [00:57<00:00,  1.73it/s]Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:57<00:00,  1.73it/s]Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:57<00:00,  1.73it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
Timing results (batch_size=128):
  cpu_to_gpu:
    human_readable:
      batch_latency: 4.149 ms +/- 1.517 ms [3.714 ms, 15.018 ms]
      batches_per_second: 250.92 +/- 28.10 [66.59, 269.28]
    metrics:
      batches_per_second_max: 269.2799178222907
      batches_per_second_mean: 250.92442709780443
      batches_per_second_min: 66.58788042356602
      batches_per_second_std: 28.103875689546623
      seconds_per_batch_max: 0.01501774787902832
      seconds_per_batch_mean: 0.004149172306060791
      seconds_per_batch_min: 0.0037136077880859375
      seconds_per_batch_std: 0.0015172795008819738
  gpu_to_cpu:
    human_readable:
      batch_latency: 22.053 ms +/- 1.858 ms [5.071 ms, 23.076 ms]
      batches_per_second: 46.57 +/- 15.23 [43.33, 197.19]
    metrics:
      batches_per_second_max: 197.19341795956746
      batches_per_second_mean: 46.57281950769411
      batches_per_second_min: 43.3349588791999
      batches_per_second_std: 15.227594555151024
      seconds_per_batch_max: 0.02307605743408203
      seconds_per_batch_mean: 0.02205252170562744
      seconds_per_batch_min: 0.005071163177490234
      seconds_per_batch_std: 0.0018584273245822556
  on_device_inference:
    human_readable:
      batch_latency: 551.713 ms +/- 2.240 ms [548.694 ms, 569.649 ms]
      batches_per_second: 1.81 +/- 0.01 [1.76, 1.82]
    metrics:
      batches_per_second_max: 1.8225097973441255
      batches_per_second_mean: 1.8125648711358187
      batches_per_second_min: 1.755465551799431
      batches_per_second_std: 0.007210809008412067
      seconds_per_batch_max: 0.5696494579315186
      seconds_per_batch_mean: 0.5517133021354675
      seconds_per_batch_min: 0.5486938953399658
      seconds_per_batch_std: 0.002239998758547487
  total:
    human_readable:
      batch_latency: 577.915 ms +/- 1.861 ms [575.361 ms, 589.385 ms]
      batches_per_second: 1.73 +/- 0.01 [1.70, 1.74]
    metrics:
      batches_per_second_max: 1.7380406483076465
      batches_per_second_mean: 1.730376041920713
      batches_per_second_min: 1.6966830511059396
      batches_per_second_std: 0.0055156012941479215
      seconds_per_batch_max: 0.5893852710723877
      seconds_per_batch_mean: 0.5779149961471558
      seconds_per_batch_min: 0.5753605365753174
      seconds_per_batch_std: 0.0018609077135066055

learner.model.forward:
  device: cuda
  flops: 138228272
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 52
        total: 104
      frequency: 2.10 GHz
      model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
    gpus:
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    memory:
      available: 177.86 GB
      total: 187.56 GB
      used: 8.32 GB
    system:
      node: charybdis
      release: 4.15.0-167-generic
      system: Linux
  max_inference_memory: 7563021824
  params: 3794322
  post_inference_memory: 7513394688
  pre_inference_memory: 73787904
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "76.745 \xB5s +/- 18.257 \xB5s [67.234 \xB5s, 221.968 \xB5\
            s]"
          batches_per_second: 13.36 K +/- 1.49 K [4.51 K, 14.87 K]
        metrics:
          batches_per_second_max: 14873.418439716312
          batches_per_second_mean: 13356.94391242652
          batches_per_second_min: 4505.160042964554
          batches_per_second_std: 1489.155380543866
          seconds_per_batch_max: 0.0002219676971435547
          seconds_per_batch_mean: 7.674455642700195e-05
          seconds_per_batch_min: 6.723403930664062e-05
          seconds_per_batch_std: 1.8256820614076345e-05
      gpu_to_cpu:
        human_readable:
          batch_latency: "32.532 \xB5s +/- 2.845 \xB5s [30.756 \xB5s, 49.114 \xB5\
            s]"
          batches_per_second: 30.91 K +/- 2.02 K [20.36 K, 32.51 K]
        metrics:
          batches_per_second_max: 32513.98449612403
          batches_per_second_mean: 30913.01785090153
          batches_per_second_min: 20360.699029126212
          batches_per_second_std: 2015.5554326843856
          seconds_per_batch_max: 4.9114227294921875e-05
          seconds_per_batch_mean: 3.25322151184082e-05
          seconds_per_batch_min: 3.075599670410156e-05
          seconds_per_batch_std: 2.844559467877033e-06
      on_device_inference:
        human_readable:
          batch_latency: 97.301 ms +/- 1.379 ms [95.017 ms, 101.924 ms]
          batches_per_second: 10.28 +/- 0.14 [9.81, 10.52]
        metrics:
          batches_per_second_max: 10.524463715313063
          batches_per_second_mean: 10.279465534040865
          batches_per_second_min: 9.811260377217257
          batches_per_second_std: 0.14311666225294406
          seconds_per_batch_max: 0.10192370414733887
          seconds_per_batch_mean: 0.09730051517486572
          seconds_per_batch_min: 0.0950167179107666
          seconds_per_batch_std: 0.0013786977510435075
      total:
        human_readable:
          batch_latency: 97.410 ms +/- 1.379 ms [95.126 ms, 102.027 ms]
          batches_per_second: 10.27 +/- 0.14 [9.80, 10.51]
        metrics:
          batches_per_second_max: 10.512382608957184
          batches_per_second_mean: 10.267930099632158
          batches_per_second_min: 9.801332922053037
          batches_per_second_std: 0.1428307832526714
          seconds_per_batch_max: 0.10202693939208984
          seconds_per_batch_mean: 0.09740979194641114
          seconds_per_batch_min: 0.09512591361999512
          seconds_per_batch_std: 0.0013789548555267707
    batch_size_128:
      cpu_to_gpu:
        human_readable:
          batch_latency: 4.149 ms +/- 1.517 ms [3.714 ms, 15.018 ms]
          batches_per_second: 250.92 +/- 28.10 [66.59, 269.28]
        metrics:
          batches_per_second_max: 269.2799178222907
          batches_per_second_mean: 250.92442709780443
          batches_per_second_min: 66.58788042356602
          batches_per_second_std: 28.103875689546623
          seconds_per_batch_max: 0.01501774787902832
          seconds_per_batch_mean: 0.004149172306060791
          seconds_per_batch_min: 0.0037136077880859375
          seconds_per_batch_std: 0.0015172795008819738
      gpu_to_cpu:
        human_readable:
          batch_latency: 22.053 ms +/- 1.858 ms [5.071 ms, 23.076 ms]
          batches_per_second: 46.57 +/- 15.23 [43.33, 197.19]
        metrics:
          batches_per_second_max: 197.19341795956746
          batches_per_second_mean: 46.57281950769411
          batches_per_second_min: 43.3349588791999
          batches_per_second_std: 15.227594555151024
          seconds_per_batch_max: 0.02307605743408203
          seconds_per_batch_mean: 0.02205252170562744
          seconds_per_batch_min: 0.005071163177490234
          seconds_per_batch_std: 0.0018584273245822556
      on_device_inference:
        human_readable:
          batch_latency: 551.713 ms +/- 2.240 ms [548.694 ms, 569.649 ms]
          batches_per_second: 1.81 +/- 0.01 [1.76, 1.82]
        metrics:
          batches_per_second_max: 1.8225097973441255
          batches_per_second_mean: 1.8125648711358187
          batches_per_second_min: 1.755465551799431
          batches_per_second_std: 0.007210809008412067
          seconds_per_batch_max: 0.5696494579315186
          seconds_per_batch_mean: 0.5517133021354675
          seconds_per_batch_min: 0.5486938953399658
          seconds_per_batch_std: 0.002239998758547487
      total:
        human_readable:
          batch_latency: 577.915 ms +/- 1.861 ms [575.361 ms, 589.385 ms]
          batches_per_second: 1.73 +/- 0.01 [1.70, 1.74]
        metrics:
          batches_per_second_max: 1.7380406483076465
          batches_per_second_mean: 1.730376041920713
          batches_per_second_min: 1.6966830511059396
          batches_per_second_std: 0.0055156012941479215
          seconds_per_batch_max: 0.5893852710723877
          seconds_per_batch_mean: 0.5779149961471558
          seconds_per_batch_min: 0.5753605365753174
          seconds_per_batch_std: 0.0018609077135066055

==== Benchmarking CoX3DLearner (s) ====
== Benchmarking learner.infer ==
Machine info:
  cpu:
    architecture: x86_64
    cores:
      physical: 52
      total: 104
    frequency: 2.10 GHz
    model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
  gpus:
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  memory:
    available: 177.86 GB
    total: 187.56 GB
    used: 8.32 GB
  system:
    node: charybdis
    release: 4.15.0-167-generic
    system: Linux

Model device: cuda:0
Allocated GPU memory prior to inference: 3423591424 (3.19 GB)
Allocated GPU memory after to inference: 7198198784 (6.70 GB)
Max allocated GPU memory during inference: 7223430656 (6.73 GB)
Warming up with batch_size=64:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=64:  10%|█         | 1/10 [00:00<00:00,  9.30it/s]Warming up with batch_size=64:  20%|██        | 2/10 [00:00<00:00,  9.52it/s]Warming up with batch_size=64:  30%|███       | 3/10 [00:00<00:00,  9.60it/s]Warming up with batch_size=64:  40%|████      | 4/10 [00:00<00:00,  9.60it/s]Warming up with batch_size=64:  50%|█████     | 5/10 [00:00<00:00,  9.40it/s]Warming up with batch_size=64:  60%|██████    | 6/10 [00:00<00:00,  9.45it/s]Warming up with batch_size=64:  70%|███████   | 7/10 [00:00<00:00,  9.50it/s]Warming up with batch_size=64:  80%|████████  | 8/10 [00:00<00:00,  9.65it/s]Warming up with batch_size=64:  90%|█████████ | 9/10 [00:00<00:00,  9.66it/s]Warming up with batch_size=64: 100%|██████████| 10/10 [00:01<00:00,  9.73it/s]Warming up with batch_size=64: 100%|██████████| 10/10 [00:01<00:00,  9.60it/s]
Measuring inference with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=64:   1%|          | 1/100 [00:00<00:10,  9.74it/s]Measuring inference with batch_size=64:   2%|▏         | 2/100 [00:00<00:09,  9.88it/s]Measuring inference with batch_size=64:   3%|▎         | 3/100 [00:00<00:09,  9.77it/s]Measuring inference with batch_size=64:   4%|▍         | 4/100 [00:00<00:09,  9.85it/s]Measuring inference with batch_size=64:   5%|▌         | 5/100 [00:00<00:09,  9.78it/s]Measuring inference with batch_size=64:   6%|▌         | 6/100 [00:00<00:09,  9.77it/s]Measuring inference with batch_size=64:   7%|▋         | 7/100 [00:00<00:09,  9.71it/s]Measuring inference with batch_size=64:   8%|▊         | 8/100 [00:00<00:09,  9.78it/s]Measuring inference with batch_size=64:   9%|▉         | 9/100 [00:00<00:09,  9.74it/s]Measuring inference with batch_size=64:  10%|█         | 10/100 [00:01<00:09,  9.74it/s]Measuring inference with batch_size=64:  11%|█         | 11/100 [00:01<00:09,  9.65it/s]Measuring inference with batch_size=64:  12%|█▏        | 12/100 [00:01<00:09,  9.69it/s]Measuring inference with batch_size=64:  13%|█▎        | 13/100 [00:01<00:09,  9.63it/s]Measuring inference with batch_size=64:  14%|█▍        | 14/100 [00:01<00:08,  9.72it/s]Measuring inference with batch_size=64:  15%|█▌        | 15/100 [00:01<00:08,  9.69it/s]Measuring inference with batch_size=64:  16%|█▌        | 16/100 [00:01<00:08,  9.74it/s]Measuring inference with batch_size=64:  17%|█▋        | 17/100 [00:01<00:08,  9.75it/s]Measuring inference with batch_size=64:  18%|█▊        | 18/100 [00:01<00:08,  9.81it/s]Measuring inference with batch_size=64:  19%|█▉        | 19/100 [00:01<00:08,  9.80it/s]Measuring inference with batch_size=64:  20%|██        | 20/100 [00:02<00:08,  9.78it/s]Measuring inference with batch_size=64:  21%|██        | 21/100 [00:02<00:08,  9.84it/s]Measuring inference with batch_size=64:  22%|██▏       | 22/100 [00:02<00:07,  9.81it/s]Measuring inference with batch_size=64:  23%|██▎       | 23/100 [00:02<00:07,  9.87it/s]Measuring inference with batch_size=64:  24%|██▍       | 24/100 [00:02<00:07,  9.89it/s]Measuring inference with batch_size=64:  25%|██▌       | 25/100 [00:02<00:07,  9.89it/s]Measuring inference with batch_size=64:  26%|██▌       | 26/100 [00:02<00:07,  9.73it/s]Measuring inference with batch_size=64:  27%|██▋       | 27/100 [00:02<00:07,  9.77it/s]Measuring inference with batch_size=64:  28%|██▊       | 28/100 [00:02<00:07,  9.79it/s]Measuring inference with batch_size=64:  29%|██▉       | 29/100 [00:02<00:07,  9.84it/s]Measuring inference with batch_size=64:  30%|███       | 30/100 [00:03<00:07,  9.77it/s]Measuring inference with batch_size=64:  31%|███       | 31/100 [00:03<00:07,  9.79it/s]Measuring inference with batch_size=64:  32%|███▏      | 32/100 [00:03<00:06,  9.80it/s]Measuring inference with batch_size=64:  33%|███▎      | 33/100 [00:03<00:06,  9.83it/s]Measuring inference with batch_size=64:  34%|███▍      | 34/100 [00:03<00:06,  9.75it/s]Measuring inference with batch_size=64:  35%|███▌      | 35/100 [00:03<00:06,  9.74it/s]Measuring inference with batch_size=64:  36%|███▌      | 36/100 [00:03<00:06,  9.76it/s]Measuring inference with batch_size=64:  37%|███▋      | 37/100 [00:03<00:06,  9.78it/s]Measuring inference with batch_size=64:  38%|███▊      | 38/100 [00:03<00:06,  9.71it/s]Measuring inference with batch_size=64:  39%|███▉      | 39/100 [00:03<00:06,  9.75it/s]Measuring inference with batch_size=64:  40%|████      | 40/100 [00:04<00:06,  9.75it/s]Measuring inference with batch_size=64:  41%|████      | 41/100 [00:04<00:06,  9.72it/s]Measuring inference with batch_size=64:  42%|████▏     | 42/100 [00:04<00:06,  9.63it/s]Measuring inference with batch_size=64:  43%|████▎     | 43/100 [00:04<00:05,  9.63it/s]Measuring inference with batch_size=64:  44%|████▍     | 44/100 [00:04<00:05,  9.66it/s]Measuring inference with batch_size=64:  45%|████▌     | 45/100 [00:04<00:05,  9.58it/s]Measuring inference with batch_size=64:  46%|████▌     | 46/100 [00:04<00:05,  9.66it/s]Measuring inference with batch_size=64:  47%|████▋     | 47/100 [00:04<00:05,  9.76it/s]Measuring inference with batch_size=64:  48%|████▊     | 48/100 [00:04<00:05,  9.79it/s]Measuring inference with batch_size=64:  49%|████▉     | 49/100 [00:05<00:05,  9.74it/s]Measuring inference with batch_size=64:  50%|█████     | 50/100 [00:05<00:05,  9.80it/s]Measuring inference with batch_size=64:  51%|█████     | 51/100 [00:05<00:04,  9.81it/s]Measuring inference with batch_size=64:  52%|█████▏    | 52/100 [00:05<00:04,  9.84it/s]Measuring inference with batch_size=64:  53%|█████▎    | 53/100 [00:05<00:04,  9.79it/s]Measuring inference with batch_size=64:  54%|█████▍    | 54/100 [00:05<00:04,  9.82it/s]Measuring inference with batch_size=64:  55%|█████▌    | 55/100 [00:05<00:04,  9.83it/s]Measuring inference with batch_size=64:  56%|█████▌    | 56/100 [00:05<00:04,  9.84it/s]Measuring inference with batch_size=64:  57%|█████▋    | 57/100 [00:05<00:04,  9.73it/s]Measuring inference with batch_size=64:  58%|█████▊    | 58/100 [00:05<00:04,  9.74it/s]Measuring inference with batch_size=64:  59%|█████▉    | 59/100 [00:06<00:04,  9.81it/s]Measuring inference with batch_size=64:  60%|██████    | 60/100 [00:06<00:04,  9.85it/s]Measuring inference with batch_size=64:  61%|██████    | 61/100 [00:06<00:03,  9.87it/s]Measuring inference with batch_size=64:  62%|██████▏   | 62/100 [00:06<00:03,  9.90it/s]Measuring inference with batch_size=64:  63%|██████▎   | 63/100 [00:06<00:03,  9.91it/s]Measuring inference with batch_size=64:  65%|██████▌   | 65/100 [00:06<00:03,  9.94it/s]Measuring inference with batch_size=64:  66%|██████▌   | 66/100 [00:06<00:03,  9.93it/s]Measuring inference with batch_size=64:  67%|██████▋   | 67/100 [00:06<00:03,  9.93it/s]Measuring inference with batch_size=64:  68%|██████▊   | 68/100 [00:06<00:03,  9.94it/s]Measuring inference with batch_size=64:  69%|██████▉   | 69/100 [00:07<00:03,  9.92it/s]Measuring inference with batch_size=64:  70%|███████   | 70/100 [00:07<00:03,  9.92it/s]Measuring inference with batch_size=64:  71%|███████   | 71/100 [00:07<00:02,  9.89it/s]Measuring inference with batch_size=64:  72%|███████▏  | 72/100 [00:07<00:02,  9.87it/s]Measuring inference with batch_size=64:  73%|███████▎  | 73/100 [00:07<00:02,  9.84it/s]Measuring inference with batch_size=64:  74%|███████▍  | 74/100 [00:07<00:02,  9.86it/s]Measuring inference with batch_size=64:  75%|███████▌  | 75/100 [00:07<00:02,  9.86it/s]Measuring inference with batch_size=64:  76%|███████▌  | 76/100 [00:07<00:02,  9.87it/s]Measuring inference with batch_size=64:  77%|███████▋  | 77/100 [00:07<00:02,  9.87it/s]Measuring inference with batch_size=64:  78%|███████▊  | 78/100 [00:07<00:02,  9.89it/s]Measuring inference with batch_size=64:  79%|███████▉  | 79/100 [00:08<00:02,  9.89it/s]Measuring inference with batch_size=64:  80%|████████  | 80/100 [00:08<00:02,  9.88it/s]Measuring inference with batch_size=64:  81%|████████  | 81/100 [00:08<00:01,  9.90it/s]Measuring inference with batch_size=64:  82%|████████▏ | 82/100 [00:08<00:01,  9.91it/s]Measuring inference with batch_size=64:  83%|████████▎ | 83/100 [00:08<00:01,  9.93it/s]Measuring inference with batch_size=64:  85%|████████▌ | 85/100 [00:08<00:01,  9.94it/s]Measuring inference with batch_size=64:  86%|████████▌ | 86/100 [00:08<00:01,  9.91it/s]Measuring inference with batch_size=64:  87%|████████▋ | 87/100 [00:08<00:01,  9.87it/s]Measuring inference with batch_size=64:  88%|████████▊ | 88/100 [00:08<00:01,  9.85it/s]Measuring inference with batch_size=64:  89%|████████▉ | 89/100 [00:09<00:01,  9.87it/s]Measuring inference with batch_size=64:  90%|█████████ | 90/100 [00:09<00:01,  9.85it/s]Measuring inference with batch_size=64:  91%|█████████ | 91/100 [00:09<00:00,  9.89it/s]Measuring inference with batch_size=64:  92%|█████████▏| 92/100 [00:09<00:00,  9.91it/s]Measuring inference with batch_size=64:  94%|█████████▍| 94/100 [00:09<00:00,  9.95it/s]Measuring inference with batch_size=64:  95%|█████████▌| 95/100 [00:09<00:00,  9.95it/s]Measuring inference with batch_size=64:  96%|█████████▌| 96/100 [00:09<00:00,  9.96it/s]Measuring inference with batch_size=64:  97%|█████████▋| 97/100 [00:09<00:00,  9.96it/s]Measuring inference with batch_size=64:  98%|█████████▊| 98/100 [00:09<00:00,  9.97it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:10<00:00,  9.97it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:10<00:00,  9.83it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "1.233 \xB5s +/- 0.172 \xB5s [0.715 \xB5s, 1.669 \xB5s]"
      batches_per_second: 829.87 K +/- 138.73 K [599.19 K, 1.40 M]
    metrics:
      batches_per_second_max: 1398101.3333333333
      batches_per_second_mean: 829873.0057142857
      batches_per_second_min: 599186.2857142857
      batches_per_second_std: 138732.43764792744
      seconds_per_batch_max: 1.6689300537109375e-06
      seconds_per_batch_mean: 1.2326240539550781e-06
      seconds_per_batch_min: 7.152557373046875e-07
      seconds_per_batch_std: 1.721078312232394e-07
  gpu_to_cpu:
    human_readable:
      batch_latency: "39.475 \xB5s +/- 4.910 \xB5s [35.524 \xB5s, 61.989 \xB5s]"
      batches_per_second: 25.63 K +/- 2.44 K [16.13 K, 28.15 K]
    metrics:
      batches_per_second_max: 28149.691275167785
      batches_per_second_mean: 25633.450804418375
      batches_per_second_min: 16131.938461538462
      batches_per_second_std: 2441.510792810971
      seconds_per_batch_max: 6.198883056640625e-05
      seconds_per_batch_mean: 3.94749641418457e-05
      seconds_per_batch_min: 3.552436828613281e-05
      seconds_per_batch_std: 4.909715888271992e-06
  on_device_inference:
    human_readable:
      batch_latency: 101.636 ms +/- 1.596 ms [99.736 ms, 106.432 ms]
      batches_per_second: 9.84 +/- 0.15 [9.40, 10.03]
    metrics:
      batches_per_second_max: 10.02644839884874
      batches_per_second_mean: 9.84142179356633
      batches_per_second_min: 9.395694959980466
      batches_per_second_std: 0.15175024094315617
      seconds_per_batch_max: 0.10643172264099121
      seconds_per_batch_mean: 0.10163594007492066
      seconds_per_batch_min: 0.09973621368408203
      seconds_per_batch_std: 0.001596027543920966
  total:
    human_readable:
      batch_latency: 101.677 ms +/- 1.597 ms [99.775 ms, 106.484 ms]
      batches_per_second: 9.84 +/- 0.15 [9.39, 10.02]
    metrics:
      batches_per_second_max: 10.022591012820053
      batches_per_second_mean: 9.837483599676
      batches_per_second_min: 9.391045792023787
      batches_per_second_std: 0.15175051683885565
      seconds_per_batch_max: 0.10648441314697266
      seconds_per_batch_mean: 0.10167664766311646
      seconds_per_batch_min: 0.09977459907531738
      seconds_per_batch_std: 0.0015973311446048943

Warming up with batch_size=64:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=64:  10%|█         | 1/10 [00:00<00:03,  2.83it/s]Warming up with batch_size=64:  20%|██        | 2/10 [00:00<00:02,  2.97it/s]Warming up with batch_size=64:  30%|███       | 3/10 [00:01<00:02,  3.02it/s]Warming up with batch_size=64:  40%|████      | 4/10 [00:01<00:01,  3.05it/s]Warming up with batch_size=64:  50%|█████     | 5/10 [00:01<00:01,  3.06it/s]Warming up with batch_size=64:  60%|██████    | 6/10 [00:01<00:01,  3.06it/s]Warming up with batch_size=64:  70%|███████   | 7/10 [00:02<00:00,  3.07it/s]Warming up with batch_size=64:  80%|████████  | 8/10 [00:02<00:00,  3.07it/s]Warming up with batch_size=64:  90%|█████████ | 9/10 [00:02<00:00,  3.06it/s]Warming up with batch_size=64: 100%|██████████| 10/10 [00:03<00:00,  3.06it/s]Warming up with batch_size=64: 100%|██████████| 10/10 [00:03<00:00,  3.05it/s]
Measuring inference with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=64:   1%|          | 1/100 [00:00<00:32,  3.08it/s]Measuring inference with batch_size=64:   2%|▏         | 2/100 [00:00<00:31,  3.08it/s]Measuring inference with batch_size=64:   3%|▎         | 3/100 [00:00<00:31,  3.08it/s]Measuring inference with batch_size=64:   4%|▍         | 4/100 [00:01<00:31,  3.09it/s]Measuring inference with batch_size=64:   5%|▌         | 5/100 [00:01<00:30,  3.08it/s]Measuring inference with batch_size=64:   6%|▌         | 6/100 [00:01<00:30,  3.07it/s]Measuring inference with batch_size=64:   7%|▋         | 7/100 [00:02<00:30,  3.07it/s]Measuring inference with batch_size=64:   8%|▊         | 8/100 [00:02<00:29,  3.07it/s]Measuring inference with batch_size=64:   9%|▉         | 9/100 [00:02<00:29,  3.06it/s]Measuring inference with batch_size=64:  10%|█         | 10/100 [00:03<00:29,  3.06it/s]Measuring inference with batch_size=64:  11%|█         | 11/100 [00:03<00:28,  3.07it/s]Measuring inference with batch_size=64:  12%|█▏        | 12/100 [00:03<00:28,  3.07it/s]Measuring inference with batch_size=64:  13%|█▎        | 13/100 [00:04<00:28,  3.07it/s]Measuring inference with batch_size=64:  14%|█▍        | 14/100 [00:04<00:28,  3.07it/s]Measuring inference with batch_size=64:  15%|█▌        | 15/100 [00:04<00:27,  3.07it/s]Measuring inference with batch_size=64:  16%|█▌        | 16/100 [00:05<00:27,  3.07it/s]Measuring inference with batch_size=64:  17%|█▋        | 17/100 [00:05<00:26,  3.07it/s]Measuring inference with batch_size=64:  18%|█▊        | 18/100 [00:05<00:26,  3.07it/s]Measuring inference with batch_size=64:  19%|█▉        | 19/100 [00:06<00:26,  3.07it/s]Measuring inference with batch_size=64:  20%|██        | 20/100 [00:06<00:26,  3.07it/s]Measuring inference with batch_size=64:  21%|██        | 21/100 [00:06<00:25,  3.07it/s]Measuring inference with batch_size=64:  22%|██▏       | 22/100 [00:07<00:25,  3.07it/s]Measuring inference with batch_size=64:  23%|██▎       | 23/100 [00:07<00:25,  3.07it/s]Measuring inference with batch_size=64:  24%|██▍       | 24/100 [00:07<00:24,  3.06it/s]Measuring inference with batch_size=64:  25%|██▌       | 25/100 [00:08<00:24,  3.06it/s]Measuring inference with batch_size=64:  26%|██▌       | 26/100 [00:08<00:24,  3.07it/s]Measuring inference with batch_size=64:  27%|██▋       | 27/100 [00:08<00:23,  3.07it/s]Measuring inference with batch_size=64:  28%|██▊       | 28/100 [00:09<00:23,  3.08it/s]Measuring inference with batch_size=64:  29%|██▉       | 29/100 [00:09<00:23,  3.07it/s]Measuring inference with batch_size=64:  30%|███       | 30/100 [00:09<00:22,  3.08it/s]Measuring inference with batch_size=64:  31%|███       | 31/100 [00:10<00:22,  3.08it/s]Measuring inference with batch_size=64:  32%|███▏      | 32/100 [00:10<00:22,  3.07it/s]Measuring inference with batch_size=64:  33%|███▎      | 33/100 [00:10<00:21,  3.07it/s]Measuring inference with batch_size=64:  34%|███▍      | 34/100 [00:11<00:21,  3.07it/s]Measuring inference with batch_size=64:  35%|███▌      | 35/100 [00:11<00:21,  3.07it/s]Measuring inference with batch_size=64:  36%|███▌      | 36/100 [00:11<00:20,  3.06it/s]Measuring inference with batch_size=64:  37%|███▋      | 37/100 [00:12<00:20,  3.06it/s]Measuring inference with batch_size=64:  38%|███▊      | 38/100 [00:12<00:20,  3.07it/s]Measuring inference with batch_size=64:  39%|███▉      | 39/100 [00:12<00:19,  3.06it/s]Measuring inference with batch_size=64:  40%|████      | 40/100 [00:13<00:19,  3.05it/s]Measuring inference with batch_size=64:  41%|████      | 41/100 [00:13<00:19,  3.06it/s]Measuring inference with batch_size=64:  42%|████▏     | 42/100 [00:13<00:18,  3.06it/s]Measuring inference with batch_size=64:  43%|████▎     | 43/100 [00:14<00:18,  3.07it/s]Measuring inference with batch_size=64:  44%|████▍     | 44/100 [00:14<00:18,  3.07it/s]Measuring inference with batch_size=64:  45%|████▌     | 45/100 [00:14<00:17,  3.06it/s]Measuring inference with batch_size=64:  46%|████▌     | 46/100 [00:14<00:17,  3.07it/s]Measuring inference with batch_size=64:  47%|████▋     | 47/100 [00:15<00:17,  3.08it/s]Measuring inference with batch_size=64:  48%|████▊     | 48/100 [00:15<00:16,  3.08it/s]Measuring inference with batch_size=64:  49%|████▉     | 49/100 [00:15<00:16,  3.07it/s]Measuring inference with batch_size=64:  50%|█████     | 50/100 [00:16<00:16,  3.07it/s]Measuring inference with batch_size=64:  51%|█████     | 51/100 [00:16<00:15,  3.07it/s]Measuring inference with batch_size=64:  52%|█████▏    | 52/100 [00:16<00:15,  3.07it/s]Measuring inference with batch_size=64:  53%|█████▎    | 53/100 [00:17<00:15,  3.07it/s]Measuring inference with batch_size=64:  54%|█████▍    | 54/100 [00:17<00:14,  3.07it/s]Measuring inference with batch_size=64:  55%|█████▌    | 55/100 [00:17<00:14,  3.07it/s]Measuring inference with batch_size=64:  56%|█████▌    | 56/100 [00:18<00:14,  3.08it/s]Measuring inference with batch_size=64:  57%|█████▋    | 57/100 [00:18<00:13,  3.08it/s]Measuring inference with batch_size=64:  58%|█████▊    | 58/100 [00:18<00:13,  3.07it/s]Measuring inference with batch_size=64:  59%|█████▉    | 59/100 [00:19<00:13,  3.07it/s]Measuring inference with batch_size=64:  60%|██████    | 60/100 [00:19<00:13,  3.08it/s]Measuring inference with batch_size=64:  61%|██████    | 61/100 [00:19<00:12,  3.08it/s]Measuring inference with batch_size=64:  62%|██████▏   | 62/100 [00:20<00:12,  3.08it/s]Measuring inference with batch_size=64:  63%|██████▎   | 63/100 [00:20<00:12,  3.07it/s]Measuring inference with batch_size=64:  64%|██████▍   | 64/100 [00:20<00:11,  3.07it/s]Measuring inference with batch_size=64:  65%|██████▌   | 65/100 [00:21<00:11,  3.07it/s]Measuring inference with batch_size=64:  66%|██████▌   | 66/100 [00:21<00:11,  3.07it/s]Measuring inference with batch_size=64:  67%|██████▋   | 67/100 [00:21<00:10,  3.07it/s]Measuring inference with batch_size=64:  68%|██████▊   | 68/100 [00:22<00:10,  3.07it/s]Measuring inference with batch_size=64:  69%|██████▉   | 69/100 [00:22<00:10,  3.07it/s]Measuring inference with batch_size=64:  70%|███████   | 70/100 [00:22<00:09,  3.07it/s]Measuring inference with batch_size=64:  71%|███████   | 71/100 [00:23<00:09,  3.07it/s]Measuring inference with batch_size=64:  72%|███████▏  | 72/100 [00:23<00:09,  3.07it/s]Measuring inference with batch_size=64:  73%|███████▎  | 73/100 [00:23<00:08,  3.06it/s]Measuring inference with batch_size=64:  74%|███████▍  | 74/100 [00:24<00:08,  3.06it/s]Measuring inference with batch_size=64:  75%|███████▌  | 75/100 [00:24<00:08,  3.06it/s]Measuring inference with batch_size=64:  76%|███████▌  | 76/100 [00:24<00:07,  3.06it/s]Measuring inference with batch_size=64:  77%|███████▋  | 77/100 [00:25<00:07,  3.06it/s]Measuring inference with batch_size=64:  78%|███████▊  | 78/100 [00:25<00:07,  3.06it/s]Measuring inference with batch_size=64:  79%|███████▉  | 79/100 [00:25<00:06,  3.07it/s]Measuring inference with batch_size=64:  80%|████████  | 80/100 [00:26<00:06,  3.07it/s]Measuring inference with batch_size=64:  81%|████████  | 81/100 [00:26<00:06,  3.07it/s]Measuring inference with batch_size=64:  82%|████████▏ | 82/100 [00:26<00:05,  3.07it/s]Measuring inference with batch_size=64:  83%|████████▎ | 83/100 [00:27<00:05,  3.07it/s]Measuring inference with batch_size=64:  84%|████████▍ | 84/100 [00:27<00:05,  3.06it/s]Measuring inference with batch_size=64:  85%|████████▌ | 85/100 [00:27<00:04,  3.06it/s]Measuring inference with batch_size=64:  86%|████████▌ | 86/100 [00:28<00:04,  3.07it/s]Measuring inference with batch_size=64:  87%|████████▋ | 87/100 [00:28<00:04,  3.07it/s]Measuring inference with batch_size=64:  88%|████████▊ | 88/100 [00:28<00:03,  3.07it/s]Measuring inference with batch_size=64:  89%|████████▉ | 89/100 [00:28<00:03,  3.07it/s]Measuring inference with batch_size=64:  90%|█████████ | 90/100 [00:29<00:03,  3.08it/s]Measuring inference with batch_size=64:  91%|█████████ | 91/100 [00:29<00:02,  3.07it/s]Measuring inference with batch_size=64:  92%|█████████▏| 92/100 [00:29<00:02,  3.07it/s]Measuring inference with batch_size=64:  93%|█████████▎| 93/100 [00:30<00:02,  3.07it/s]Measuring inference with batch_size=64:  94%|█████████▍| 94/100 [00:30<00:01,  3.07it/s]Measuring inference with batch_size=64:  95%|█████████▌| 95/100 [00:30<00:01,  3.07it/s]Measuring inference with batch_size=64:  96%|█████████▌| 96/100 [00:31<00:01,  3.07it/s]Measuring inference with batch_size=64:  97%|█████████▋| 97/100 [00:31<00:00,  3.08it/s]Measuring inference with batch_size=64:  98%|█████████▊| 98/100 [00:31<00:00,  3.07it/s]Measuring inference with batch_size=64:  99%|█████████▉| 99/100 [00:32<00:00,  3.07it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:32<00:00,  3.07it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:32<00:00,  3.07it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Timing results (batch_size=64):
  cpu_to_gpu:
    human_readable:
      batch_latency: "1.571 \xB5s +/- 0.208 \xB5s [1.192 \xB5s, 2.384 \xB5s]"
      batches_per_second: 646.99 K +/- 80.92 K [419.43 K, 838.86 K]
    metrics:
      batches_per_second_max: 838860.8
      batches_per_second_mean: 646988.036063492
      batches_per_second_min: 419430.4
      batches_per_second_std: 80918.25366280242
      seconds_per_batch_max: 2.384185791015625e-06
      seconds_per_batch_mean: 1.5711784362792968e-06
      seconds_per_batch_min: 1.1920928955078125e-06
      seconds_per_batch_std: 2.0810814695741643e-07
  gpu_to_cpu:
    human_readable:
      batch_latency: "1.033 ms +/- 18.714 \xB5s [1.001 ms, 1.089 ms]"
      batches_per_second: 968.45 +/- 17.28 [918.59, 999.12]
    metrics:
      batches_per_second_max: 999.1195807527394
      batches_per_second_mean: 968.4534323700587
      batches_per_second_min: 918.5948313622426
      batches_per_second_std: 17.277352144772266
      seconds_per_batch_max: 0.0010886192321777344
      seconds_per_batch_mean: 0.0010329079627990722
      seconds_per_batch_min: 0.0010008811950683594
      seconds_per_batch_std: 1.8713597294712536e-05
  on_device_inference:
    human_readable:
      batch_latency: 324.598 ms +/- 1.424 ms [321.775 ms, 329.051 ms]
      batches_per_second: 3.08 +/- 0.01 [3.04, 3.11]
    metrics:
      batches_per_second_max: 3.107764327911317
      batches_per_second_mean: 3.0807968234270824
      batches_per_second_min: 3.0390468199556855
      batches_per_second_std: 0.013467826604246199
      seconds_per_batch_max: 0.32905054092407227
      seconds_per_batch_mean: 0.32459757566452024
      seconds_per_batch_min: 0.3217747211456299
      seconds_per_batch_std: 0.0014241668929862907
  total:
    human_readable:
      batch_latency: 325.632 ms +/- 1.425 ms [322.796 ms, 330.108 ms]
      batches_per_second: 3.07 +/- 0.01 [3.03, 3.10]
    metrics:
      batches_per_second_max: 3.0979330883134995
      batches_per_second_mean: 3.071009353119672
      batches_per_second_min: 3.029307910369608
      batches_per_second_std: 0.013392004935947957
      seconds_per_batch_max: 0.3301084041595459
      seconds_per_batch_mean: 0.32563205480575563
      seconds_per_batch_min: 0.3227958679199219
      seconds_per_batch_std: 0.0014252097055644285

learner.infer:
  device: cuda
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 52
        total: 104
      frequency: 2.10 GHz
      model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
    gpus:
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    memory:
      available: 177.86 GB
      total: 187.56 GB
      used: 8.32 GB
    system:
      node: charybdis
      release: 4.15.0-167-generic
      system: Linux
  max_inference_memory: 7223430656
  post_inference_memory: 7198198784
  pre_inference_memory: 3423591424
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "1.233 \xB5s +/- 0.172 \xB5s [0.715 \xB5s, 1.669 \xB5s]"
          batches_per_second: 829.87 K +/- 138.73 K [599.19 K, 1.40 M]
        metrics:
          batches_per_second_max: 1398101.3333333333
          batches_per_second_mean: 829873.0057142857
          batches_per_second_min: 599186.2857142857
          batches_per_second_std: 138732.43764792744
          seconds_per_batch_max: 1.6689300537109375e-06
          seconds_per_batch_mean: 1.2326240539550781e-06
          seconds_per_batch_min: 7.152557373046875e-07
          seconds_per_batch_std: 1.721078312232394e-07
      gpu_to_cpu:
        human_readable:
          batch_latency: "39.475 \xB5s +/- 4.910 \xB5s [35.524 \xB5s, 61.989 \xB5\
            s]"
          batches_per_second: 25.63 K +/- 2.44 K [16.13 K, 28.15 K]
        metrics:
          batches_per_second_max: 28149.691275167785
          batches_per_second_mean: 25633.450804418375
          batches_per_second_min: 16131.938461538462
          batches_per_second_std: 2441.510792810971
          seconds_per_batch_max: 6.198883056640625e-05
          seconds_per_batch_mean: 3.94749641418457e-05
          seconds_per_batch_min: 3.552436828613281e-05
          seconds_per_batch_std: 4.909715888271992e-06
      on_device_inference:
        human_readable:
          batch_latency: 101.636 ms +/- 1.596 ms [99.736 ms, 106.432 ms]
          batches_per_second: 9.84 +/- 0.15 [9.40, 10.03]
        metrics:
          batches_per_second_max: 10.02644839884874
          batches_per_second_mean: 9.84142179356633
          batches_per_second_min: 9.395694959980466
          batches_per_second_std: 0.15175024094315617
          seconds_per_batch_max: 0.10643172264099121
          seconds_per_batch_mean: 0.10163594007492066
          seconds_per_batch_min: 0.09973621368408203
          seconds_per_batch_std: 0.001596027543920966
      total:
        human_readable:
          batch_latency: 101.677 ms +/- 1.597 ms [99.775 ms, 106.484 ms]
          batches_per_second: 9.84 +/- 0.15 [9.39, 10.02]
        metrics:
          batches_per_second_max: 10.022591012820053
          batches_per_second_mean: 9.837483599676
          batches_per_second_min: 9.391045792023787
          batches_per_second_std: 0.15175051683885565
          seconds_per_batch_max: 0.10648441314697266
          seconds_per_batch_mean: 0.10167664766311646
          seconds_per_batch_min: 0.09977459907531738
          seconds_per_batch_std: 0.0015973311446048943
    batch_size_64:
      cpu_to_gpu:
        human_readable:
          batch_latency: "1.571 \xB5s +/- 0.208 \xB5s [1.192 \xB5s, 2.384 \xB5s]"
          batches_per_second: 646.99 K +/- 80.92 K [419.43 K, 838.86 K]
        metrics:
          batches_per_second_max: 838860.8
          batches_per_second_mean: 646988.036063492
          batches_per_second_min: 419430.4
          batches_per_second_std: 80918.25366280242
          seconds_per_batch_max: 2.384185791015625e-06
          seconds_per_batch_mean: 1.5711784362792968e-06
          seconds_per_batch_min: 1.1920928955078125e-06
          seconds_per_batch_std: 2.0810814695741643e-07
      gpu_to_cpu:
        human_readable:
          batch_latency: "1.033 ms +/- 18.714 \xB5s [1.001 ms, 1.089 ms]"
          batches_per_second: 968.45 +/- 17.28 [918.59, 999.12]
        metrics:
          batches_per_second_max: 999.1195807527394
          batches_per_second_mean: 968.4534323700587
          batches_per_second_min: 918.5948313622426
          batches_per_second_std: 17.277352144772266
          seconds_per_batch_max: 0.0010886192321777344
          seconds_per_batch_mean: 0.0010329079627990722
          seconds_per_batch_min: 0.0010008811950683594
          seconds_per_batch_std: 1.8713597294712536e-05
      on_device_inference:
        human_readable:
          batch_latency: 324.598 ms +/- 1.424 ms [321.775 ms, 329.051 ms]
          batches_per_second: 3.08 +/- 0.01 [3.04, 3.11]
        metrics:
          batches_per_second_max: 3.107764327911317
          batches_per_second_mean: 3.0807968234270824
          batches_per_second_min: 3.0390468199556855
          batches_per_second_std: 0.013467826604246199
          seconds_per_batch_max: 0.32905054092407227
          seconds_per_batch_mean: 0.32459757566452024
          seconds_per_batch_min: 0.3217747211456299
          seconds_per_batch_std: 0.0014241668929862907
      total:
        human_readable:
          batch_latency: 325.632 ms +/- 1.425 ms [322.796 ms, 330.108 ms]
          batches_per_second: 3.07 +/- 0.01 [3.03, 3.10]
        metrics:
          batches_per_second_max: 3.0979330883134995
          batches_per_second_mean: 3.071009353119672
          batches_per_second_min: 3.029307910369608
          batches_per_second_std: 0.013392004935947957
          seconds_per_batch_max: 0.3301084041595459
          seconds_per_batch_mean: 0.32563205480575563
          seconds_per_batch_min: 0.3227958679199219
          seconds_per_batch_std: 0.0014252097055644285

== Benchmarking model directly ==
Machine info:
  cpu:
    architecture: x86_64
    cores:
      physical: 52
      total: 104
    frequency: 2.10 GHz
    model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
  gpus:
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  memory:
    available: 177.81 GB
    total: 187.56 GB
    used: 8.37 GB
  system:
    node: charybdis
    release: 4.15.0-167-generic
    system: Linux

Model device: cuda:0
Model parameters: 3794322 (3.79 M)
Model FLOPs: 138232160 (138.23 M)
Allocated GPU memory prior to inference: 73803264 (70.38 MB)
Allocated GPU memory after to inference: 3791818240 (3.53 GB)
Max allocated GPU memory during inference: 3817033216 (3.55 GB)
Warming up with batch_size=64:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=64:  10%|█         | 1/10 [00:00<00:00,  9.87it/s]Warming up with batch_size=64:  30%|███       | 3/10 [00:00<00:00, 10.29it/s]Warming up with batch_size=64:  50%|█████     | 5/10 [00:00<00:00, 10.34it/s]Warming up with batch_size=64:  70%|███████   | 7/10 [00:00<00:00, 10.23it/s]Warming up with batch_size=64:  90%|█████████ | 9/10 [00:00<00:00, 10.25it/s]Warming up with batch_size=64: 100%|██████████| 10/10 [00:00<00:00, 10.26it/s]
Measuring inference with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=64:   2%|▏         | 2/100 [00:00<00:09, 10.31it/s]Measuring inference with batch_size=64:   4%|▍         | 4/100 [00:00<00:09, 10.31it/s]Measuring inference with batch_size=64:   6%|▌         | 6/100 [00:00<00:09, 10.31it/s]Measuring inference with batch_size=64:   8%|▊         | 8/100 [00:00<00:08, 10.31it/s]Measuring inference with batch_size=64:  10%|█         | 10/100 [00:00<00:08, 10.23it/s]Measuring inference with batch_size=64:  12%|█▏        | 12/100 [00:01<00:08, 10.22it/s]Measuring inference with batch_size=64:  14%|█▍        | 14/100 [00:01<00:08, 10.15it/s]Measuring inference with batch_size=64:  16%|█▌        | 16/100 [00:01<00:08, 10.18it/s]Measuring inference with batch_size=64:  18%|█▊        | 18/100 [00:01<00:08, 10.24it/s]Measuring inference with batch_size=64:  20%|██        | 20/100 [00:01<00:07, 10.27it/s]Measuring inference with batch_size=64:  22%|██▏       | 22/100 [00:02<00:07, 10.25it/s]Measuring inference with batch_size=64:  24%|██▍       | 24/100 [00:02<00:07, 10.28it/s]Measuring inference with batch_size=64:  26%|██▌       | 26/100 [00:02<00:07, 10.31it/s]Measuring inference with batch_size=64:  28%|██▊       | 28/100 [00:02<00:06, 10.30it/s]Measuring inference with batch_size=64:  30%|███       | 30/100 [00:02<00:06, 10.32it/s]Measuring inference with batch_size=64:  32%|███▏      | 32/100 [00:03<00:06, 10.33it/s]Measuring inference with batch_size=64:  34%|███▍      | 34/100 [00:03<00:06, 10.28it/s]Measuring inference with batch_size=64:  36%|███▌      | 36/100 [00:03<00:06, 10.30it/s]Measuring inference with batch_size=64:  38%|███▊      | 38/100 [00:03<00:06, 10.32it/s]Measuring inference with batch_size=64:  40%|████      | 40/100 [00:03<00:05, 10.27it/s]Measuring inference with batch_size=64:  42%|████▏     | 42/100 [00:04<00:05, 10.27it/s]Measuring inference with batch_size=64:  44%|████▍     | 44/100 [00:04<00:05, 10.25it/s]Measuring inference with batch_size=64:  46%|████▌     | 46/100 [00:04<00:05, 10.26it/s]Measuring inference with batch_size=64:  48%|████▊     | 48/100 [00:04<00:05, 10.22it/s]Measuring inference with batch_size=64:  50%|█████     | 50/100 [00:04<00:04, 10.24it/s]Measuring inference with batch_size=64:  52%|█████▏    | 52/100 [00:05<00:04, 10.25it/s]Measuring inference with batch_size=64:  54%|█████▍    | 54/100 [00:05<00:04, 10.25it/s]Measuring inference with batch_size=64:  56%|█████▌    | 56/100 [00:05<00:04, 10.26it/s]Measuring inference with batch_size=64:  58%|█████▊    | 58/100 [00:05<00:04, 10.28it/s]Measuring inference with batch_size=64:  60%|██████    | 60/100 [00:05<00:03, 10.23it/s]Measuring inference with batch_size=64:  62%|██████▏   | 62/100 [00:06<00:03, 10.26it/s]Measuring inference with batch_size=64:  64%|██████▍   | 64/100 [00:06<00:03, 10.28it/s]Measuring inference with batch_size=64:  66%|██████▌   | 66/100 [00:06<00:03, 10.28it/s]Measuring inference with batch_size=64:  68%|██████▊   | 68/100 [00:06<00:03, 10.29it/s]Measuring inference with batch_size=64:  70%|███████   | 70/100 [00:06<00:02, 10.25it/s]Measuring inference with batch_size=64:  72%|███████▏  | 72/100 [00:07<00:02, 10.28it/s]Measuring inference with batch_size=64:  74%|███████▍  | 74/100 [00:07<00:02, 10.27it/s]Measuring inference with batch_size=64:  76%|███████▌  | 76/100 [00:07<00:02, 10.28it/s]Measuring inference with batch_size=64:  78%|███████▊  | 78/100 [00:07<00:02, 10.31it/s]Measuring inference with batch_size=64:  80%|████████  | 80/100 [00:07<00:01, 10.32it/s]Measuring inference with batch_size=64:  82%|████████▏ | 82/100 [00:07<00:01, 10.33it/s]Measuring inference with batch_size=64:  84%|████████▍ | 84/100 [00:08<00:01, 10.34it/s]Measuring inference with batch_size=64:  86%|████████▌ | 86/100 [00:08<00:01, 10.28it/s]Measuring inference with batch_size=64:  88%|████████▊ | 88/100 [00:08<00:01, 10.29it/s]Measuring inference with batch_size=64:  90%|█████████ | 90/100 [00:08<00:00, 10.31it/s]Measuring inference with batch_size=64:  92%|█████████▏| 92/100 [00:08<00:00, 10.27it/s]Measuring inference with batch_size=64:  94%|█████████▍| 94/100 [00:09<00:00, 10.26it/s]Measuring inference with batch_size=64:  96%|█████████▌| 96/100 [00:09<00:00, 10.28it/s]Measuring inference with batch_size=64:  98%|█████████▊| 98/100 [00:09<00:00, 10.30it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:09<00:00, 10.32it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:09<00:00, 10.28it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "75.395 \xB5s +/- 11.082 \xB5s [66.757 \xB5s, 169.754 \xB5s]"
      batches_per_second: 13.43 K +/- 1.25 K [5.89 K, 14.98 K]
    metrics:
      batches_per_second_max: 14979.657142857142
      batches_per_second_mean: 13434.340824134684
      batches_per_second_min: 5890.876404494382
      batches_per_second_std: 1253.1564786071149
      seconds_per_batch_max: 0.0001697540283203125
      seconds_per_batch_mean: 7.539510726928711e-05
      seconds_per_batch_min: 6.67572021484375e-05
      seconds_per_batch_std: 1.1082020504909663e-05
  gpu_to_cpu:
    human_readable:
      batch_latency: "32.966 \xB5s +/- 1.053 \xB5s [31.948 \xB5s, 39.816 \xB5s]"
      batches_per_second: 30.36 K +/- 883.22 [25.12 K, 31.30 K]
    metrics:
      batches_per_second_max: 31300.776119402984
      batches_per_second_mean: 30362.276332567217
      batches_per_second_min: 25115.59281437126
      batches_per_second_std: 883.222889807004
      seconds_per_batch_max: 3.981590270996094e-05
      seconds_per_batch_mean: 3.296613693237305e-05
      seconds_per_batch_min: 3.1948089599609375e-05
      seconds_per_batch_std: 1.0527496482407676e-06
  on_device_inference:
    human_readable:
      batch_latency: 97.140 ms +/- 1.042 ms [95.613 ms, 100.832 ms]
      batches_per_second: 10.30 +/- 0.11 [9.92, 10.46]
    metrics:
      batches_per_second_max: 10.458880676660982
      batches_per_second_mean: 10.295575473946313
      batches_per_second_min: 9.917511391070157
      batches_per_second_std: 0.10855576338128063
      seconds_per_batch_max: 0.10083174705505371
      seconds_per_batch_mean: 0.09714008331298828
      seconds_per_batch_min: 0.0956125259399414
      seconds_per_batch_std: 0.0010415777344963061
  total:
    human_readable:
      batch_latency: 97.248 ms +/- 1.041 ms [95.731 ms, 100.935 ms]
      batches_per_second: 10.28 +/- 0.11 [9.91, 10.45]
    metrics:
      batches_per_second_max: 10.445960889012861
      batches_per_second_mean: 10.284098843298182
      batches_per_second_min: 9.907344461950192
      batches_per_second_std: 0.10822064130316582
      seconds_per_batch_max: 0.10093522071838379
      seconds_per_batch_mean: 0.09724844455718994
      seconds_per_batch_min: 0.09573078155517578
      seconds_per_batch_std: 0.0010407073791778205

Warming up with batch_size=64:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=64:  10%|█         | 1/10 [00:00<00:03,  2.99it/s]Warming up with batch_size=64:  20%|██        | 2/10 [00:00<00:02,  3.09it/s]Warming up with batch_size=64:  30%|███       | 3/10 [00:00<00:02,  3.12it/s]Warming up with batch_size=64:  40%|████      | 4/10 [00:01<00:01,  3.13it/s]Warming up with batch_size=64:  50%|█████     | 5/10 [00:01<00:01,  3.14it/s]Warming up with batch_size=64:  60%|██████    | 6/10 [00:01<00:01,  3.14it/s]Warming up with batch_size=64:  70%|███████   | 7/10 [00:02<00:00,  3.15it/s]Warming up with batch_size=64:  80%|████████  | 8/10 [00:02<00:00,  3.15it/s]Warming up with batch_size=64:  90%|█████████ | 9/10 [00:02<00:00,  3.15it/s]Warming up with batch_size=64: 100%|██████████| 10/10 [00:03<00:00,  3.15it/s]Warming up with batch_size=64: 100%|██████████| 10/10 [00:03<00:00,  3.14it/s]
Measuring inference with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=64:   1%|          | 1/100 [00:00<00:31,  3.13it/s]Measuring inference with batch_size=64:   2%|▏         | 2/100 [00:00<00:31,  3.14it/s]Measuring inference with batch_size=64:   3%|▎         | 3/100 [00:00<00:30,  3.15it/s]Measuring inference with batch_size=64:   4%|▍         | 4/100 [00:01<00:30,  3.15it/s]Measuring inference with batch_size=64:   5%|▌         | 5/100 [00:01<00:30,  3.15it/s]Measuring inference with batch_size=64:   6%|▌         | 6/100 [00:01<00:29,  3.16it/s]Measuring inference with batch_size=64:   7%|▋         | 7/100 [00:02<00:29,  3.16it/s]Measuring inference with batch_size=64:   8%|▊         | 8/100 [00:02<00:29,  3.16it/s]Measuring inference with batch_size=64:   9%|▉         | 9/100 [00:02<00:28,  3.16it/s]Measuring inference with batch_size=64:  10%|█         | 10/100 [00:03<00:28,  3.15it/s]Measuring inference with batch_size=64:  11%|█         | 11/100 [00:03<00:28,  3.16it/s]Measuring inference with batch_size=64:  12%|█▏        | 12/100 [00:03<00:27,  3.15it/s]Measuring inference with batch_size=64:  13%|█▎        | 13/100 [00:04<00:27,  3.16it/s]Measuring inference with batch_size=64:  14%|█▍        | 14/100 [00:04<00:27,  3.16it/s]Measuring inference with batch_size=64:  15%|█▌        | 15/100 [00:04<00:26,  3.16it/s]Measuring inference with batch_size=64:  16%|█▌        | 16/100 [00:05<00:26,  3.16it/s]Measuring inference with batch_size=64:  17%|█▋        | 17/100 [00:05<00:26,  3.16it/s]Measuring inference with batch_size=64:  18%|█▊        | 18/100 [00:05<00:25,  3.16it/s]Measuring inference with batch_size=64:  19%|█▉        | 19/100 [00:06<00:25,  3.16it/s]Measuring inference with batch_size=64:  20%|██        | 20/100 [00:06<00:25,  3.16it/s]Measuring inference with batch_size=64:  21%|██        | 21/100 [00:06<00:25,  3.16it/s]Measuring inference with batch_size=64:  22%|██▏       | 22/100 [00:06<00:24,  3.16it/s]Measuring inference with batch_size=64:  23%|██▎       | 23/100 [00:07<00:24,  3.15it/s]Measuring inference with batch_size=64:  24%|██▍       | 24/100 [00:07<00:24,  3.15it/s]Measuring inference with batch_size=64:  25%|██▌       | 25/100 [00:07<00:23,  3.16it/s]Measuring inference with batch_size=64:  26%|██▌       | 26/100 [00:08<00:23,  3.16it/s]Measuring inference with batch_size=64:  27%|██▋       | 27/100 [00:08<00:23,  3.16it/s]Measuring inference with batch_size=64:  28%|██▊       | 28/100 [00:08<00:22,  3.16it/s]Measuring inference with batch_size=64:  29%|██▉       | 29/100 [00:09<00:22,  3.15it/s]Measuring inference with batch_size=64:  30%|███       | 30/100 [00:09<00:22,  3.15it/s]Measuring inference with batch_size=64:  31%|███       | 31/100 [00:09<00:21,  3.15it/s]Measuring inference with batch_size=64:  32%|███▏      | 32/100 [00:10<00:21,  3.15it/s]Measuring inference with batch_size=64:  33%|███▎      | 33/100 [00:10<00:21,  3.15it/s]Measuring inference with batch_size=64:  34%|███▍      | 34/100 [00:10<00:20,  3.15it/s]Measuring inference with batch_size=64:  35%|███▌      | 35/100 [00:11<00:20,  3.15it/s]Measuring inference with batch_size=64:  36%|███▌      | 36/100 [00:11<00:20,  3.15it/s]Measuring inference with batch_size=64:  37%|███▋      | 37/100 [00:11<00:19,  3.15it/s]Measuring inference with batch_size=64:  38%|███▊      | 38/100 [00:12<00:19,  3.15it/s]Measuring inference with batch_size=64:  39%|███▉      | 39/100 [00:12<00:19,  3.15it/s]Measuring inference with batch_size=64:  40%|████      | 40/100 [00:12<00:19,  3.15it/s]Measuring inference with batch_size=64:  41%|████      | 41/100 [00:12<00:18,  3.15it/s]Measuring inference with batch_size=64:  42%|████▏     | 42/100 [00:13<00:18,  3.15it/s]Measuring inference with batch_size=64:  43%|████▎     | 43/100 [00:13<00:18,  3.15it/s]Measuring inference with batch_size=64:  44%|████▍     | 44/100 [00:13<00:17,  3.15it/s]Measuring inference with batch_size=64:  45%|████▌     | 45/100 [00:14<00:17,  3.15it/s]Measuring inference with batch_size=64:  46%|████▌     | 46/100 [00:14<00:17,  3.15it/s]Measuring inference with batch_size=64:  47%|████▋     | 47/100 [00:14<00:16,  3.15it/s]Measuring inference with batch_size=64:  48%|████▊     | 48/100 [00:15<00:16,  3.15it/s]Measuring inference with batch_size=64:  49%|████▉     | 49/100 [00:15<00:16,  3.15it/s]Measuring inference with batch_size=64:  50%|█████     | 50/100 [00:15<00:15,  3.14it/s]Measuring inference with batch_size=64:  51%|█████     | 51/100 [00:16<00:15,  3.15it/s]Measuring inference with batch_size=64:  52%|█████▏    | 52/100 [00:16<00:15,  3.15it/s]Measuring inference with batch_size=64:  53%|█████▎    | 53/100 [00:16<00:14,  3.15it/s]Measuring inference with batch_size=64:  54%|█████▍    | 54/100 [00:17<00:14,  3.15it/s]Measuring inference with batch_size=64:  55%|█████▌    | 55/100 [00:17<00:14,  3.15it/s]Measuring inference with batch_size=64:  56%|█████▌    | 56/100 [00:17<00:13,  3.15it/s]Measuring inference with batch_size=64:  57%|█████▋    | 57/100 [00:18<00:13,  3.15it/s]Measuring inference with batch_size=64:  58%|█████▊    | 58/100 [00:18<00:13,  3.16it/s]Measuring inference with batch_size=64:  59%|█████▉    | 59/100 [00:18<00:13,  3.15it/s]Measuring inference with batch_size=64:  60%|██████    | 60/100 [00:19<00:12,  3.16it/s]Measuring inference with batch_size=64:  61%|██████    | 61/100 [00:19<00:12,  3.16it/s]Measuring inference with batch_size=64:  62%|██████▏   | 62/100 [00:19<00:12,  3.15it/s]Measuring inference with batch_size=64:  63%|██████▎   | 63/100 [00:19<00:11,  3.16it/s]Measuring inference with batch_size=64:  64%|██████▍   | 64/100 [00:20<00:11,  3.15it/s]Measuring inference with batch_size=64:  65%|██████▌   | 65/100 [00:20<00:11,  3.15it/s]Measuring inference with batch_size=64:  66%|██████▌   | 66/100 [00:20<00:10,  3.15it/s]Measuring inference with batch_size=64:  67%|██████▋   | 67/100 [00:21<00:10,  3.15it/s]Measuring inference with batch_size=64:  68%|██████▊   | 68/100 [00:21<00:10,  3.15it/s]Measuring inference with batch_size=64:  69%|██████▉   | 69/100 [00:21<00:09,  3.15it/s]Measuring inference with batch_size=64:  70%|███████   | 70/100 [00:22<00:09,  3.14it/s]Measuring inference with batch_size=64:  71%|███████   | 71/100 [00:22<00:09,  3.14it/s]Measuring inference with batch_size=64:  72%|███████▏  | 72/100 [00:22<00:08,  3.14it/s]Measuring inference with batch_size=64:  73%|███████▎  | 73/100 [00:23<00:08,  3.14it/s]Measuring inference with batch_size=64:  74%|███████▍  | 74/100 [00:23<00:08,  3.14it/s]Measuring inference with batch_size=64:  75%|███████▌  | 75/100 [00:23<00:07,  3.14it/s]Measuring inference with batch_size=64:  76%|███████▌  | 76/100 [00:24<00:07,  3.14it/s]Measuring inference with batch_size=64:  77%|███████▋  | 77/100 [00:24<00:07,  3.14it/s]Measuring inference with batch_size=64:  78%|███████▊  | 78/100 [00:24<00:06,  3.15it/s]Measuring inference with batch_size=64:  79%|███████▉  | 79/100 [00:25<00:06,  3.15it/s]Measuring inference with batch_size=64:  80%|████████  | 80/100 [00:25<00:06,  3.14it/s]Measuring inference with batch_size=64:  81%|████████  | 81/100 [00:25<00:06,  3.15it/s]Measuring inference with batch_size=64:  82%|████████▏ | 82/100 [00:26<00:05,  3.15it/s]Measuring inference with batch_size=64:  83%|████████▎ | 83/100 [00:26<00:05,  3.14it/s]Measuring inference with batch_size=64:  84%|████████▍ | 84/100 [00:26<00:05,  3.14it/s]Measuring inference with batch_size=64:  85%|████████▌ | 85/100 [00:26<00:04,  3.14it/s]Measuring inference with batch_size=64:  86%|████████▌ | 86/100 [00:27<00:04,  3.14it/s]Measuring inference with batch_size=64:  87%|████████▋ | 87/100 [00:27<00:04,  3.15it/s]Measuring inference with batch_size=64:  88%|████████▊ | 88/100 [00:27<00:03,  3.15it/s]Measuring inference with batch_size=64:  89%|████████▉ | 89/100 [00:28<00:03,  3.15it/s]Measuring inference with batch_size=64:  90%|█████████ | 90/100 [00:28<00:03,  3.15it/s]Measuring inference with batch_size=64:  91%|█████████ | 91/100 [00:28<00:02,  3.15it/s]Measuring inference with batch_size=64:  92%|█████████▏| 92/100 [00:29<00:02,  3.15it/s]Measuring inference with batch_size=64:  93%|█████████▎| 93/100 [00:29<00:02,  3.14it/s]Measuring inference with batch_size=64:  94%|█████████▍| 94/100 [00:29<00:01,  3.15it/s]Measuring inference with batch_size=64:  95%|█████████▌| 95/100 [00:30<00:01,  3.14it/s]Measuring inference with batch_size=64:  96%|█████████▌| 96/100 [00:30<00:01,  3.14it/s]Measuring inference with batch_size=64:  97%|█████████▋| 97/100 [00:30<00:00,  3.15it/s]Measuring inference with batch_size=64:  98%|█████████▊| 98/100 [00:31<00:00,  3.15it/s]Measuring inference with batch_size=64:  99%|█████████▉| 99/100 [00:31<00:00,  3.14it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:31<00:00,  3.14it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:31<00:00,  3.15it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
Timing results (batch_size=64):
  cpu_to_gpu:
    human_readable:
      batch_latency: "1.910 ms +/- 528.926 \xB5s [1.788 ms, 7.151 ms]"
      batches_per_second: 534.97 +/- 41.99 [139.85, 559.17]
    metrics:
      batches_per_second_max: 559.1659778696173
      batches_per_second_mean: 534.9686223575768
      batches_per_second_min: 139.8474259802614
      batches_per_second_std: 41.98944334567953
      seconds_per_batch_max: 0.0071506500244140625
      seconds_per_batch_mean: 0.0019095921516418457
      seconds_per_batch_min: 0.0017883777618408203
      seconds_per_batch_std: 0.0005289259375677093
  gpu_to_cpu:
    human_readable:
      batch_latency: 16.428 ms +/- 1.788 ms [10.326 ms, 17.698 ms]
      batches_per_second: 61.85 +/- 8.99 [56.50, 96.84]
    metrics:
      batches_per_second_max: 96.84377741861002
      batches_per_second_mean: 61.84551719590387
      batches_per_second_min: 56.50492395155532
      batches_per_second_std: 8.992952775775153
      seconds_per_batch_max: 0.017697572708129883
      seconds_per_batch_mean: 0.01642841100692749
      seconds_per_batch_min: 0.010325908660888672
      seconds_per_batch_std: 0.001788132136449596
  on_device_inference:
    human_readable:
      batch_latency: 298.968 ms +/- 2.176 ms [296.247 ms, 305.724 ms]
      batches_per_second: 3.35 +/- 0.02 [3.27, 3.38]
    metrics:
      batches_per_second_max: 3.375556113546958
      batches_per_second_mean: 3.3450127521022557
      batches_per_second_min: 3.2709200102004132
      batches_per_second_std: 0.02406240640984424
      seconds_per_batch_max: 0.3057243824005127
      seconds_per_batch_mean: 0.29896817207336424
      seconds_per_batch_min: 0.2962474822998047
      seconds_per_batch_std: 0.002175590101693535
  total:
    human_readable:
      batch_latency: 317.306 ms +/- 1.170 ms [314.806 ms, 322.623 ms]
      batches_per_second: 3.15 +/- 0.01 [3.10, 3.18]
    metrics:
      batches_per_second_max: 3.1765644998826104
      batches_per_second_mean: 3.151572864753683
      batches_per_second_min: 3.099592884607284
      batches_per_second_std: 0.011569688575648253
      seconds_per_batch_max: 0.32262301445007324
      seconds_per_batch_mean: 0.3173061752319336
      seconds_per_batch_min: 0.3148055076599121
      seconds_per_batch_std: 0.0011696108607942308

learner.model.forward:
  device: cuda
  flops: 138232160
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 52
        total: 104
      frequency: 2.10 GHz
      model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
    gpus:
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    memory:
      available: 177.81 GB
      total: 187.56 GB
      used: 8.37 GB
    system:
      node: charybdis
      release: 4.15.0-167-generic
      system: Linux
  max_inference_memory: 3817033216
  params: 3794322
  post_inference_memory: 3791818240
  pre_inference_memory: 73803264
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "75.395 \xB5s +/- 11.082 \xB5s [66.757 \xB5s, 169.754 \xB5\
            s]"
          batches_per_second: 13.43 K +/- 1.25 K [5.89 K, 14.98 K]
        metrics:
          batches_per_second_max: 14979.657142857142
          batches_per_second_mean: 13434.340824134684
          batches_per_second_min: 5890.876404494382
          batches_per_second_std: 1253.1564786071149
          seconds_per_batch_max: 0.0001697540283203125
          seconds_per_batch_mean: 7.539510726928711e-05
          seconds_per_batch_min: 6.67572021484375e-05
          seconds_per_batch_std: 1.1082020504909663e-05
      gpu_to_cpu:
        human_readable:
          batch_latency: "32.966 \xB5s +/- 1.053 \xB5s [31.948 \xB5s, 39.816 \xB5\
            s]"
          batches_per_second: 30.36 K +/- 883.22 [25.12 K, 31.30 K]
        metrics:
          batches_per_second_max: 31300.776119402984
          batches_per_second_mean: 30362.276332567217
          batches_per_second_min: 25115.59281437126
          batches_per_second_std: 883.222889807004
          seconds_per_batch_max: 3.981590270996094e-05
          seconds_per_batch_mean: 3.296613693237305e-05
          seconds_per_batch_min: 3.1948089599609375e-05
          seconds_per_batch_std: 1.0527496482407676e-06
      on_device_inference:
        human_readable:
          batch_latency: 97.140 ms +/- 1.042 ms [95.613 ms, 100.832 ms]
          batches_per_second: 10.30 +/- 0.11 [9.92, 10.46]
        metrics:
          batches_per_second_max: 10.458880676660982
          batches_per_second_mean: 10.295575473946313
          batches_per_second_min: 9.917511391070157
          batches_per_second_std: 0.10855576338128063
          seconds_per_batch_max: 0.10083174705505371
          seconds_per_batch_mean: 0.09714008331298828
          seconds_per_batch_min: 0.0956125259399414
          seconds_per_batch_std: 0.0010415777344963061
      total:
        human_readable:
          batch_latency: 97.248 ms +/- 1.041 ms [95.731 ms, 100.935 ms]
          batches_per_second: 10.28 +/- 0.11 [9.91, 10.45]
        metrics:
          batches_per_second_max: 10.445960889012861
          batches_per_second_mean: 10.284098843298182
          batches_per_second_min: 9.907344461950192
          batches_per_second_std: 0.10822064130316582
          seconds_per_batch_max: 0.10093522071838379
          seconds_per_batch_mean: 0.09724844455718994
          seconds_per_batch_min: 0.09573078155517578
          seconds_per_batch_std: 0.0010407073791778205
    batch_size_64:
      cpu_to_gpu:
        human_readable:
          batch_latency: "1.910 ms +/- 528.926 \xB5s [1.788 ms, 7.151 ms]"
          batches_per_second: 534.97 +/- 41.99 [139.85, 559.17]
        metrics:
          batches_per_second_max: 559.1659778696173
          batches_per_second_mean: 534.9686223575768
          batches_per_second_min: 139.8474259802614
          batches_per_second_std: 41.98944334567953
          seconds_per_batch_max: 0.0071506500244140625
          seconds_per_batch_mean: 0.0019095921516418457
          seconds_per_batch_min: 0.0017883777618408203
          seconds_per_batch_std: 0.0005289259375677093
      gpu_to_cpu:
        human_readable:
          batch_latency: 16.428 ms +/- 1.788 ms [10.326 ms, 17.698 ms]
          batches_per_second: 61.85 +/- 8.99 [56.50, 96.84]
        metrics:
          batches_per_second_max: 96.84377741861002
          batches_per_second_mean: 61.84551719590387
          batches_per_second_min: 56.50492395155532
          batches_per_second_std: 8.992952775775153
          seconds_per_batch_max: 0.017697572708129883
          seconds_per_batch_mean: 0.01642841100692749
          seconds_per_batch_min: 0.010325908660888672
          seconds_per_batch_std: 0.001788132136449596
      on_device_inference:
        human_readable:
          batch_latency: 298.968 ms +/- 2.176 ms [296.247 ms, 305.724 ms]
          batches_per_second: 3.35 +/- 0.02 [3.27, 3.38]
        metrics:
          batches_per_second_max: 3.375556113546958
          batches_per_second_mean: 3.3450127521022557
          batches_per_second_min: 3.2709200102004132
          batches_per_second_std: 0.02406240640984424
          seconds_per_batch_max: 0.3057243824005127
          seconds_per_batch_mean: 0.29896817207336424
          seconds_per_batch_min: 0.2962474822998047
          seconds_per_batch_std: 0.002175590101693535
      total:
        human_readable:
          batch_latency: 317.306 ms +/- 1.170 ms [314.806 ms, 322.623 ms]
          batches_per_second: 3.15 +/- 0.01 [3.10, 3.18]
        metrics:
          batches_per_second_max: 3.1765644998826104
          batches_per_second_mean: 3.151572864753683
          batches_per_second_min: 3.099592884607284
          batches_per_second_std: 0.011569688575648253
          seconds_per_batch_max: 0.32262301445007324
          seconds_per_batch_mean: 0.3173061752319336
          seconds_per_batch_min: 0.3148055076599121
          seconds_per_batch_std: 0.0011696108607942308

==== Benchmarking CoX3DLearner (m) ====
== Benchmarking learner.infer ==
Machine info:
  cpu:
    architecture: x86_64
    cores:
      physical: 52
      total: 104
    frequency: 2.10 GHz
    model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
  gpus:
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  memory:
    available: 177.77 GB
    total: 187.56 GB
    used: 8.42 GB
  system:
    node: charybdis
    release: 4.15.0-167-generic
    system: Linux

Model device: cuda:0
Allocated GPU memory prior to inference: 1738595328 (1.62 GB)
Allocated GPU memory after to inference: 9049275392 (8.43 GB)
Max allocated GPU memory during inference: 9097375232 (8.47 GB)
Warming up with batch_size=64:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=64:  10%|█         | 1/10 [00:00<00:00,  9.36it/s]Warming up with batch_size=64:  20%|██        | 2/10 [00:00<00:00,  9.54it/s]Warming up with batch_size=64:  30%|███       | 3/10 [00:00<00:00,  9.68it/s]Warming up with batch_size=64:  40%|████      | 4/10 [00:00<00:00,  9.74it/s]Warming up with batch_size=64:  50%|█████     | 5/10 [00:00<00:00,  9.79it/s]Warming up with batch_size=64:  60%|██████    | 6/10 [00:00<00:00,  9.86it/s]Warming up with batch_size=64:  80%|████████  | 8/10 [00:00<00:00,  9.92it/s]Warming up with batch_size=64: 100%|██████████| 10/10 [00:01<00:00,  9.96it/s]Warming up with batch_size=64: 100%|██████████| 10/10 [00:01<00:00,  9.86it/s]
Measuring inference with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=64:   1%|          | 1/100 [00:00<00:09,  9.95it/s]Measuring inference with batch_size=64:   2%|▏         | 2/100 [00:00<00:09,  9.94it/s]Measuring inference with batch_size=64:   3%|▎         | 3/100 [00:00<00:09,  9.93it/s]Measuring inference with batch_size=64:   4%|▍         | 4/100 [00:00<00:09,  9.90it/s]Measuring inference with batch_size=64:   5%|▌         | 5/100 [00:00<00:09,  9.89it/s]Measuring inference with batch_size=64:   6%|▌         | 6/100 [00:00<00:09,  9.89it/s]Measuring inference with batch_size=64:   7%|▋         | 7/100 [00:00<00:09,  9.81it/s]Measuring inference with batch_size=64:   8%|▊         | 8/100 [00:00<00:09,  9.78it/s]Measuring inference with batch_size=64:   9%|▉         | 9/100 [00:00<00:09,  9.77it/s]Measuring inference with batch_size=64:  10%|█         | 10/100 [00:01<00:09,  9.79it/s]Measuring inference with batch_size=64:  11%|█         | 11/100 [00:01<00:09,  9.84it/s]Measuring inference with batch_size=64:  12%|█▏        | 12/100 [00:01<00:08,  9.86it/s]Measuring inference with batch_size=64:  13%|█▎        | 13/100 [00:01<00:08,  9.88it/s]Measuring inference with batch_size=64:  14%|█▍        | 14/100 [00:01<00:08,  9.92it/s]Measuring inference with batch_size=64:  15%|█▌        | 15/100 [00:01<00:08,  9.90it/s]Measuring inference with batch_size=64:  16%|█▌        | 16/100 [00:01<00:08,  9.92it/s]Measuring inference with batch_size=64:  17%|█▋        | 17/100 [00:01<00:08,  9.88it/s]Measuring inference with batch_size=64:  18%|█▊        | 18/100 [00:01<00:08,  9.88it/s]Measuring inference with batch_size=64:  20%|██        | 20/100 [00:02<00:08,  9.94it/s]Measuring inference with batch_size=64:  21%|██        | 21/100 [00:02<00:07,  9.95it/s]Measuring inference with batch_size=64:  22%|██▏       | 22/100 [00:02<00:07,  9.92it/s]Measuring inference with batch_size=64:  23%|██▎       | 23/100 [00:02<00:07,  9.87it/s]Measuring inference with batch_size=64:  24%|██▍       | 24/100 [00:02<00:07,  9.88it/s]Measuring inference with batch_size=64:  25%|██▌       | 25/100 [00:02<00:07,  9.89it/s]Measuring inference with batch_size=64:  26%|██▌       | 26/100 [00:02<00:07,  9.88it/s]Measuring inference with batch_size=64:  27%|██▋       | 27/100 [00:02<00:07,  9.89it/s]Measuring inference with batch_size=64:  28%|██▊       | 28/100 [00:02<00:07,  9.86it/s]Measuring inference with batch_size=64:  29%|██▉       | 29/100 [00:02<00:07,  9.84it/s]Measuring inference with batch_size=64:  30%|███       | 30/100 [00:03<00:07,  9.89it/s]Measuring inference with batch_size=64:  31%|███       | 31/100 [00:03<00:06,  9.88it/s]Measuring inference with batch_size=64:  32%|███▏      | 32/100 [00:03<00:06,  9.89it/s]Measuring inference with batch_size=64:  33%|███▎      | 33/100 [00:03<00:06,  9.88it/s]Measuring inference with batch_size=64:  34%|███▍      | 34/100 [00:03<00:06,  9.92it/s]Measuring inference with batch_size=64:  35%|███▌      | 35/100 [00:03<00:06,  9.93it/s]Measuring inference with batch_size=64:  36%|███▌      | 36/100 [00:03<00:06,  9.90it/s]Measuring inference with batch_size=64:  37%|███▋      | 37/100 [00:03<00:06,  9.84it/s]Measuring inference with batch_size=64:  38%|███▊      | 38/100 [00:03<00:06,  9.82it/s]Measuring inference with batch_size=64:  39%|███▉      | 39/100 [00:03<00:06,  9.80it/s]Measuring inference with batch_size=64:  40%|████      | 40/100 [00:04<00:06,  9.79it/s]Measuring inference with batch_size=64:  41%|████      | 41/100 [00:04<00:06,  9.73it/s]Measuring inference with batch_size=64:  42%|████▏     | 42/100 [00:04<00:05,  9.75it/s]Measuring inference with batch_size=64:  43%|████▎     | 43/100 [00:04<00:05,  9.74it/s]Measuring inference with batch_size=64:  44%|████▍     | 44/100 [00:04<00:06,  9.00it/s]Measuring inference with batch_size=64:  45%|████▌     | 45/100 [00:04<00:06,  8.69it/s]Measuring inference with batch_size=64:  47%|████▋     | 47/100 [00:04<00:05,  9.88it/s]Measuring inference with batch_size=64:  49%|████▉     | 49/100 [00:04<00:04, 10.68it/s]Measuring inference with batch_size=64:  51%|█████     | 51/100 [00:05<00:04, 10.45it/s]Measuring inference with batch_size=64:  53%|█████▎    | 53/100 [00:05<00:04, 10.22it/s]Measuring inference with batch_size=64:  55%|█████▌    | 55/100 [00:05<00:04, 10.25it/s]Measuring inference with batch_size=64:  57%|█████▋    | 57/100 [00:05<00:03, 10.79it/s]Measuring inference with batch_size=64:  59%|█████▉    | 59/100 [00:05<00:03, 10.27it/s]Measuring inference with batch_size=64:  61%|██████    | 61/100 [00:06<00:03, 10.16it/s]Measuring inference with batch_size=64:  63%|██████▎   | 63/100 [00:06<00:03, 10.48it/s]Measuring inference with batch_size=64:  65%|██████▌   | 65/100 [00:06<00:03, 10.89it/s]Measuring inference with batch_size=64:  67%|██████▋   | 67/100 [00:06<00:02, 11.19it/s]Measuring inference with batch_size=64:  69%|██████▉   | 69/100 [00:06<00:02, 11.41it/s]Measuring inference with batch_size=64:  71%|███████   | 71/100 [00:06<00:02, 11.57it/s]Measuring inference with batch_size=64:  73%|███████▎  | 73/100 [00:07<00:02, 11.00it/s]Measuring inference with batch_size=64:  75%|███████▌  | 75/100 [00:07<00:02,  9.47it/s]Measuring inference with batch_size=64:  76%|███████▌  | 76/100 [00:07<00:02,  9.56it/s]Measuring inference with batch_size=64:  77%|███████▋  | 77/100 [00:07<00:02,  9.61it/s]Measuring inference with batch_size=64:  78%|███████▊  | 78/100 [00:07<00:02,  9.60it/s]Measuring inference with batch_size=64:  79%|███████▉  | 79/100 [00:07<00:02,  9.65it/s]Measuring inference with batch_size=64:  80%|████████  | 80/100 [00:07<00:02,  9.69it/s]Measuring inference with batch_size=64:  81%|████████  | 81/100 [00:08<00:01,  9.68it/s]Measuring inference with batch_size=64:  82%|████████▏ | 82/100 [00:08<00:01,  9.71it/s]Measuring inference with batch_size=64:  83%|████████▎ | 83/100 [00:08<00:01,  9.77it/s]Measuring inference with batch_size=64:  84%|████████▍ | 84/100 [00:08<00:01,  9.79it/s]Measuring inference with batch_size=64:  85%|████████▌ | 85/100 [00:08<00:01,  9.79it/s]Measuring inference with batch_size=64:  86%|████████▌ | 86/100 [00:08<00:01,  9.82it/s]Measuring inference with batch_size=64:  87%|████████▋ | 87/100 [00:08<00:01,  9.87it/s]Measuring inference with batch_size=64:  88%|████████▊ | 88/100 [00:08<00:01,  9.90it/s]Measuring inference with batch_size=64:  90%|█████████ | 90/100 [00:08<00:01,  9.94it/s]Measuring inference with batch_size=64:  91%|█████████ | 91/100 [00:09<00:00,  9.94it/s]Measuring inference with batch_size=64:  92%|█████████▏| 92/100 [00:09<00:00,  9.95it/s]Measuring inference with batch_size=64:  93%|█████████▎| 93/100 [00:09<00:00,  9.96it/s]Measuring inference with batch_size=64:  94%|█████████▍| 94/100 [00:09<00:00,  9.95it/s]Measuring inference with batch_size=64:  95%|█████████▌| 95/100 [00:09<00:00,  9.95it/s]Measuring inference with batch_size=64:  96%|█████████▌| 96/100 [00:09<00:00,  9.87it/s]Measuring inference with batch_size=64:  97%|█████████▋| 97/100 [00:09<00:00,  9.90it/s]Measuring inference with batch_size=64:  98%|█████████▊| 98/100 [00:09<00:00,  9.92it/s]Measuring inference with batch_size=64:  99%|█████████▉| 99/100 [00:09<00:00,  9.90it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:09<00:00,  9.88it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:09<00:00, 10.01it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "1.342 \xB5s +/- 0.650 \xB5s [0.715 \xB5s, 7.153 \xB5s]"
      batches_per_second: 813.33 K +/- 208.35 K [139.81 K, 1.40 M]
    metrics:
      batches_per_second_max: 1398101.3333333333
      batches_per_second_mean: 813328.8066031747
      batches_per_second_min: 139810.13333333333
      batches_per_second_std: 208352.85985439253
      seconds_per_batch_max: 7.152557373046875e-06
      seconds_per_batch_mean: 1.3422966003417968e-06
      seconds_per_batch_min: 7.152557373046875e-07
      seconds_per_batch_std: 6.500175510971006e-07
  gpu_to_cpu:
    human_readable:
      batch_latency: "43.938 \xB5s +/- 25.772 \xB5s [36.478 \xB5s, 288.725 \xB5s]"
      batches_per_second: 24.44 K +/- 3.59 K [3.46 K, 27.41 K]
    metrics:
      batches_per_second_max: 27413.75163398693
      batches_per_second_mean: 24435.93737527218
      batches_per_second_min: 3463.5045417010733
      batches_per_second_std: 3589.5663758979595
      seconds_per_batch_max: 0.0002887248992919922
      seconds_per_batch_mean: 4.393815994262695e-05
      seconds_per_batch_min: 3.647804260253906e-05
      seconds_per_batch_std: 2.577169846643832e-05
  on_device_inference:
    human_readable:
      batch_latency: 99.769 ms +/- 10.553 ms [81.692 ms, 164.515 ms]
      batches_per_second: 10.12 +/- 0.98 [6.08, 12.24]
    metrics:
      batches_per_second_max: 12.241139388279244
      batches_per_second_mean: 10.123089127544782
      batches_per_second_min: 6.078454787341963
      batches_per_second_std: 0.9803131468793987
      seconds_per_batch_max: 0.16451549530029297
      seconds_per_batch_mean: 0.09976871967315674
      seconds_per_batch_min: 0.08169174194335938
      seconds_per_batch_std: 0.010553059104896132
  total:
    human_readable:
      batch_latency: 99.814 ms +/- 10.554 ms [81.732 ms, 164.591 ms]
      batches_per_second: 10.12 +/- 0.98 [6.08, 12.24]
    metrics:
      batches_per_second_max: 12.235140370119717
      batches_per_second_mean: 10.118367560147288
      batches_per_second_min: 6.0756812193358956
      batches_per_second_std: 0.978990751356408
      seconds_per_batch_max: 0.16459059715270996
      seconds_per_batch_mean: 0.09981400012969971
      seconds_per_batch_min: 0.08173179626464844
      seconds_per_batch_std: 0.010553510872123369

Warming up with batch_size=64:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=64:  10%|█         | 1/10 [00:00<00:05,  1.59it/s]Warming up with batch_size=64:  20%|██        | 2/10 [00:01<00:04,  1.63it/s]Warming up with batch_size=64:  30%|███       | 3/10 [00:01<00:04,  1.64it/s]Warming up with batch_size=64:  40%|████      | 4/10 [00:02<00:03,  1.65it/s]Warming up with batch_size=64:  50%|█████     | 5/10 [00:03<00:03,  1.65it/s]Warming up with batch_size=64:  60%|██████    | 6/10 [00:03<00:02,  1.65it/s]Warming up with batch_size=64:  70%|███████   | 7/10 [00:04<00:01,  1.65it/s]Warming up with batch_size=64:  80%|████████  | 8/10 [00:04<00:01,  1.65it/s]Warming up with batch_size=64:  90%|█████████ | 9/10 [00:05<00:00,  1.65it/s]Warming up with batch_size=64: 100%|██████████| 10/10 [00:06<00:00,  1.65it/s]Warming up with batch_size=64: 100%|██████████| 10/10 [00:06<00:00,  1.65it/s]
Measuring inference with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=64:   1%|          | 1/100 [00:00<00:59,  1.65it/s]Measuring inference with batch_size=64:   2%|▏         | 2/100 [00:01<00:59,  1.65it/s]Measuring inference with batch_size=64:   3%|▎         | 3/100 [00:01<00:58,  1.65it/s]Measuring inference with batch_size=64:   4%|▍         | 4/100 [00:02<00:58,  1.65it/s]Measuring inference with batch_size=64:   5%|▌         | 5/100 [00:03<00:57,  1.65it/s]Measuring inference with batch_size=64:   6%|▌         | 6/100 [00:03<00:56,  1.65it/s]Measuring inference with batch_size=64:   7%|▋         | 7/100 [00:04<00:56,  1.65it/s]Measuring inference with batch_size=64:   8%|▊         | 8/100 [00:04<00:55,  1.65it/s]Measuring inference with batch_size=64:   9%|▉         | 9/100 [00:05<00:55,  1.65it/s]Measuring inference with batch_size=64:  10%|█         | 10/100 [00:06<00:54,  1.65it/s]Measuring inference with batch_size=64:  11%|█         | 11/100 [00:06<00:53,  1.65it/s]Measuring inference with batch_size=64:  12%|█▏        | 12/100 [00:07<00:53,  1.65it/s]Measuring inference with batch_size=64:  13%|█▎        | 13/100 [00:07<00:52,  1.65it/s]Measuring inference with batch_size=64:  14%|█▍        | 14/100 [00:08<00:52,  1.65it/s]Measuring inference with batch_size=64:  15%|█▌        | 15/100 [00:09<00:51,  1.65it/s]Measuring inference with batch_size=64:  16%|█▌        | 16/100 [00:09<00:50,  1.65it/s]Measuring inference with batch_size=64:  17%|█▋        | 17/100 [00:10<00:50,  1.65it/s]Measuring inference with batch_size=64:  18%|█▊        | 18/100 [00:10<00:49,  1.65it/s]Measuring inference with batch_size=64:  19%|█▉        | 19/100 [00:11<00:48,  1.65it/s]Measuring inference with batch_size=64:  20%|██        | 20/100 [00:12<00:48,  1.66it/s]Measuring inference with batch_size=64:  21%|██        | 21/100 [00:12<00:47,  1.65it/s]Measuring inference with batch_size=64:  22%|██▏       | 22/100 [00:13<00:47,  1.65it/s]Measuring inference with batch_size=64:  23%|██▎       | 23/100 [00:13<00:46,  1.65it/s]Measuring inference with batch_size=64:  24%|██▍       | 24/100 [00:14<00:45,  1.65it/s]Measuring inference with batch_size=64:  25%|██▌       | 25/100 [00:15<00:45,  1.65it/s]Measuring inference with batch_size=64:  26%|██▌       | 26/100 [00:15<00:44,  1.65it/s]Measuring inference with batch_size=64:  27%|██▋       | 27/100 [00:16<00:44,  1.65it/s]Measuring inference with batch_size=64:  28%|██▊       | 28/100 [00:16<00:43,  1.65it/s]Measuring inference with batch_size=64:  29%|██▉       | 29/100 [00:17<00:42,  1.65it/s]Measuring inference with batch_size=64:  30%|███       | 30/100 [00:18<00:42,  1.65it/s]Measuring inference with batch_size=64:  31%|███       | 31/100 [00:18<00:41,  1.65it/s]Measuring inference with batch_size=64:  32%|███▏      | 32/100 [00:19<00:41,  1.65it/s]Measuring inference with batch_size=64:  33%|███▎      | 33/100 [00:19<00:40,  1.65it/s]Measuring inference with batch_size=64:  34%|███▍      | 34/100 [00:20<00:39,  1.65it/s]Measuring inference with batch_size=64:  35%|███▌      | 35/100 [00:21<00:39,  1.65it/s]Measuring inference with batch_size=64:  36%|███▌      | 36/100 [00:21<00:38,  1.65it/s]Measuring inference with batch_size=64:  37%|███▋      | 37/100 [00:22<00:38,  1.65it/s]Measuring inference with batch_size=64:  38%|███▊      | 38/100 [00:23<00:37,  1.65it/s]Measuring inference with batch_size=64:  39%|███▉      | 39/100 [00:23<00:36,  1.65it/s]Measuring inference with batch_size=64:  40%|████      | 40/100 [00:24<00:36,  1.65it/s]Measuring inference with batch_size=64:  41%|████      | 41/100 [00:24<00:35,  1.65it/s]Measuring inference with batch_size=64:  42%|████▏     | 42/100 [00:25<00:35,  1.65it/s]Measuring inference with batch_size=64:  43%|████▎     | 43/100 [00:26<00:34,  1.65it/s]Measuring inference with batch_size=64:  44%|████▍     | 44/100 [00:26<00:33,  1.65it/s]Measuring inference with batch_size=64:  45%|████▌     | 45/100 [00:27<00:33,  1.65it/s]Measuring inference with batch_size=64:  46%|████▌     | 46/100 [00:27<00:32,  1.65it/s]Measuring inference with batch_size=64:  47%|████▋     | 47/100 [00:28<00:32,  1.65it/s]Measuring inference with batch_size=64:  48%|████▊     | 48/100 [00:29<00:31,  1.65it/s]Measuring inference with batch_size=64:  49%|████▉     | 49/100 [00:29<00:30,  1.65it/s]Measuring inference with batch_size=64:  50%|█████     | 50/100 [00:30<00:30,  1.65it/s]Measuring inference with batch_size=64:  51%|█████     | 51/100 [00:30<00:29,  1.65it/s]Measuring inference with batch_size=64:  52%|█████▏    | 52/100 [00:31<00:29,  1.65it/s]Measuring inference with batch_size=64:  53%|█████▎    | 53/100 [00:32<00:28,  1.65it/s]Measuring inference with batch_size=64:  54%|█████▍    | 54/100 [00:32<00:27,  1.65it/s]Measuring inference with batch_size=64:  55%|█████▌    | 55/100 [00:33<00:27,  1.65it/s]Measuring inference with batch_size=64:  56%|█████▌    | 56/100 [00:33<00:26,  1.65it/s]Measuring inference with batch_size=64:  57%|█████▋    | 57/100 [00:34<00:26,  1.65it/s]Measuring inference with batch_size=64:  58%|█████▊    | 58/100 [00:35<00:25,  1.65it/s]Measuring inference with batch_size=64:  59%|█████▉    | 59/100 [00:35<00:24,  1.65it/s]Measuring inference with batch_size=64:  60%|██████    | 60/100 [00:36<00:24,  1.65it/s]Measuring inference with batch_size=64:  61%|██████    | 61/100 [00:36<00:23,  1.65it/s]Measuring inference with batch_size=64:  62%|██████▏   | 62/100 [00:37<00:23,  1.65it/s]Measuring inference with batch_size=64:  63%|██████▎   | 63/100 [00:38<00:22,  1.65it/s]Measuring inference with batch_size=64:  64%|██████▍   | 64/100 [00:38<00:21,  1.65it/s]Measuring inference with batch_size=64:  65%|██████▌   | 65/100 [00:39<00:21,  1.65it/s]Measuring inference with batch_size=64:  66%|██████▌   | 66/100 [00:39<00:20,  1.65it/s]Measuring inference with batch_size=64:  67%|██████▋   | 67/100 [00:40<00:20,  1.65it/s]Measuring inference with batch_size=64:  68%|██████▊   | 68/100 [00:41<00:19,  1.65it/s]Measuring inference with batch_size=64:  69%|██████▉   | 69/100 [00:41<00:18,  1.65it/s]Measuring inference with batch_size=64:  70%|███████   | 70/100 [00:42<00:18,  1.65it/s]Measuring inference with batch_size=64:  71%|███████   | 71/100 [00:43<00:17,  1.65it/s]Measuring inference with batch_size=64:  72%|███████▏  | 72/100 [00:43<00:16,  1.65it/s]Measuring inference with batch_size=64:  73%|███████▎  | 73/100 [00:44<00:16,  1.65it/s]Measuring inference with batch_size=64:  74%|███████▍  | 74/100 [00:44<00:15,  1.65it/s]Measuring inference with batch_size=64:  75%|███████▌  | 75/100 [00:45<00:15,  1.65it/s]Measuring inference with batch_size=64:  76%|███████▌  | 76/100 [00:46<00:14,  1.65it/s]Measuring inference with batch_size=64:  77%|███████▋  | 77/100 [00:46<00:13,  1.65it/s]Measuring inference with batch_size=64:  78%|███████▊  | 78/100 [00:47<00:13,  1.65it/s]Measuring inference with batch_size=64:  79%|███████▉  | 79/100 [00:47<00:12,  1.65it/s]Measuring inference with batch_size=64:  80%|████████  | 80/100 [00:48<00:12,  1.65it/s]Measuring inference with batch_size=64:  81%|████████  | 81/100 [00:49<00:11,  1.65it/s]Measuring inference with batch_size=64:  82%|████████▏ | 82/100 [00:49<00:10,  1.65it/s]Measuring inference with batch_size=64:  83%|████████▎ | 83/100 [00:50<00:10,  1.65it/s]Measuring inference with batch_size=64:  84%|████████▍ | 84/100 [00:50<00:09,  1.65it/s]Measuring inference with batch_size=64:  85%|████████▌ | 85/100 [00:51<00:09,  1.65it/s]Measuring inference with batch_size=64:  86%|████████▌ | 86/100 [00:52<00:08,  1.65it/s]Measuring inference with batch_size=64:  87%|████████▋ | 87/100 [00:52<00:07,  1.64it/s]Measuring inference with batch_size=64:  88%|████████▊ | 88/100 [00:53<00:07,  1.65it/s]Measuring inference with batch_size=64:  89%|████████▉ | 89/100 [00:53<00:06,  1.65it/s]Measuring inference with batch_size=64:  90%|█████████ | 90/100 [00:54<00:06,  1.65it/s]Measuring inference with batch_size=64:  91%|█████████ | 91/100 [00:55<00:05,  1.65it/s]Measuring inference with batch_size=64:  92%|█████████▏| 92/100 [00:55<00:04,  1.64it/s]Measuring inference with batch_size=64:  93%|█████████▎| 93/100 [00:56<00:04,  1.65it/s]Measuring inference with batch_size=64:  94%|█████████▍| 94/100 [00:56<00:03,  1.65it/s]Measuring inference with batch_size=64:  95%|█████████▌| 95/100 [00:57<00:03,  1.65it/s]Measuring inference with batch_size=64:  96%|█████████▌| 96/100 [00:58<00:02,  1.64it/s]Measuring inference with batch_size=64:  97%|█████████▋| 97/100 [00:58<00:01,  1.64it/s]Measuring inference with batch_size=64:  98%|█████████▊| 98/100 [00:59<00:01,  1.64it/s]Measuring inference with batch_size=64:  99%|█████████▉| 99/100 [00:59<00:00,  1.65it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [01:00<00:00,  1.65it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [01:00<00:00,  1.65it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Timing results (batch_size=64):
  cpu_to_gpu:
    human_readable:
      batch_latency: "1.867 \xB5s +/- 0.519 \xB5s [1.192 \xB5s, 6.199 \xB5s]"
      batches_per_second: 556.81 K +/- 90.25 K [161.32 K, 838.86 K]
    metrics:
      batches_per_second_max: 838860.8
      batches_per_second_mean: 556812.245944278
      batches_per_second_min: 161319.38461538462
      batches_per_second_std: 90251.50827089911
      seconds_per_batch_max: 6.198883056640625e-06
      seconds_per_batch_mean: 1.8668174743652344e-06
      seconds_per_batch_min: 1.1920928955078125e-06
      seconds_per_batch_std: 5.19134214978147e-07
  gpu_to_cpu:
    human_readable:
      batch_latency: "1.049 ms +/- 29.048 \xB5s [1.005 ms, 1.260 ms]"
      batches_per_second: 954.15 +/- 23.95 [793.62, 995.09]
    metrics:
      batches_per_second_max: 995.0899169632265
      batches_per_second_mean: 954.152472644322
      batches_per_second_min: 793.6242194891202
      batches_per_second_std: 23.947917230589862
      seconds_per_batch_max: 0.0012600421905517578
      seconds_per_batch_mean: 0.0010487771034240723
      seconds_per_batch_min: 0.001004934310913086
      seconds_per_batch_std: 2.9048148975148604e-05
  on_device_inference:
    human_readable:
      batch_latency: 604.785 ms +/- 2.358 ms [600.894 ms, 612.833 ms]
      batches_per_second: 1.65 +/- 0.01 [1.63, 1.66]
    metrics:
      batches_per_second_max: 1.6641864388555005
      batches_per_second_mean: 1.6535050673629133
      batches_per_second_min: 1.6317645823039904
      batches_per_second_std: 0.0064157302725160095
      seconds_per_batch_max: 0.6128334999084473
      seconds_per_batch_mean: 0.6047850394248963
      seconds_per_batch_min: 0.6008942127227783
      seconds_per_batch_std: 0.0023578931477225315
  total:
    human_readable:
      batch_latency: 605.836 ms +/- 2.359 ms [601.969 ms, 613.874 ms]
      batches_per_second: 1.65 +/- 0.01 [1.63, 1.66]
    metrics:
      batches_per_second_max: 1.6612151022579245
      batches_per_second_mean: 1.6506374921904348
      batches_per_second_min: 1.628997629308336
      batches_per_second_std: 0.006397372351785605
      seconds_per_batch_max: 0.6138744354248047
      seconds_per_batch_mean: 0.6058356833457946
      seconds_per_batch_min: 0.6019690036773682
      seconds_per_batch_std: 0.0023592945633150128

learner.infer:
  device: cuda
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 52
        total: 104
      frequency: 2.10 GHz
      model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
    gpus:
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    memory:
      available: 177.77 GB
      total: 187.56 GB
      used: 8.42 GB
    system:
      node: charybdis
      release: 4.15.0-167-generic
      system: Linux
  max_inference_memory: 9097375232
  post_inference_memory: 9049275392
  pre_inference_memory: 1738595328
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "1.342 \xB5s +/- 0.650 \xB5s [0.715 \xB5s, 7.153 \xB5s]"
          batches_per_second: 813.33 K +/- 208.35 K [139.81 K, 1.40 M]
        metrics:
          batches_per_second_max: 1398101.3333333333
          batches_per_second_mean: 813328.8066031747
          batches_per_second_min: 139810.13333333333
          batches_per_second_std: 208352.85985439253
          seconds_per_batch_max: 7.152557373046875e-06
          seconds_per_batch_mean: 1.3422966003417968e-06
          seconds_per_batch_min: 7.152557373046875e-07
          seconds_per_batch_std: 6.500175510971006e-07
      gpu_to_cpu:
        human_readable:
          batch_latency: "43.938 \xB5s +/- 25.772 \xB5s [36.478 \xB5s, 288.725 \xB5\
            s]"
          batches_per_second: 24.44 K +/- 3.59 K [3.46 K, 27.41 K]
        metrics:
          batches_per_second_max: 27413.75163398693
          batches_per_second_mean: 24435.93737527218
          batches_per_second_min: 3463.5045417010733
          batches_per_second_std: 3589.5663758979595
          seconds_per_batch_max: 0.0002887248992919922
          seconds_per_batch_mean: 4.393815994262695e-05
          seconds_per_batch_min: 3.647804260253906e-05
          seconds_per_batch_std: 2.577169846643832e-05
      on_device_inference:
        human_readable:
          batch_latency: 99.769 ms +/- 10.553 ms [81.692 ms, 164.515 ms]
          batches_per_second: 10.12 +/- 0.98 [6.08, 12.24]
        metrics:
          batches_per_second_max: 12.241139388279244
          batches_per_second_mean: 10.123089127544782
          batches_per_second_min: 6.078454787341963
          batches_per_second_std: 0.9803131468793987
          seconds_per_batch_max: 0.16451549530029297
          seconds_per_batch_mean: 0.09976871967315674
          seconds_per_batch_min: 0.08169174194335938
          seconds_per_batch_std: 0.010553059104896132
      total:
        human_readable:
          batch_latency: 99.814 ms +/- 10.554 ms [81.732 ms, 164.591 ms]
          batches_per_second: 10.12 +/- 0.98 [6.08, 12.24]
        metrics:
          batches_per_second_max: 12.235140370119717
          batches_per_second_mean: 10.118367560147288
          batches_per_second_min: 6.0756812193358956
          batches_per_second_std: 0.978990751356408
          seconds_per_batch_max: 0.16459059715270996
          seconds_per_batch_mean: 0.09981400012969971
          seconds_per_batch_min: 0.08173179626464844
          seconds_per_batch_std: 0.010553510872123369
    batch_size_64:
      cpu_to_gpu:
        human_readable:
          batch_latency: "1.867 \xB5s +/- 0.519 \xB5s [1.192 \xB5s, 6.199 \xB5s]"
          batches_per_second: 556.81 K +/- 90.25 K [161.32 K, 838.86 K]
        metrics:
          batches_per_second_max: 838860.8
          batches_per_second_mean: 556812.245944278
          batches_per_second_min: 161319.38461538462
          batches_per_second_std: 90251.50827089911
          seconds_per_batch_max: 6.198883056640625e-06
          seconds_per_batch_mean: 1.8668174743652344e-06
          seconds_per_batch_min: 1.1920928955078125e-06
          seconds_per_batch_std: 5.19134214978147e-07
      gpu_to_cpu:
        human_readable:
          batch_latency: "1.049 ms +/- 29.048 \xB5s [1.005 ms, 1.260 ms]"
          batches_per_second: 954.15 +/- 23.95 [793.62, 995.09]
        metrics:
          batches_per_second_max: 995.0899169632265
          batches_per_second_mean: 954.152472644322
          batches_per_second_min: 793.6242194891202
          batches_per_second_std: 23.947917230589862
          seconds_per_batch_max: 0.0012600421905517578
          seconds_per_batch_mean: 0.0010487771034240723
          seconds_per_batch_min: 0.001004934310913086
          seconds_per_batch_std: 2.9048148975148604e-05
      on_device_inference:
        human_readable:
          batch_latency: 604.785 ms +/- 2.358 ms [600.894 ms, 612.833 ms]
          batches_per_second: 1.65 +/- 0.01 [1.63, 1.66]
        metrics:
          batches_per_second_max: 1.6641864388555005
          batches_per_second_mean: 1.6535050673629133
          batches_per_second_min: 1.6317645823039904
          batches_per_second_std: 0.0064157302725160095
          seconds_per_batch_max: 0.6128334999084473
          seconds_per_batch_mean: 0.6047850394248963
          seconds_per_batch_min: 0.6008942127227783
          seconds_per_batch_std: 0.0023578931477225315
      total:
        human_readable:
          batch_latency: 605.836 ms +/- 2.359 ms [601.969 ms, 613.874 ms]
          batches_per_second: 1.65 +/- 0.01 [1.63, 1.66]
        metrics:
          batches_per_second_max: 1.6612151022579245
          batches_per_second_mean: 1.6506374921904348
          batches_per_second_min: 1.628997629308336
          batches_per_second_std: 0.006397372351785605
          seconds_per_batch_max: 0.6138744354248047
          seconds_per_batch_mean: 0.6058356833457946
          seconds_per_batch_min: 0.6019690036773682
          seconds_per_batch_std: 0.0023592945633150128

== Benchmarking model directly ==
Machine info:
  cpu:
    architecture: x86_64
    cores:
      physical: 52
      total: 104
    frequency: 2.10 GHz
    model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
  gpus:
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  memory:
    available: 177.79 GB
    total: 187.56 GB
    used: 8.39 GB
  system:
    node: charybdis
    release: 4.15.0-167-generic
    system: Linux

Model device: cuda:0
Model parameters: 3794322 (3.79 M)
Model FLOPs: 269136368 (269.14 M)
Allocated GPU memory prior to inference: 1855218176 (1.73 GB)
Allocated GPU memory after to inference: 9058081792 (8.44 GB)
Max allocated GPU memory during inference: 9105660928 (8.48 GB)
Warming up with batch_size=64:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=64:  10%|█         | 1/10 [00:00<00:00,  9.90it/s]Warming up with batch_size=64:  30%|███       | 3/10 [00:00<00:00, 10.13it/s]Warming up with batch_size=64:  50%|█████     | 5/10 [00:00<00:00, 10.28it/s]Warming up with batch_size=64:  70%|███████   | 7/10 [00:00<00:00, 10.34it/s]Warming up with batch_size=64:  90%|█████████ | 9/10 [00:00<00:00, 10.37it/s]Warming up with batch_size=64: 100%|██████████| 10/10 [00:00<00:00, 10.32it/s]
Measuring inference with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=64:   2%|▏         | 2/100 [00:00<00:09, 10.27it/s]Measuring inference with batch_size=64:   4%|▍         | 4/100 [00:00<00:09, 10.36it/s]Measuring inference with batch_size=64:   6%|▌         | 6/100 [00:00<00:09, 10.38it/s]Measuring inference with batch_size=64:   8%|▊         | 8/100 [00:00<00:08, 10.39it/s]Measuring inference with batch_size=64:  10%|█         | 10/100 [00:00<00:08, 10.31it/s]Measuring inference with batch_size=64:  12%|█▏        | 12/100 [00:01<00:08, 10.30it/s]Measuring inference with batch_size=64:  14%|█▍        | 14/100 [00:01<00:08, 10.28it/s]Measuring inference with batch_size=64:  16%|█▌        | 16/100 [00:01<00:08, 10.22it/s]Measuring inference with batch_size=64:  18%|█▊        | 18/100 [00:01<00:08, 10.23it/s]Measuring inference with batch_size=64:  20%|██        | 20/100 [00:01<00:07, 10.28it/s]Measuring inference with batch_size=64:  22%|██▏       | 22/100 [00:02<00:07, 10.31it/s]Measuring inference with batch_size=64:  24%|██▍       | 24/100 [00:02<00:07, 10.34it/s]Measuring inference with batch_size=64:  26%|██▌       | 26/100 [00:02<00:07, 10.31it/s]Measuring inference with batch_size=64:  28%|██▊       | 28/100 [00:02<00:06, 10.32it/s]Measuring inference with batch_size=64:  30%|███       | 30/100 [00:02<00:06, 10.32it/s]Measuring inference with batch_size=64:  32%|███▏      | 32/100 [00:03<00:06, 10.35it/s]Measuring inference with batch_size=64:  34%|███▍      | 34/100 [00:03<00:06, 10.32it/s]Measuring inference with batch_size=64:  36%|███▌      | 36/100 [00:03<00:06, 10.35it/s]Measuring inference with batch_size=64:  38%|███▊      | 38/100 [00:03<00:05, 10.36it/s]Measuring inference with batch_size=64:  40%|████      | 40/100 [00:03<00:05, 10.38it/s]Measuring inference with batch_size=64:  42%|████▏     | 42/100 [00:04<00:05, 10.34it/s]Measuring inference with batch_size=64:  44%|████▍     | 44/100 [00:04<00:05, 10.37it/s]Measuring inference with batch_size=64:  46%|████▌     | 46/100 [00:04<00:05, 10.38it/s]Measuring inference with batch_size=64:  48%|████▊     | 48/100 [00:04<00:05, 10.39it/s]Measuring inference with batch_size=64:  50%|█████     | 50/100 [00:04<00:04, 10.40it/s]Measuring inference with batch_size=64:  52%|█████▏    | 52/100 [00:05<00:04, 10.37it/s]Measuring inference with batch_size=64:  54%|█████▍    | 54/100 [00:05<00:04, 10.36it/s]Measuring inference with batch_size=64:  56%|█████▌    | 56/100 [00:05<00:04, 10.35it/s]Measuring inference with batch_size=64:  58%|█████▊    | 58/100 [00:05<00:04, 10.35it/s]Measuring inference with batch_size=64:  60%|██████    | 60/100 [00:05<00:03, 10.30it/s]Measuring inference with batch_size=64:  62%|██████▏   | 62/100 [00:06<00:03, 10.32it/s]Measuring inference with batch_size=64:  64%|██████▍   | 64/100 [00:06<00:03, 10.34it/s]Measuring inference with batch_size=64:  66%|██████▌   | 66/100 [00:06<00:03, 10.32it/s]Measuring inference with batch_size=64:  68%|██████▊   | 68/100 [00:06<00:03, 10.33it/s]Measuring inference with batch_size=64:  70%|███████   | 70/100 [00:06<00:02, 10.33it/s]Measuring inference with batch_size=64:  72%|███████▏  | 72/100 [00:06<00:02, 10.30it/s]Measuring inference with batch_size=64:  74%|███████▍  | 74/100 [00:07<00:02, 10.32it/s]Measuring inference with batch_size=64:  76%|███████▌  | 76/100 [00:07<00:02, 10.31it/s]Measuring inference with batch_size=64:  78%|███████▊  | 78/100 [00:07<00:02, 10.33it/s]Measuring inference with batch_size=64:  80%|████████  | 80/100 [00:07<00:01, 10.35it/s]Measuring inference with batch_size=64:  82%|████████▏ | 82/100 [00:07<00:01, 10.37it/s]Measuring inference with batch_size=64:  84%|████████▍ | 84/100 [00:08<00:01, 10.37it/s]Measuring inference with batch_size=64:  86%|████████▌ | 86/100 [00:08<00:01, 10.38it/s]Measuring inference with batch_size=64:  88%|████████▊ | 88/100 [00:08<00:01, 10.39it/s]Measuring inference with batch_size=64:  90%|█████████ | 90/100 [00:08<00:00, 10.39it/s]Measuring inference with batch_size=64:  92%|█████████▏| 92/100 [00:08<00:00, 10.38it/s]Measuring inference with batch_size=64:  94%|█████████▍| 94/100 [00:09<00:00, 10.39it/s]Measuring inference with batch_size=64:  96%|█████████▌| 96/100 [00:09<00:00, 10.39it/s]Measuring inference with batch_size=64:  98%|█████████▊| 98/100 [00:09<00:00, 10.38it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:09<00:00, 10.39it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:09<00:00, 10.34it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "121.062 \xB5s +/- 18.591 \xB5s [112.295 \xB5s, 300.884 \xB5\
        s]"
      batches_per_second: 8.35 K +/- 590.93 [3.32 K, 8.91 K]
    metrics:
      batches_per_second_max: 8905.104033970276
      batches_per_second_mean: 8346.662389246365
      batches_per_second_min: 3323.5372424722664
      batches_per_second_std: 590.9316244672681
      seconds_per_batch_max: 0.0003008842468261719
      seconds_per_batch_mean: 0.00012106180191040039
      seconds_per_batch_min: 0.00011229515075683594
      seconds_per_batch_std: 1.8590805621144905e-05
  gpu_to_cpu:
    human_readable:
      batch_latency: "32.349 \xB5s +/- 1.951 \xB5s [30.041 \xB5s, 44.584 \xB5s]"
      batches_per_second: 31.01 K +/- 1.59 K [22.43 K, 33.29 K]
    metrics:
      batches_per_second_max: 33288.12698412698
      batches_per_second_mean: 31008.37543182074
      batches_per_second_min: 22429.433155080213
      batches_per_second_std: 1591.844054140922
      seconds_per_batch_max: 4.458427429199219e-05
      seconds_per_batch_mean: 3.23486328125e-05
      seconds_per_batch_min: 3.0040740966796875e-05
      seconds_per_batch_std: 1.9512139224317907e-06
  on_device_inference:
    human_readable:
      batch_latency: 96.468 ms +/- 1.016 ms [95.512 ms, 99.611 ms]
      batches_per_second: 10.37 +/- 0.11 [10.04, 10.47]
    metrics:
      batches_per_second_max: 10.469898104371877
      batches_per_second_mean: 10.367241047462104
      batches_per_second_min: 10.039071513027826
      batches_per_second_std: 0.10726206038096045
      seconds_per_batch_max: 0.09961080551147461
      seconds_per_batch_mean: 0.09646818637847901
      seconds_per_batch_min: 0.09551191329956055
      seconds_per_batch_std: 0.001015702707735789
  total:
    human_readable:
      batch_latency: 96.622 ms +/- 1.017 ms [95.661 ms, 99.768 ms]
      batches_per_second: 10.35 +/- 0.11 [10.02, 10.45]
    metrics:
      batches_per_second_max: 10.453536972282938
      batches_per_second_mean: 10.350779724937532
      batches_per_second_min: 10.023261649345095
      batches_per_second_std: 0.10705794545317049
      seconds_per_batch_max: 0.09976792335510254
      seconds_per_batch_mean: 0.0966215968132019
      seconds_per_batch_min: 0.09566140174865723
      seconds_per_batch_std: 0.0010169267150644886

Warming up with batch_size=64:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=64:  10%|█         | 1/10 [00:00<00:05,  1.61it/s]Warming up with batch_size=64:  20%|██        | 2/10 [00:01<00:04,  1.66it/s]Warming up with batch_size=64:  30%|███       | 3/10 [00:01<00:04,  1.67it/s]Warming up with batch_size=64:  40%|████      | 4/10 [00:02<00:03,  1.68it/s]Warming up with batch_size=64:  50%|█████     | 5/10 [00:02<00:02,  1.69it/s]Warming up with batch_size=64:  60%|██████    | 6/10 [00:03<00:02,  1.69it/s]Warming up with batch_size=64:  70%|███████   | 7/10 [00:04<00:01,  1.69it/s]Warming up with batch_size=64:  80%|████████  | 8/10 [00:04<00:01,  1.69it/s]Warming up with batch_size=64:  90%|█████████ | 9/10 [00:05<00:00,  1.69it/s]Warming up with batch_size=64: 100%|██████████| 10/10 [00:05<00:00,  1.69it/s]Warming up with batch_size=64: 100%|██████████| 10/10 [00:05<00:00,  1.68it/s]
Measuring inference with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=64:   1%|          | 1/100 [00:00<00:58,  1.69it/s]Measuring inference with batch_size=64:   2%|▏         | 2/100 [00:01<00:57,  1.69it/s]Measuring inference with batch_size=64:   3%|▎         | 3/100 [00:01<00:57,  1.69it/s]Measuring inference with batch_size=64:   4%|▍         | 4/100 [00:02<00:56,  1.69it/s]Measuring inference with batch_size=64:   5%|▌         | 5/100 [00:02<00:56,  1.69it/s]Measuring inference with batch_size=64:   6%|▌         | 6/100 [00:03<00:55,  1.69it/s]Measuring inference with batch_size=64:   7%|▋         | 7/100 [00:04<00:54,  1.69it/s]Measuring inference with batch_size=64:   8%|▊         | 8/100 [00:04<00:54,  1.69it/s]Measuring inference with batch_size=64:   9%|▉         | 9/100 [00:05<00:53,  1.70it/s]Measuring inference with batch_size=64:  10%|█         | 10/100 [00:05<00:53,  1.70it/s]Measuring inference with batch_size=64:  11%|█         | 11/100 [00:06<00:52,  1.70it/s]Measuring inference with batch_size=64:  12%|█▏        | 12/100 [00:07<00:51,  1.70it/s]Measuring inference with batch_size=64:  13%|█▎        | 13/100 [00:07<00:51,  1.69it/s]Measuring inference with batch_size=64:  14%|█▍        | 14/100 [00:08<00:50,  1.69it/s]Measuring inference with batch_size=64:  15%|█▌        | 15/100 [00:08<00:50,  1.69it/s]Measuring inference with batch_size=64:  16%|█▌        | 16/100 [00:09<00:49,  1.69it/s]Measuring inference with batch_size=64:  17%|█▋        | 17/100 [00:10<00:49,  1.69it/s]Measuring inference with batch_size=64:  18%|█▊        | 18/100 [00:10<00:48,  1.69it/s]Measuring inference with batch_size=64:  19%|█▉        | 19/100 [00:11<00:47,  1.69it/s]Measuring inference with batch_size=64:  20%|██        | 20/100 [00:11<00:47,  1.69it/s]Measuring inference with batch_size=64:  21%|██        | 21/100 [00:12<00:46,  1.69it/s]Measuring inference with batch_size=64:  22%|██▏       | 22/100 [00:12<00:46,  1.69it/s]Measuring inference with batch_size=64:  23%|██▎       | 23/100 [00:13<00:45,  1.69it/s]Measuring inference with batch_size=64:  24%|██▍       | 24/100 [00:14<00:44,  1.69it/s]Measuring inference with batch_size=64:  25%|██▌       | 25/100 [00:14<00:44,  1.69it/s]Measuring inference with batch_size=64:  26%|██▌       | 26/100 [00:15<00:43,  1.69it/s]Measuring inference with batch_size=64:  27%|██▋       | 27/100 [00:15<00:43,  1.69it/s]Measuring inference with batch_size=64:  28%|██▊       | 28/100 [00:16<00:42,  1.69it/s]Measuring inference with batch_size=64:  29%|██▉       | 29/100 [00:17<00:42,  1.69it/s]Measuring inference with batch_size=64:  30%|███       | 30/100 [00:17<00:41,  1.69it/s]Measuring inference with batch_size=64:  31%|███       | 31/100 [00:18<00:40,  1.69it/s]Measuring inference with batch_size=64:  32%|███▏      | 32/100 [00:18<00:40,  1.69it/s]Measuring inference with batch_size=64:  33%|███▎      | 33/100 [00:19<00:39,  1.69it/s]Measuring inference with batch_size=64:  34%|███▍      | 34/100 [00:20<00:39,  1.69it/s]Measuring inference with batch_size=64:  35%|███▌      | 35/100 [00:20<00:38,  1.69it/s]Measuring inference with batch_size=64:  36%|███▌      | 36/100 [00:21<00:37,  1.69it/s]Measuring inference with batch_size=64:  37%|███▋      | 37/100 [00:21<00:37,  1.69it/s]Measuring inference with batch_size=64:  38%|███▊      | 38/100 [00:22<00:36,  1.69it/s]Measuring inference with batch_size=64:  39%|███▉      | 39/100 [00:23<00:36,  1.69it/s]Measuring inference with batch_size=64:  40%|████      | 40/100 [00:23<00:35,  1.69it/s]Measuring inference with batch_size=64:  41%|████      | 41/100 [00:24<00:34,  1.69it/s]Measuring inference with batch_size=64:  42%|████▏     | 42/100 [00:24<00:34,  1.69it/s]Measuring inference with batch_size=64:  43%|████▎     | 43/100 [00:25<00:33,  1.69it/s]Measuring inference with batch_size=64:  44%|████▍     | 44/100 [00:26<00:33,  1.69it/s]Measuring inference with batch_size=64:  45%|████▌     | 45/100 [00:26<00:32,  1.69it/s]Measuring inference with batch_size=64:  46%|████▌     | 46/100 [00:27<00:31,  1.69it/s]Measuring inference with batch_size=64:  47%|████▋     | 47/100 [00:27<00:31,  1.69it/s]Measuring inference with batch_size=64:  48%|████▊     | 48/100 [00:28<00:30,  1.69it/s]Measuring inference with batch_size=64:  49%|████▉     | 49/100 [00:28<00:30,  1.69it/s]Measuring inference with batch_size=64:  50%|█████     | 50/100 [00:29<00:29,  1.69it/s]Measuring inference with batch_size=64:  51%|█████     | 51/100 [00:30<00:28,  1.69it/s]Measuring inference with batch_size=64:  52%|█████▏    | 52/100 [00:30<00:28,  1.69it/s]Measuring inference with batch_size=64:  53%|█████▎    | 53/100 [00:31<00:27,  1.69it/s]Measuring inference with batch_size=64:  54%|█████▍    | 54/100 [00:31<00:27,  1.69it/s]Measuring inference with batch_size=64:  55%|█████▌    | 55/100 [00:32<00:26,  1.69it/s]Measuring inference with batch_size=64:  56%|█████▌    | 56/100 [00:33<00:26,  1.69it/s]Measuring inference with batch_size=64:  57%|█████▋    | 57/100 [00:33<00:25,  1.69it/s]Measuring inference with batch_size=64:  58%|█████▊    | 58/100 [00:34<00:24,  1.69it/s]Measuring inference with batch_size=64:  59%|█████▉    | 59/100 [00:34<00:24,  1.69it/s]Measuring inference with batch_size=64:  60%|██████    | 60/100 [00:35<00:23,  1.69it/s]Measuring inference with batch_size=64:  61%|██████    | 61/100 [00:36<00:23,  1.69it/s]Measuring inference with batch_size=64:  62%|██████▏   | 62/100 [00:36<00:22,  1.69it/s]Measuring inference with batch_size=64:  63%|██████▎   | 63/100 [00:37<00:21,  1.69it/s]Measuring inference with batch_size=64:  64%|██████▍   | 64/100 [00:37<00:21,  1.69it/s]Measuring inference with batch_size=64:  65%|██████▌   | 65/100 [00:38<00:20,  1.69it/s]Measuring inference with batch_size=64:  66%|██████▌   | 66/100 [00:39<00:20,  1.69it/s]Measuring inference with batch_size=64:  67%|██████▋   | 67/100 [00:39<00:19,  1.69it/s]Measuring inference with batch_size=64:  68%|██████▊   | 68/100 [00:40<00:19,  1.68it/s]Measuring inference with batch_size=64:  69%|██████▉   | 69/100 [00:40<00:18,  1.68it/s]Measuring inference with batch_size=64:  70%|███████   | 70/100 [00:41<00:17,  1.69it/s]Measuring inference with batch_size=64:  71%|███████   | 71/100 [00:42<00:17,  1.69it/s]Measuring inference with batch_size=64:  72%|███████▏  | 72/100 [00:42<00:16,  1.69it/s]Measuring inference with batch_size=64:  73%|███████▎  | 73/100 [00:43<00:15,  1.69it/s]Measuring inference with batch_size=64:  74%|███████▍  | 74/100 [00:43<00:15,  1.69it/s]Measuring inference with batch_size=64:  75%|███████▌  | 75/100 [00:44<00:14,  1.69it/s]Measuring inference with batch_size=64:  76%|███████▌  | 76/100 [00:44<00:14,  1.69it/s]Measuring inference with batch_size=64:  77%|███████▋  | 77/100 [00:45<00:13,  1.69it/s]Measuring inference with batch_size=64:  78%|███████▊  | 78/100 [00:46<00:13,  1.69it/s]Measuring inference with batch_size=64:  79%|███████▉  | 79/100 [00:46<00:12,  1.69it/s]Measuring inference with batch_size=64:  80%|████████  | 80/100 [00:47<00:11,  1.69it/s]Measuring inference with batch_size=64:  81%|████████  | 81/100 [00:47<00:11,  1.69it/s]Measuring inference with batch_size=64:  82%|████████▏ | 82/100 [00:48<00:10,  1.69it/s]Measuring inference with batch_size=64:  83%|████████▎ | 83/100 [00:49<00:10,  1.69it/s]Measuring inference with batch_size=64:  84%|████████▍ | 84/100 [00:49<00:09,  1.69it/s]Measuring inference with batch_size=64:  85%|████████▌ | 85/100 [00:50<00:08,  1.69it/s]Measuring inference with batch_size=64:  86%|████████▌ | 86/100 [00:50<00:08,  1.69it/s]Measuring inference with batch_size=64:  87%|████████▋ | 87/100 [00:51<00:07,  1.69it/s]Measuring inference with batch_size=64:  88%|████████▊ | 88/100 [00:52<00:07,  1.69it/s]Measuring inference with batch_size=64:  89%|████████▉ | 89/100 [00:52<00:06,  1.69it/s]Measuring inference with batch_size=64:  90%|█████████ | 90/100 [00:53<00:05,  1.69it/s]Measuring inference with batch_size=64:  91%|█████████ | 91/100 [00:53<00:05,  1.69it/s]Measuring inference with batch_size=64:  92%|█████████▏| 92/100 [00:54<00:04,  1.69it/s]Measuring inference with batch_size=64:  93%|█████████▎| 93/100 [00:55<00:04,  1.69it/s]Measuring inference with batch_size=64:  94%|█████████▍| 94/100 [00:55<00:03,  1.69it/s]Measuring inference with batch_size=64:  95%|█████████▌| 95/100 [00:56<00:02,  1.69it/s]Measuring inference with batch_size=64:  96%|█████████▌| 96/100 [00:56<00:02,  1.69it/s]Measuring inference with batch_size=64:  97%|█████████▋| 97/100 [00:57<00:01,  1.69it/s]Measuring inference with batch_size=64:  98%|█████████▊| 98/100 [00:58<00:01,  1.69it/s]Measuring inference with batch_size=64:  99%|█████████▉| 99/100 [00:58<00:00,  1.69it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:59<00:00,  1.69it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:59<00:00,  1.69it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
Timing results (batch_size=64):
  cpu_to_gpu:
    human_readable:
      batch_latency: 3.961 ms +/- 1.282 ms [3.590 ms, 12.873 ms]
      batches_per_second: 261.60 +/- 29.15 [77.68, 278.58]
    metrics:
      batches_per_second_max: 278.5802337938363
      batches_per_second_mean: 261.60081525557024
      batches_per_second_min: 77.6794888415594
      batches_per_second_std: 29.147215687545884
      seconds_per_batch_max: 0.012873411178588867
      seconds_per_batch_mean: 0.003960525989532471
      seconds_per_batch_min: 0.003589630126953125
      seconds_per_batch_std: 0.0012821046246680628
  gpu_to_cpu:
    human_readable:
      batch_latency: "22.007 ms +/- 989.210 \xB5s [13.190 ms, 22.857 ms]"
      batches_per_second: 45.58 +/- 3.19 [43.75, 75.82]
    metrics:
      batches_per_second_max: 75.81620331875203
      batches_per_second_mean: 45.582102001335194
      batches_per_second_min: 43.75082404973505
      batches_per_second_std: 3.191288922669123
      seconds_per_batch_max: 0.022856712341308594
      seconds_per_batch_mean: 0.02200690269470215
      seconds_per_batch_min: 0.01318979263305664
      seconds_per_batch_std: 0.000989210041557429
  on_device_inference:
    human_readable:
      batch_latency: 565.763 ms +/- 1.780 ms [561.809 ms, 576.228 ms]
      batches_per_second: 1.77 +/- 0.01 [1.74, 1.78]
    metrics:
      batches_per_second_max: 1.7799634102713504
      batches_per_second_mean: 1.7675411958475218
      batches_per_second_min: 1.735425181453398
      batches_per_second_std: 0.005528074825481362
      seconds_per_batch_max: 0.5762276649475098
      seconds_per_batch_mean: 0.5657632422447204
      seconds_per_batch_min: 0.5618093013763428
      seconds_per_batch_std: 0.0017796737320807852
  total:
    human_readable:
      batch_latency: 591.731 ms +/- 1.898 ms [587.589 ms, 601.412 ms]
      batches_per_second: 1.69 +/- 0.01 [1.66, 1.70]
    metrics:
      batches_per_second_max: 1.701871151641797
      batches_per_second_mean: 1.689975320558328
      batches_per_second_min: 1.6627534930555143
      batches_per_second_std: 0.005390895717016597
      seconds_per_batch_max: 0.6014120578765869
      seconds_per_batch_mean: 0.591730670928955
      seconds_per_batch_min: 0.5875885486602783
      seconds_per_batch_std: 0.0018977554278984345

learner.model.forward:
  device: cuda
  flops: 269136368
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 52
        total: 104
      frequency: 2.10 GHz
      model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
    gpus:
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    memory:
      available: 177.79 GB
      total: 187.56 GB
      used: 8.39 GB
    system:
      node: charybdis
      release: 4.15.0-167-generic
      system: Linux
  max_inference_memory: 9105660928
  params: 3794322
  post_inference_memory: 9058081792
  pre_inference_memory: 1855218176
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "121.062 \xB5s +/- 18.591 \xB5s [112.295 \xB5s, 300.884 \xB5\
            s]"
          batches_per_second: 8.35 K +/- 590.93 [3.32 K, 8.91 K]
        metrics:
          batches_per_second_max: 8905.104033970276
          batches_per_second_mean: 8346.662389246365
          batches_per_second_min: 3323.5372424722664
          batches_per_second_std: 590.9316244672681
          seconds_per_batch_max: 0.0003008842468261719
          seconds_per_batch_mean: 0.00012106180191040039
          seconds_per_batch_min: 0.00011229515075683594
          seconds_per_batch_std: 1.8590805621144905e-05
      gpu_to_cpu:
        human_readable:
          batch_latency: "32.349 \xB5s +/- 1.951 \xB5s [30.041 \xB5s, 44.584 \xB5\
            s]"
          batches_per_second: 31.01 K +/- 1.59 K [22.43 K, 33.29 K]
        metrics:
          batches_per_second_max: 33288.12698412698
          batches_per_second_mean: 31008.37543182074
          batches_per_second_min: 22429.433155080213
          batches_per_second_std: 1591.844054140922
          seconds_per_batch_max: 4.458427429199219e-05
          seconds_per_batch_mean: 3.23486328125e-05
          seconds_per_batch_min: 3.0040740966796875e-05
          seconds_per_batch_std: 1.9512139224317907e-06
      on_device_inference:
        human_readable:
          batch_latency: 96.468 ms +/- 1.016 ms [95.512 ms, 99.611 ms]
          batches_per_second: 10.37 +/- 0.11 [10.04, 10.47]
        metrics:
          batches_per_second_max: 10.469898104371877
          batches_per_second_mean: 10.367241047462104
          batches_per_second_min: 10.039071513027826
          batches_per_second_std: 0.10726206038096045
          seconds_per_batch_max: 0.09961080551147461
          seconds_per_batch_mean: 0.09646818637847901
          seconds_per_batch_min: 0.09551191329956055
          seconds_per_batch_std: 0.001015702707735789
      total:
        human_readable:
          batch_latency: 96.622 ms +/- 1.017 ms [95.661 ms, 99.768 ms]
          batches_per_second: 10.35 +/- 0.11 [10.02, 10.45]
        metrics:
          batches_per_second_max: 10.453536972282938
          batches_per_second_mean: 10.350779724937532
          batches_per_second_min: 10.023261649345095
          batches_per_second_std: 0.10705794545317049
          seconds_per_batch_max: 0.09976792335510254
          seconds_per_batch_mean: 0.0966215968132019
          seconds_per_batch_min: 0.09566140174865723
          seconds_per_batch_std: 0.0010169267150644886
    batch_size_64:
      cpu_to_gpu:
        human_readable:
          batch_latency: 3.961 ms +/- 1.282 ms [3.590 ms, 12.873 ms]
          batches_per_second: 261.60 +/- 29.15 [77.68, 278.58]
        metrics:
          batches_per_second_max: 278.5802337938363
          batches_per_second_mean: 261.60081525557024
          batches_per_second_min: 77.6794888415594
          batches_per_second_std: 29.147215687545884
          seconds_per_batch_max: 0.012873411178588867
          seconds_per_batch_mean: 0.003960525989532471
          seconds_per_batch_min: 0.003589630126953125
          seconds_per_batch_std: 0.0012821046246680628
      gpu_to_cpu:
        human_readable:
          batch_latency: "22.007 ms +/- 989.210 \xB5s [13.190 ms, 22.857 ms]"
          batches_per_second: 45.58 +/- 3.19 [43.75, 75.82]
        metrics:
          batches_per_second_max: 75.81620331875203
          batches_per_second_mean: 45.582102001335194
          batches_per_second_min: 43.75082404973505
          batches_per_second_std: 3.191288922669123
          seconds_per_batch_max: 0.022856712341308594
          seconds_per_batch_mean: 0.02200690269470215
          seconds_per_batch_min: 0.01318979263305664
          seconds_per_batch_std: 0.000989210041557429
      on_device_inference:
        human_readable:
          batch_latency: 565.763 ms +/- 1.780 ms [561.809 ms, 576.228 ms]
          batches_per_second: 1.77 +/- 0.01 [1.74, 1.78]
        metrics:
          batches_per_second_max: 1.7799634102713504
          batches_per_second_mean: 1.7675411958475218
          batches_per_second_min: 1.735425181453398
          batches_per_second_std: 0.005528074825481362
          seconds_per_batch_max: 0.5762276649475098
          seconds_per_batch_mean: 0.5657632422447204
          seconds_per_batch_min: 0.5618093013763428
          seconds_per_batch_std: 0.0017796737320807852
      total:
        human_readable:
          batch_latency: 591.731 ms +/- 1.898 ms [587.589 ms, 601.412 ms]
          batches_per_second: 1.69 +/- 0.01 [1.66, 1.70]
        metrics:
          batches_per_second_max: 1.701871151641797
          batches_per_second_mean: 1.689975320558328
          batches_per_second_min: 1.6627534930555143
          batches_per_second_std: 0.005390895717016597
          seconds_per_batch_max: 0.6014120578765869
          seconds_per_batch_mean: 0.591730670928955
          seconds_per_batch_min: 0.5875885486602783
          seconds_per_batch_std: 0.0018977554278984345

==== Benchmarking CoX3DLearner (l) ====
== Benchmarking learner.infer ==
Machine info:
  cpu:
    architecture: x86_64
    cores:
      physical: 52
      total: 104
    frequency: 2.10 GHz
    model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
  gpus:
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  memory:
    available: 177.80 GB
    total: 187.56 GB
    used: 8.39 GB
  system:
    node: charybdis
    release: 4.15.0-167-generic
    system: Linux

Model device: cuda:0
Allocated GPU memory prior to inference: 5085070336 (4.74 GB)
Allocated GPU memory after to inference: 8203826176 (7.64 GB)
Max allocated GPU memory during inference: 8215507968 (7.65 GB)
Warming up with batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=8:  10%|█         | 1/10 [00:00<00:02,  4.37it/s]Warming up with batch_size=8:  20%|██        | 2/10 [00:00<00:01,  4.52it/s]Warming up with batch_size=8:  30%|███       | 3/10 [00:00<00:01,  4.54it/s]Warming up with batch_size=8:  40%|████      | 4/10 [00:00<00:01,  4.56it/s]Warming up with batch_size=8:  50%|█████     | 5/10 [00:01<00:01,  4.56it/s]Warming up with batch_size=8:  60%|██████    | 6/10 [00:01<00:00,  4.57it/s]Warming up with batch_size=8:  70%|███████   | 7/10 [00:01<00:00,  4.55it/s]Warming up with batch_size=8:  80%|████████  | 8/10 [00:01<00:00,  4.55it/s]Warming up with batch_size=8:  90%|█████████ | 9/10 [00:01<00:00,  4.56it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:02<00:00,  4.56it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:02<00:00,  4.55it/s]
Measuring inference with batch_size=8:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=8:   1%|          | 1/100 [00:00<00:21,  4.57it/s]Measuring inference with batch_size=8:   2%|▏         | 2/100 [00:00<00:21,  4.53it/s]Measuring inference with batch_size=8:   3%|▎         | 3/100 [00:00<00:21,  4.54it/s]Measuring inference with batch_size=8:   4%|▍         | 4/100 [00:00<00:21,  4.56it/s]Measuring inference with batch_size=8:   5%|▌         | 5/100 [00:01<00:20,  4.57it/s]Measuring inference with batch_size=8:   6%|▌         | 6/100 [00:01<00:20,  4.56it/s]Measuring inference with batch_size=8:   7%|▋         | 7/100 [00:01<00:20,  4.57it/s]Measuring inference with batch_size=8:   8%|▊         | 8/100 [00:01<00:20,  4.55it/s]Measuring inference with batch_size=8:   9%|▉         | 9/100 [00:01<00:19,  4.57it/s]Measuring inference with batch_size=8:  10%|█         | 10/100 [00:02<00:19,  4.58it/s]Measuring inference with batch_size=8:  11%|█         | 11/100 [00:02<00:19,  4.60it/s]Measuring inference with batch_size=8:  12%|█▏        | 12/100 [00:02<00:19,  4.61it/s]Measuring inference with batch_size=8:  13%|█▎        | 13/100 [00:02<00:19,  4.57it/s]Measuring inference with batch_size=8:  14%|█▍        | 14/100 [00:03<00:18,  4.56it/s]Measuring inference with batch_size=8:  15%|█▌        | 15/100 [00:03<00:18,  4.55it/s]Measuring inference with batch_size=8:  16%|█▌        | 16/100 [00:03<00:18,  4.55it/s]Measuring inference with batch_size=8:  17%|█▋        | 17/100 [00:03<00:18,  4.54it/s]Measuring inference with batch_size=8:  18%|█▊        | 18/100 [00:03<00:18,  4.54it/s]Measuring inference with batch_size=8:  19%|█▉        | 19/100 [00:04<00:17,  4.52it/s]Measuring inference with batch_size=8:  20%|██        | 20/100 [00:04<00:17,  4.53it/s]Measuring inference with batch_size=8:  21%|██        | 21/100 [00:04<00:17,  4.52it/s]Measuring inference with batch_size=8:  22%|██▏       | 22/100 [00:04<00:17,  4.51it/s]Measuring inference with batch_size=8:  23%|██▎       | 23/100 [00:05<00:17,  4.52it/s]Measuring inference with batch_size=8:  24%|██▍       | 24/100 [00:05<00:16,  4.52it/s]Measuring inference with batch_size=8:  25%|██▌       | 25/100 [00:05<00:16,  4.53it/s]Measuring inference with batch_size=8:  26%|██▌       | 26/100 [00:05<00:16,  4.52it/s]Measuring inference with batch_size=8:  27%|██▋       | 27/100 [00:05<00:16,  4.52it/s]Measuring inference with batch_size=8:  28%|██▊       | 28/100 [00:06<00:15,  4.53it/s]Measuring inference with batch_size=8:  29%|██▉       | 29/100 [00:06<00:15,  4.56it/s]Measuring inference with batch_size=8:  30%|███       | 30/100 [00:06<00:15,  4.58it/s]Measuring inference with batch_size=8:  31%|███       | 31/100 [00:06<00:14,  4.60it/s]Measuring inference with batch_size=8:  32%|███▏      | 32/100 [00:07<00:14,  4.60it/s]Measuring inference with batch_size=8:  33%|███▎      | 33/100 [00:07<00:14,  4.58it/s]Measuring inference with batch_size=8:  34%|███▍      | 34/100 [00:07<00:14,  4.59it/s]Measuring inference with batch_size=8:  35%|███▌      | 35/100 [00:07<00:14,  4.60it/s]Measuring inference with batch_size=8:  36%|███▌      | 36/100 [00:07<00:13,  4.62it/s]Measuring inference with batch_size=8:  37%|███▋      | 37/100 [00:08<00:13,  4.66it/s]Measuring inference with batch_size=8:  38%|███▊      | 38/100 [00:08<00:13,  4.68it/s]Measuring inference with batch_size=8:  39%|███▉      | 39/100 [00:08<00:12,  4.71it/s]Measuring inference with batch_size=8:  40%|████      | 40/100 [00:08<00:12,  4.73it/s]Measuring inference with batch_size=8:  41%|████      | 41/100 [00:08<00:12,  4.72it/s]Measuring inference with batch_size=8:  42%|████▏     | 42/100 [00:09<00:12,  4.68it/s]Measuring inference with batch_size=8:  43%|████▎     | 43/100 [00:09<00:12,  4.67it/s]Measuring inference with batch_size=8:  44%|████▍     | 44/100 [00:09<00:11,  4.67it/s]Measuring inference with batch_size=8:  45%|████▌     | 45/100 [00:09<00:11,  4.66it/s]Measuring inference with batch_size=8:  46%|████▌     | 46/100 [00:10<00:11,  4.65it/s]Measuring inference with batch_size=8:  47%|████▋     | 47/100 [00:10<00:11,  4.66it/s]Measuring inference with batch_size=8:  48%|████▊     | 48/100 [00:10<00:11,  4.65it/s]Measuring inference with batch_size=8:  49%|████▉     | 49/100 [00:10<00:10,  4.65it/s]Measuring inference with batch_size=8:  50%|█████     | 50/100 [00:10<00:10,  4.62it/s]Measuring inference with batch_size=8:  51%|█████     | 51/100 [00:11<00:10,  4.63it/s]Measuring inference with batch_size=8:  52%|█████▏    | 52/100 [00:11<00:10,  4.64it/s]Measuring inference with batch_size=8:  53%|█████▎    | 53/100 [00:11<00:10,  4.64it/s]Measuring inference with batch_size=8:  54%|█████▍    | 54/100 [00:11<00:09,  4.63it/s]Measuring inference with batch_size=8:  55%|█████▌    | 55/100 [00:11<00:09,  4.62it/s]Measuring inference with batch_size=8:  56%|█████▌    | 56/100 [00:12<00:09,  4.62it/s]Measuring inference with batch_size=8:  57%|█████▋    | 57/100 [00:12<00:09,  4.62it/s]Measuring inference with batch_size=8:  58%|█████▊    | 58/100 [00:12<00:09,  4.60it/s]Measuring inference with batch_size=8:  59%|█████▉    | 59/100 [00:12<00:08,  4.62it/s]Measuring inference with batch_size=8:  60%|██████    | 60/100 [00:13<00:08,  4.63it/s]Measuring inference with batch_size=8:  61%|██████    | 61/100 [00:13<00:08,  4.63it/s]Measuring inference with batch_size=8:  62%|██████▏   | 62/100 [00:13<00:08,  4.60it/s]Measuring inference with batch_size=8:  63%|██████▎   | 63/100 [00:13<00:08,  4.60it/s]Measuring inference with batch_size=8:  64%|██████▍   | 64/100 [00:13<00:07,  4.61it/s]Measuring inference with batch_size=8:  65%|██████▌   | 65/100 [00:14<00:07,  4.63it/s]Measuring inference with batch_size=8:  66%|██████▌   | 66/100 [00:14<00:07,  4.63it/s]Measuring inference with batch_size=8:  67%|██████▋   | 67/100 [00:14<00:07,  4.64it/s]Measuring inference with batch_size=8:  68%|██████▊   | 68/100 [00:14<00:06,  4.65it/s]Measuring inference with batch_size=8:  69%|██████▉   | 69/100 [00:14<00:06,  4.65it/s]Measuring inference with batch_size=8:  70%|███████   | 70/100 [00:15<00:06,  4.63it/s]Measuring inference with batch_size=8:  71%|███████   | 71/100 [00:15<00:06,  4.64it/s]Measuring inference with batch_size=8:  72%|███████▏  | 72/100 [00:15<00:06,  4.64it/s]Measuring inference with batch_size=8:  73%|███████▎  | 73/100 [00:15<00:05,  4.64it/s]Measuring inference with batch_size=8:  74%|███████▍  | 74/100 [00:16<00:05,  4.64it/s]Measuring inference with batch_size=8:  75%|███████▌  | 75/100 [00:16<00:05,  4.63it/s]Measuring inference with batch_size=8:  76%|███████▌  | 76/100 [00:16<00:05,  4.62it/s]Measuring inference with batch_size=8:  77%|███████▋  | 77/100 [00:16<00:04,  4.63it/s]Measuring inference with batch_size=8:  78%|███████▊  | 78/100 [00:16<00:04,  4.63it/s]Measuring inference with batch_size=8:  79%|███████▉  | 79/100 [00:17<00:04,  4.64it/s]Measuring inference with batch_size=8:  80%|████████  | 80/100 [00:17<00:04,  4.65it/s]Measuring inference with batch_size=8:  81%|████████  | 81/100 [00:17<00:04,  4.65it/s]Measuring inference with batch_size=8:  82%|████████▏ | 82/100 [00:17<00:03,  4.61it/s]Measuring inference with batch_size=8:  83%|████████▎ | 83/100 [00:18<00:03,  4.61it/s]Measuring inference with batch_size=8:  84%|████████▍ | 84/100 [00:18<00:03,  4.59it/s]Measuring inference with batch_size=8:  85%|████████▌ | 85/100 [00:18<00:03,  4.60it/s]Measuring inference with batch_size=8:  86%|████████▌ | 86/100 [00:18<00:03,  4.62it/s]Measuring inference with batch_size=8:  87%|████████▋ | 87/100 [00:18<00:02,  4.63it/s]Measuring inference with batch_size=8:  88%|████████▊ | 88/100 [00:19<00:02,  4.65it/s]Measuring inference with batch_size=8:  89%|████████▉ | 89/100 [00:19<00:02,  4.65it/s]Measuring inference with batch_size=8:  90%|█████████ | 90/100 [00:19<00:02,  4.63it/s]Measuring inference with batch_size=8:  91%|█████████ | 91/100 [00:19<00:01,  4.61it/s]Measuring inference with batch_size=8:  92%|█████████▏| 92/100 [00:19<00:01,  4.62it/s]Measuring inference with batch_size=8:  93%|█████████▎| 93/100 [00:20<00:01,  4.62it/s]Measuring inference with batch_size=8:  94%|█████████▍| 94/100 [00:20<00:01,  4.64it/s]Measuring inference with batch_size=8:  95%|█████████▌| 95/100 [00:20<00:01,  4.65it/s]Measuring inference with batch_size=8:  96%|█████████▌| 96/100 [00:20<00:00,  4.66it/s]Measuring inference with batch_size=8:  97%|█████████▋| 97/100 [00:21<00:00,  4.67it/s]Measuring inference with batch_size=8:  98%|█████████▊| 98/100 [00:21<00:00,  4.63it/s]Measuring inference with batch_size=8:  99%|█████████▉| 99/100 [00:21<00:00,  4.61it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [00:21<00:00,  4.63it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [00:21<00:00,  4.61it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "1.204 \xB5s +/- 0.225 \xB5s [0.954 \xB5s, 2.146 \xB5s]"
      batches_per_second: 856.50 K +/- 143.83 K [466.03 K, 1.05 M]
    metrics:
      batches_per_second_max: 1048576.0
      batches_per_second_mean: 856503.5073015873
      batches_per_second_min: 466033.77777777775
      batches_per_second_std: 143829.55693608779
      seconds_per_batch_max: 2.1457672119140625e-06
      seconds_per_batch_mean: 1.2040138244628906e-06
      seconds_per_batch_min: 9.5367431640625e-07
      seconds_per_batch_std: 2.24607511537275e-07
  gpu_to_cpu:
    human_readable:
      batch_latency: "43.476 \xB5s +/- 8.503 \xB5s [35.763 \xB5s, 81.301 \xB5s]"
      batches_per_second: 23.67 K +/- 3.48 K [12.30 K, 27.96 K]
    metrics:
      batches_per_second_max: 27962.02666666667
      batches_per_second_mean: 23666.305115619045
      batches_per_second_min: 12300.011730205279
      batches_per_second_std: 3480.07605751578
      seconds_per_batch_max: 8.130073547363281e-05
      seconds_per_batch_mean: 4.347562789916992e-05
      seconds_per_batch_min: 3.5762786865234375e-05
      seconds_per_batch_std: 8.502648017011778e-06
  on_device_inference:
    human_readable:
      batch_latency: 216.792 ms +/- 3.053 ms [209.245 ms, 223.494 ms]
      batches_per_second: 4.61 +/- 0.06 [4.47, 4.78]
    metrics:
      batches_per_second_max: 4.779087481498615
      batches_per_second_mean: 4.613638088689235
      batches_per_second_min: 4.474396762964836
      batches_per_second_std: 0.06489579633218134
      seconds_per_batch_max: 0.2234938144683838
      seconds_per_batch_mean: 0.21679162979125977
      seconds_per_batch_min: 0.209244966506958
      seconds_per_batch_std: 0.0030534378583087244
  total:
    human_readable:
      batch_latency: 216.836 ms +/- 3.055 ms [209.286 ms, 223.553 ms]
      batches_per_second: 4.61 +/- 0.06 [4.47, 4.78]
    metrics:
      batches_per_second_max: 4.7781564989154806
      batches_per_second_mean: 4.612688004199629
      batches_per_second_min: 4.473203782644076
      batches_per_second_std: 0.06490251802918706
      seconds_per_batch_max: 0.22355341911315918
      seconds_per_batch_mean: 0.2168363094329834
      seconds_per_batch_min: 0.20928573608398438
      seconds_per_batch_std: 0.0030550156783498146

Warming up with batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=8:  10%|█         | 1/10 [00:00<00:05,  1.76it/s]Warming up with batch_size=8:  20%|██        | 2/10 [00:01<00:04,  1.80it/s]Warming up with batch_size=8:  30%|███       | 3/10 [00:01<00:03,  1.81it/s]Warming up with batch_size=8:  40%|████      | 4/10 [00:02<00:03,  1.81it/s]Warming up with batch_size=8:  50%|█████     | 5/10 [00:02<00:02,  1.81it/s]Warming up with batch_size=8:  60%|██████    | 6/10 [00:03<00:02,  1.81it/s]Warming up with batch_size=8:  70%|███████   | 7/10 [00:03<00:01,  1.81it/s]Warming up with batch_size=8:  80%|████████  | 8/10 [00:04<00:01,  1.82it/s]Warming up with batch_size=8:  90%|█████████ | 9/10 [00:04<00:00,  1.82it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:05<00:00,  1.82it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:05<00:00,  1.81it/s]
Measuring inference with batch_size=8:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=8:   1%|          | 1/100 [00:00<00:54,  1.80it/s]Measuring inference with batch_size=8:   2%|▏         | 2/100 [00:01<00:54,  1.81it/s]Measuring inference with batch_size=8:   3%|▎         | 3/100 [00:01<00:53,  1.81it/s]Measuring inference with batch_size=8:   4%|▍         | 4/100 [00:02<00:52,  1.82it/s]Measuring inference with batch_size=8:   5%|▌         | 5/100 [00:02<00:52,  1.82it/s]Measuring inference with batch_size=8:   6%|▌         | 6/100 [00:03<00:51,  1.82it/s]Measuring inference with batch_size=8:   7%|▋         | 7/100 [00:03<00:51,  1.82it/s]Measuring inference with batch_size=8:   8%|▊         | 8/100 [00:04<00:50,  1.82it/s]Measuring inference with batch_size=8:   9%|▉         | 9/100 [00:04<00:50,  1.80it/s]Measuring inference with batch_size=8:  10%|█         | 10/100 [00:05<00:49,  1.80it/s]Measuring inference with batch_size=8:  11%|█         | 11/100 [00:06<00:49,  1.79it/s]Measuring inference with batch_size=8:  12%|█▏        | 12/100 [00:06<00:48,  1.80it/s]Measuring inference with batch_size=8:  13%|█▎        | 13/100 [00:07<00:48,  1.80it/s]Measuring inference with batch_size=8:  14%|█▍        | 14/100 [00:07<00:47,  1.81it/s]Measuring inference with batch_size=8:  15%|█▌        | 15/100 [00:08<00:46,  1.81it/s]Measuring inference with batch_size=8:  16%|█▌        | 16/100 [00:08<00:46,  1.82it/s]Measuring inference with batch_size=8:  17%|█▋        | 17/100 [00:09<00:45,  1.82it/s]Measuring inference with batch_size=8:  18%|█▊        | 18/100 [00:09<00:45,  1.82it/s]Measuring inference with batch_size=8:  19%|█▉        | 19/100 [00:10<00:44,  1.80it/s]Measuring inference with batch_size=8:  20%|██        | 20/100 [00:11<00:46,  1.71it/s]Measuring inference with batch_size=8:  21%|██        | 21/100 [00:11<00:46,  1.70it/s]Measuring inference with batch_size=8:  22%|██▏       | 22/100 [00:12<00:46,  1.68it/s]Measuring inference with batch_size=8:  23%|██▎       | 23/100 [00:12<00:44,  1.72it/s]Measuring inference with batch_size=8:  24%|██▍       | 24/100 [00:13<00:43,  1.75it/s]Measuring inference with batch_size=8:  25%|██▌       | 25/100 [00:14<00:42,  1.77it/s]Measuring inference with batch_size=8:  26%|██▌       | 26/100 [00:14<00:41,  1.79it/s]Measuring inference with batch_size=8:  27%|██▋       | 27/100 [00:15<00:40,  1.80it/s]Measuring inference with batch_size=8:  28%|██▊       | 28/100 [00:15<00:39,  1.80it/s]Measuring inference with batch_size=8:  29%|██▉       | 29/100 [00:16<00:39,  1.81it/s]Measuring inference with batch_size=8:  30%|███       | 30/100 [00:16<00:38,  1.81it/s]Measuring inference with batch_size=8:  31%|███       | 31/100 [00:17<00:38,  1.82it/s]Measuring inference with batch_size=8:  32%|███▏      | 32/100 [00:17<00:37,  1.82it/s]Measuring inference with batch_size=8:  33%|███▎      | 33/100 [00:18<00:36,  1.82it/s]Measuring inference with batch_size=8:  34%|███▍      | 34/100 [00:18<00:36,  1.82it/s]Measuring inference with batch_size=8:  35%|███▌      | 35/100 [00:19<00:35,  1.82it/s]Measuring inference with batch_size=8:  36%|███▌      | 36/100 [00:20<00:35,  1.82it/s]Measuring inference with batch_size=8:  37%|███▋      | 37/100 [00:20<00:34,  1.82it/s]Measuring inference with batch_size=8:  38%|███▊      | 38/100 [00:21<00:34,  1.82it/s]Measuring inference with batch_size=8:  39%|███▉      | 39/100 [00:21<00:33,  1.82it/s]Measuring inference with batch_size=8:  40%|████      | 40/100 [00:22<00:32,  1.82it/s]Measuring inference with batch_size=8:  41%|████      | 41/100 [00:22<00:32,  1.82it/s]Measuring inference with batch_size=8:  42%|████▏     | 42/100 [00:23<00:31,  1.82it/s]Measuring inference with batch_size=8:  43%|████▎     | 43/100 [00:23<00:31,  1.82it/s]Measuring inference with batch_size=8:  44%|████▍     | 44/100 [00:24<00:30,  1.82it/s]Measuring inference with batch_size=8:  45%|████▌     | 45/100 [00:24<00:30,  1.82it/s]Measuring inference with batch_size=8:  46%|████▌     | 46/100 [00:25<00:29,  1.82it/s]Measuring inference with batch_size=8:  47%|████▋     | 47/100 [00:26<00:29,  1.81it/s]Measuring inference with batch_size=8:  48%|████▊     | 48/100 [00:26<00:28,  1.80it/s]Measuring inference with batch_size=8:  49%|████▉     | 49/100 [00:27<00:28,  1.80it/s]Measuring inference with batch_size=8:  50%|█████     | 50/100 [00:27<00:27,  1.80it/s]Measuring inference with batch_size=8:  51%|█████     | 51/100 [00:28<00:27,  1.81it/s]Measuring inference with batch_size=8:  52%|█████▏    | 52/100 [00:28<00:26,  1.81it/s]Measuring inference with batch_size=8:  53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s]Measuring inference with batch_size=8:  54%|█████▍    | 54/100 [00:29<00:25,  1.81it/s]Measuring inference with batch_size=8:  55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s]Measuring inference with batch_size=8:  56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s]Measuring inference with batch_size=8:  57%|█████▋    | 57/100 [00:31<00:23,  1.82it/s]Measuring inference with batch_size=8:  58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s]Measuring inference with batch_size=8:  59%|█████▉    | 59/100 [00:32<00:22,  1.82it/s]Measuring inference with batch_size=8:  60%|██████    | 60/100 [00:33<00:21,  1.82it/s]Measuring inference with batch_size=8:  61%|██████    | 61/100 [00:33<00:21,  1.82it/s]Measuring inference with batch_size=8:  62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s]Measuring inference with batch_size=8:  63%|██████▎   | 63/100 [00:34<00:20,  1.82it/s]Measuring inference with batch_size=8:  64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s]Measuring inference with batch_size=8:  65%|██████▌   | 65/100 [00:36<00:19,  1.82it/s]Measuring inference with batch_size=8:  66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s]Measuring inference with batch_size=8:  67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s]Measuring inference with batch_size=8:  68%|██████▊   | 68/100 [00:37<00:17,  1.81it/s]Measuring inference with batch_size=8:  69%|██████▉   | 69/100 [00:38<00:17,  1.81it/s]Measuring inference with batch_size=8:  70%|███████   | 70/100 [00:38<00:16,  1.81it/s]Measuring inference with batch_size=8:  71%|███████   | 71/100 [00:39<00:16,  1.81it/s]Measuring inference with batch_size=8:  72%|███████▏  | 72/100 [00:39<00:15,  1.81it/s]Measuring inference with batch_size=8:  73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s]Measuring inference with batch_size=8:  74%|███████▍  | 74/100 [00:40<00:14,  1.81it/s]Measuring inference with batch_size=8:  75%|███████▌  | 75/100 [00:41<00:13,  1.81it/s]Measuring inference with batch_size=8:  76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s]Measuring inference with batch_size=8:  77%|███████▋  | 77/100 [00:42<00:12,  1.80it/s]Measuring inference with batch_size=8:  78%|███████▊  | 78/100 [00:43<00:12,  1.81it/s]Measuring inference with batch_size=8:  79%|███████▉  | 79/100 [00:43<00:11,  1.81it/s]Measuring inference with batch_size=8:  80%|████████  | 80/100 [00:44<00:11,  1.81it/s]Measuring inference with batch_size=8:  81%|████████  | 81/100 [00:44<00:10,  1.81it/s]Measuring inference with batch_size=8:  82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s]Measuring inference with batch_size=8:  83%|████████▎ | 83/100 [00:45<00:09,  1.81it/s]Measuring inference with batch_size=8:  84%|████████▍ | 84/100 [00:46<00:08,  1.81it/s]Measuring inference with batch_size=8:  85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s]Measuring inference with batch_size=8:  86%|████████▌ | 86/100 [00:47<00:07,  1.81it/s]Measuring inference with batch_size=8:  87%|████████▋ | 87/100 [00:48<00:07,  1.81it/s]Measuring inference with batch_size=8:  88%|████████▊ | 88/100 [00:48<00:06,  1.81it/s]Measuring inference with batch_size=8:  89%|████████▉ | 89/100 [00:49<00:06,  1.81it/s]Measuring inference with batch_size=8:  90%|█████████ | 90/100 [00:49<00:05,  1.81it/s]Measuring inference with batch_size=8:  91%|█████████ | 91/100 [00:50<00:04,  1.81it/s]Measuring inference with batch_size=8:  92%|█████████▏| 92/100 [00:50<00:04,  1.82it/s]Measuring inference with batch_size=8:  93%|█████████▎| 93/100 [00:51<00:03,  1.81it/s]Measuring inference with batch_size=8:  94%|█████████▍| 94/100 [00:52<00:03,  1.81it/s]Measuring inference with batch_size=8:  95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s]Measuring inference with batch_size=8:  96%|█████████▌| 96/100 [00:53<00:02,  1.81it/s]Measuring inference with batch_size=8:  97%|█████████▋| 97/100 [00:53<00:01,  1.81it/s]Measuring inference with batch_size=8:  98%|█████████▊| 98/100 [00:54<00:01,  1.81it/s]Measuring inference with batch_size=8:  99%|█████████▉| 99/100 [00:54<00:00,  1.81it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [00:55<00:00,  1.81it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [00:55<00:00,  1.81it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Timing results (batch_size=8):
  cpu_to_gpu:
    human_readable:
      batch_latency: "1.457 \xB5s +/- 1.120 \xB5s [0.954 \xB5s, 12.398 \xB5s]"
      batches_per_second: 754.35 K +/- 134.92 K [80.66 K, 1.05 M]
    metrics:
      batches_per_second_max: 1048576.0
      batches_per_second_mean: 754349.9274627597
      batches_per_second_min: 80659.69230769231
      batches_per_second_std: 134922.6265132385
      seconds_per_batch_max: 1.239776611328125e-05
      seconds_per_batch_mean: 1.456737518310547e-06
      seconds_per_batch_min: 9.5367431640625e-07
      seconds_per_batch_std: 1.1202603792275408e-06
  gpu_to_cpu:
    human_readable:
      batch_latency: "170.794 \xB5s +/- 17.242 \xB5s [154.734 \xB5s, 258.446 \xB5\
        s]"
      batches_per_second: 5.90 K +/- 492.63 [3.87 K, 6.46 K]
    metrics:
      batches_per_second_max: 6462.718027734977
      batches_per_second_mean: 5904.171946063566
      batches_per_second_min: 3869.2841328413283
      batches_per_second_std: 492.62537903656727
      seconds_per_batch_max: 0.00025844573974609375
      seconds_per_batch_mean: 0.00017079353332519532
      seconds_per_batch_min: 0.00015473365783691406
      seconds_per_batch_std: 1.724201624926949e-05
  on_device_inference:
    human_readable:
      batch_latency: 553.093 ms +/- 13.296 ms [546.279 ms, 654.131 ms]
      batches_per_second: 1.81 +/- 0.04 [1.53, 1.83]
    metrics:
      batches_per_second_max: 1.8305673867511267
      batches_per_second_mean: 1.8089365126043495
      batches_per_second_min: 1.528744807054606
      batches_per_second_std: 0.03847312854979841
      seconds_per_batch_max: 0.6541314125061035
      seconds_per_batch_mean: 0.5530933904647827
      seconds_per_batch_min: 0.546278715133667
      seconds_per_batch_std: 0.013296422670681766
  total:
    human_readable:
      batch_latency: 553.266 ms +/- 13.304 ms [546.441 ms, 654.319 ms]
      batches_per_second: 1.81 +/- 0.04 [1.53, 1.83]
    metrics:
      batches_per_second_max: 1.8300242720246918
      batches_per_second_mean: 1.8083738017975577
      batches_per_second_min: 1.528305860558939
      batches_per_second_std: 0.038472184627852575
      seconds_per_batch_max: 0.6543192863464355
      seconds_per_batch_mean: 0.5532656407356262
      seconds_per_batch_min: 0.546440839767456
      seconds_per_batch_std: 0.013303548128076137

learner.infer:
  device: cuda
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 52
        total: 104
      frequency: 2.10 GHz
      model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
    gpus:
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    memory:
      available: 177.80 GB
      total: 187.56 GB
      used: 8.39 GB
    system:
      node: charybdis
      release: 4.15.0-167-generic
      system: Linux
  max_inference_memory: 8215507968
  post_inference_memory: 8203826176
  pre_inference_memory: 5085070336
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "1.204 \xB5s +/- 0.225 \xB5s [0.954 \xB5s, 2.146 \xB5s]"
          batches_per_second: 856.50 K +/- 143.83 K [466.03 K, 1.05 M]
        metrics:
          batches_per_second_max: 1048576.0
          batches_per_second_mean: 856503.5073015873
          batches_per_second_min: 466033.77777777775
          batches_per_second_std: 143829.55693608779
          seconds_per_batch_max: 2.1457672119140625e-06
          seconds_per_batch_mean: 1.2040138244628906e-06
          seconds_per_batch_min: 9.5367431640625e-07
          seconds_per_batch_std: 2.24607511537275e-07
      gpu_to_cpu:
        human_readable:
          batch_latency: "43.476 \xB5s +/- 8.503 \xB5s [35.763 \xB5s, 81.301 \xB5\
            s]"
          batches_per_second: 23.67 K +/- 3.48 K [12.30 K, 27.96 K]
        metrics:
          batches_per_second_max: 27962.02666666667
          batches_per_second_mean: 23666.305115619045
          batches_per_second_min: 12300.011730205279
          batches_per_second_std: 3480.07605751578
          seconds_per_batch_max: 8.130073547363281e-05
          seconds_per_batch_mean: 4.347562789916992e-05
          seconds_per_batch_min: 3.5762786865234375e-05
          seconds_per_batch_std: 8.502648017011778e-06
      on_device_inference:
        human_readable:
          batch_latency: 216.792 ms +/- 3.053 ms [209.245 ms, 223.494 ms]
          batches_per_second: 4.61 +/- 0.06 [4.47, 4.78]
        metrics:
          batches_per_second_max: 4.779087481498615
          batches_per_second_mean: 4.613638088689235
          batches_per_second_min: 4.474396762964836
          batches_per_second_std: 0.06489579633218134
          seconds_per_batch_max: 0.2234938144683838
          seconds_per_batch_mean: 0.21679162979125977
          seconds_per_batch_min: 0.209244966506958
          seconds_per_batch_std: 0.0030534378583087244
      total:
        human_readable:
          batch_latency: 216.836 ms +/- 3.055 ms [209.286 ms, 223.553 ms]
          batches_per_second: 4.61 +/- 0.06 [4.47, 4.78]
        metrics:
          batches_per_second_max: 4.7781564989154806
          batches_per_second_mean: 4.612688004199629
          batches_per_second_min: 4.473203782644076
          batches_per_second_std: 0.06490251802918706
          seconds_per_batch_max: 0.22355341911315918
          seconds_per_batch_mean: 0.2168363094329834
          seconds_per_batch_min: 0.20928573608398438
          seconds_per_batch_std: 0.0030550156783498146
    batch_size_8:
      cpu_to_gpu:
        human_readable:
          batch_latency: "1.457 \xB5s +/- 1.120 \xB5s [0.954 \xB5s, 12.398 \xB5s]"
          batches_per_second: 754.35 K +/- 134.92 K [80.66 K, 1.05 M]
        metrics:
          batches_per_second_max: 1048576.0
          batches_per_second_mean: 754349.9274627597
          batches_per_second_min: 80659.69230769231
          batches_per_second_std: 134922.6265132385
          seconds_per_batch_max: 1.239776611328125e-05
          seconds_per_batch_mean: 1.456737518310547e-06
          seconds_per_batch_min: 9.5367431640625e-07
          seconds_per_batch_std: 1.1202603792275408e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: "170.794 \xB5s +/- 17.242 \xB5s [154.734 \xB5s, 258.446 \xB5\
            s]"
          batches_per_second: 5.90 K +/- 492.63 [3.87 K, 6.46 K]
        metrics:
          batches_per_second_max: 6462.718027734977
          batches_per_second_mean: 5904.171946063566
          batches_per_second_min: 3869.2841328413283
          batches_per_second_std: 492.62537903656727
          seconds_per_batch_max: 0.00025844573974609375
          seconds_per_batch_mean: 0.00017079353332519532
          seconds_per_batch_min: 0.00015473365783691406
          seconds_per_batch_std: 1.724201624926949e-05
      on_device_inference:
        human_readable:
          batch_latency: 553.093 ms +/- 13.296 ms [546.279 ms, 654.131 ms]
          batches_per_second: 1.81 +/- 0.04 [1.53, 1.83]
        metrics:
          batches_per_second_max: 1.8305673867511267
          batches_per_second_mean: 1.8089365126043495
          batches_per_second_min: 1.528744807054606
          batches_per_second_std: 0.03847312854979841
          seconds_per_batch_max: 0.6541314125061035
          seconds_per_batch_mean: 0.5530933904647827
          seconds_per_batch_min: 0.546278715133667
          seconds_per_batch_std: 0.013296422670681766
      total:
        human_readable:
          batch_latency: 553.266 ms +/- 13.304 ms [546.441 ms, 654.319 ms]
          batches_per_second: 1.81 +/- 0.04 [1.53, 1.83]
        metrics:
          batches_per_second_max: 1.8300242720246918
          batches_per_second_mean: 1.8083738017975577
          batches_per_second_min: 1.528305860558939
          batches_per_second_std: 0.038472184627852575
          seconds_per_batch_max: 0.6543192863464355
          seconds_per_batch_mean: 0.5532656407356262
          seconds_per_batch_min: 0.546440839767456
          seconds_per_batch_std: 0.013303548128076137

== Benchmarking model directly ==
Machine info:
  cpu:
    architecture: x86_64
    cores:
      physical: 52
      total: 104
    frequency: 2.10 GHz
    model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
  gpus:
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  - memory: 11019.0 MB
    name: NVIDIA GeForce RTX 2080 Ti
  memory:
    available: 177.78 GB
    total: 187.56 GB
    used: 8.40 GB
  system:
    node: charybdis
    release: 4.15.0-167-generic
    system: Linux

Model device: cuda:0
Model parameters: 6153432 (6.15 M)
Model FLOPs: 1032310310 (1.03 G)
Allocated GPU memory prior to inference: 5484315136 (5.11 GB)
Allocated GPU memory after to inference: 8203826176 (7.64 GB)
Max allocated GPU memory during inference: 8215507456 (7.65 GB)
Warming up with batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=8:  10%|█         | 1/10 [00:00<00:01,  4.55it/s]Warming up with batch_size=8:  20%|██        | 2/10 [00:00<00:01,  4.67it/s]Warming up with batch_size=8:  30%|███       | 3/10 [00:00<00:01,  4.64it/s]Warming up with batch_size=8:  40%|████      | 4/10 [00:00<00:01,  4.59it/s]Warming up with batch_size=8:  50%|█████     | 5/10 [00:01<00:01,  4.58it/s]Warming up with batch_size=8:  60%|██████    | 6/10 [00:01<00:00,  4.56it/s]Warming up with batch_size=8:  70%|███████   | 7/10 [00:01<00:00,  4.57it/s]Warming up with batch_size=8:  80%|████████  | 8/10 [00:01<00:00,  4.61it/s]Warming up with batch_size=8:  90%|█████████ | 9/10 [00:01<00:00,  4.61it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:02<00:00,  4.63it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:02<00:00,  4.61it/s]
Measuring inference with batch_size=8:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=8:   1%|          | 1/100 [00:00<00:21,  4.69it/s]Measuring inference with batch_size=8:   2%|▏         | 2/100 [00:00<00:20,  4.67it/s]Measuring inference with batch_size=8:   3%|▎         | 3/100 [00:00<00:20,  4.67it/s]Measuring inference with batch_size=8:   4%|▍         | 4/100 [00:00<00:20,  4.70it/s]Measuring inference with batch_size=8:   5%|▌         | 5/100 [00:01<00:20,  4.70it/s]Measuring inference with batch_size=8:   6%|▌         | 6/100 [00:01<00:19,  4.70it/s]Measuring inference with batch_size=8:   7%|▋         | 7/100 [00:01<00:19,  4.72it/s]Measuring inference with batch_size=8:   8%|▊         | 8/100 [00:01<00:19,  4.70it/s]Measuring inference with batch_size=8:   9%|▉         | 9/100 [00:01<00:19,  4.68it/s]Measuring inference with batch_size=8:  10%|█         | 10/100 [00:02<00:19,  4.65it/s]Measuring inference with batch_size=8:  11%|█         | 11/100 [00:02<00:19,  4.67it/s]Measuring inference with batch_size=8:  12%|█▏        | 12/100 [00:02<00:18,  4.69it/s]Measuring inference with batch_size=8:  13%|█▎        | 13/100 [00:02<00:18,  4.69it/s]Measuring inference with batch_size=8:  14%|█▍        | 14/100 [00:02<00:18,  4.68it/s]Measuring inference with batch_size=8:  15%|█▌        | 15/100 [00:03<00:18,  4.69it/s]Measuring inference with batch_size=8:  16%|█▌        | 16/100 [00:03<00:17,  4.70it/s]Measuring inference with batch_size=8:  17%|█▋        | 17/100 [00:03<00:17,  4.71it/s]Measuring inference with batch_size=8:  18%|█▊        | 18/100 [00:03<00:17,  4.72it/s]Measuring inference with batch_size=8:  19%|█▉        | 19/100 [00:04<00:17,  4.68it/s]Measuring inference with batch_size=8:  20%|██        | 20/100 [00:04<00:17,  4.68it/s]Measuring inference with batch_size=8:  21%|██        | 21/100 [00:04<00:16,  4.69it/s]Measuring inference with batch_size=8:  22%|██▏       | 22/100 [00:04<00:16,  4.71it/s]Measuring inference with batch_size=8:  23%|██▎       | 23/100 [00:04<00:16,  4.73it/s]Measuring inference with batch_size=8:  24%|██▍       | 24/100 [00:05<00:16,  4.72it/s]Measuring inference with batch_size=8:  25%|██▌       | 25/100 [00:05<00:15,  4.74it/s]Measuring inference with batch_size=8:  26%|██▌       | 26/100 [00:05<00:15,  4.75it/s]Measuring inference with batch_size=8:  27%|██▋       | 27/100 [00:05<00:15,  4.74it/s]Measuring inference with batch_size=8:  28%|██▊       | 28/100 [00:05<00:15,  4.75it/s]Measuring inference with batch_size=8:  29%|██▉       | 29/100 [00:06<00:14,  4.75it/s]Measuring inference with batch_size=8:  30%|███       | 30/100 [00:06<00:14,  4.73it/s]Measuring inference with batch_size=8:  31%|███       | 31/100 [00:06<00:14,  4.73it/s]Measuring inference with batch_size=8:  32%|███▏      | 32/100 [00:06<00:14,  4.72it/s]Measuring inference with batch_size=8:  33%|███▎      | 33/100 [00:07<00:14,  4.72it/s]Measuring inference with batch_size=8:  34%|███▍      | 34/100 [00:07<00:13,  4.73it/s]Measuring inference with batch_size=8:  35%|███▌      | 35/100 [00:07<00:13,  4.70it/s]Measuring inference with batch_size=8:  36%|███▌      | 36/100 [00:07<00:13,  4.70it/s]Measuring inference with batch_size=8:  37%|███▋      | 37/100 [00:07<00:13,  4.69it/s]Measuring inference with batch_size=8:  38%|███▊      | 38/100 [00:08<00:13,  4.67it/s]Measuring inference with batch_size=8:  39%|███▉      | 39/100 [00:08<00:13,  4.66it/s]Measuring inference with batch_size=8:  40%|████      | 40/100 [00:08<00:12,  4.64it/s]Measuring inference with batch_size=8:  41%|████      | 41/100 [00:08<00:12,  4.65it/s]Measuring inference with batch_size=8:  42%|████▏     | 42/100 [00:08<00:12,  4.69it/s]Measuring inference with batch_size=8:  43%|████▎     | 43/100 [00:09<00:12,  4.70it/s]Measuring inference with batch_size=8:  44%|████▍     | 44/100 [00:09<00:11,  4.69it/s]Measuring inference with batch_size=8:  45%|████▌     | 45/100 [00:09<00:11,  4.70it/s]Measuring inference with batch_size=8:  46%|████▌     | 46/100 [00:09<00:11,  4.72it/s]Measuring inference with batch_size=8:  47%|████▋     | 47/100 [00:09<00:11,  4.73it/s]Measuring inference with batch_size=8:  48%|████▊     | 48/100 [00:10<00:10,  4.75it/s]Measuring inference with batch_size=8:  49%|████▉     | 49/100 [00:10<00:10,  4.75it/s]Measuring inference with batch_size=8:  50%|█████     | 50/100 [00:10<00:10,  4.76it/s]Measuring inference with batch_size=8:  51%|█████     | 51/100 [00:10<00:10,  4.73it/s]Measuring inference with batch_size=8:  52%|█████▏    | 52/100 [00:11<00:10,  4.73it/s]Measuring inference with batch_size=8:  53%|█████▎    | 53/100 [00:11<00:09,  4.74it/s]Measuring inference with batch_size=8:  54%|█████▍    | 54/100 [00:11<00:09,  4.74it/s]Measuring inference with batch_size=8:  55%|█████▌    | 55/100 [00:11<00:09,  4.74it/s]Measuring inference with batch_size=8:  56%|█████▌    | 56/100 [00:11<00:09,  4.74it/s]Measuring inference with batch_size=8:  57%|█████▋    | 57/100 [00:12<00:09,  4.72it/s]Measuring inference with batch_size=8:  58%|█████▊    | 58/100 [00:12<00:08,  4.71it/s]Measuring inference with batch_size=8:  59%|█████▉    | 59/100 [00:12<00:08,  4.70it/s]Measuring inference with batch_size=8:  60%|██████    | 60/100 [00:12<00:08,  4.69it/s]Measuring inference with batch_size=8:  61%|██████    | 61/100 [00:12<00:08,  4.70it/s]Measuring inference with batch_size=8:  62%|██████▏   | 62/100 [00:13<00:08,  4.69it/s]Measuring inference with batch_size=8:  63%|██████▎   | 63/100 [00:13<00:07,  4.70it/s]Measuring inference with batch_size=8:  64%|██████▍   | 64/100 [00:13<00:07,  4.72it/s]Measuring inference with batch_size=8:  65%|██████▌   | 65/100 [00:13<00:07,  4.71it/s]Measuring inference with batch_size=8:  66%|██████▌   | 66/100 [00:14<00:07,  4.71it/s]Measuring inference with batch_size=8:  67%|██████▋   | 67/100 [00:14<00:06,  4.72it/s]Measuring inference with batch_size=8:  68%|██████▊   | 68/100 [00:14<00:06,  4.70it/s]Measuring inference with batch_size=8:  69%|██████▉   | 69/100 [00:14<00:06,  4.67it/s]Measuring inference with batch_size=8:  70%|███████   | 70/100 [00:14<00:06,  4.69it/s]Measuring inference with batch_size=8:  71%|███████   | 71/100 [00:15<00:06,  4.71it/s]Measuring inference with batch_size=8:  72%|███████▏  | 72/100 [00:15<00:05,  4.72it/s]Measuring inference with batch_size=8:  73%|███████▎  | 73/100 [00:15<00:05,  4.69it/s]Measuring inference with batch_size=8:  74%|███████▍  | 74/100 [00:15<00:05,  4.67it/s]Measuring inference with batch_size=8:  75%|███████▌  | 75/100 [00:15<00:05,  4.67it/s]Measuring inference with batch_size=8:  76%|███████▌  | 76/100 [00:16<00:05,  4.66it/s]Measuring inference with batch_size=8:  77%|███████▋  | 77/100 [00:16<00:04,  4.66it/s]Measuring inference with batch_size=8:  78%|███████▊  | 78/100 [00:16<00:04,  4.64it/s]Measuring inference with batch_size=8:  79%|███████▉  | 79/100 [00:16<00:04,  4.64it/s]Measuring inference with batch_size=8:  80%|████████  | 80/100 [00:17<00:04,  4.64it/s]Measuring inference with batch_size=8:  81%|████████  | 81/100 [00:17<00:04,  4.64it/s]Measuring inference with batch_size=8:  82%|████████▏ | 82/100 [00:17<00:03,  4.65it/s]Measuring inference with batch_size=8:  83%|████████▎ | 83/100 [00:17<00:03,  4.65it/s]Measuring inference with batch_size=8:  84%|████████▍ | 84/100 [00:17<00:03,  4.63it/s]Measuring inference with batch_size=8:  85%|████████▌ | 85/100 [00:18<00:03,  4.63it/s]Measuring inference with batch_size=8:  86%|████████▌ | 86/100 [00:18<00:03,  4.64it/s]Measuring inference with batch_size=8:  87%|████████▋ | 87/100 [00:18<00:02,  4.62it/s]Measuring inference with batch_size=8:  88%|████████▊ | 88/100 [00:18<00:02,  4.61it/s]Measuring inference with batch_size=8:  89%|████████▉ | 89/100 [00:18<00:02,  4.60it/s]Measuring inference with batch_size=8:  90%|█████████ | 90/100 [00:19<00:02,  4.60it/s]Measuring inference with batch_size=8:  91%|█████████ | 91/100 [00:19<00:01,  4.61it/s]Measuring inference with batch_size=8:  92%|█████████▏| 92/100 [00:19<00:01,  4.62it/s]Measuring inference with batch_size=8:  93%|█████████▎| 93/100 [00:19<00:01,  4.62it/s]Measuring inference with batch_size=8:  94%|█████████▍| 94/100 [00:20<00:01,  4.59it/s]Measuring inference with batch_size=8:  95%|█████████▌| 95/100 [00:20<00:01,  4.60it/s]Measuring inference with batch_size=8:  96%|█████████▌| 96/100 [00:20<00:00,  4.62it/s]Measuring inference with batch_size=8:  97%|█████████▋| 97/100 [00:20<00:00,  4.63it/s]Measuring inference with batch_size=8:  98%|█████████▊| 98/100 [00:20<00:00,  4.64it/s]Measuring inference with batch_size=8:  99%|█████████▉| 99/100 [00:21<00:00,  4.67it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [00:21<00:00,  4.68it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [00:21<00:00,  4.69it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "223.773 \xB5s +/- 53.958 \xB5s [210.762 \xB5s, 538.588 \xB5\
        s]"
      batches_per_second: 4.58 K +/- 478.28 [1.86 K, 4.74 K]
    metrics:
      batches_per_second_max: 4744.68778280543
      batches_per_second_mean: 4583.7980730528225
      batches_per_second_min: 1856.7082779991147
      batches_per_second_std: 478.2809899050142
      seconds_per_batch_max: 0.0005385875701904297
      seconds_per_batch_mean: 0.00022377252578735353
      seconds_per_batch_min: 0.00021076202392578125
      seconds_per_batch_std: 5.395804093685597e-05
  gpu_to_cpu:
    human_readable:
      batch_latency: "36.206 \xB5s +/- 7.280 \xB5s [30.518 \xB5s, 68.426 \xB5s]"
      batches_per_second: 28.40 K +/- 3.96 K [14.61 K, 32.77 K]
    metrics:
      batches_per_second_max: 32768.0
      batches_per_second_mean: 28395.497335002256
      batches_per_second_min: 14614.299651567944
      batches_per_second_std: 3963.2110097554964
      seconds_per_batch_max: 6.842613220214844e-05
      seconds_per_batch_mean: 3.6206245422363285e-05
      seconds_per_batch_min: 3.0517578125e-05
      seconds_per_batch_std: 7.280478964786163e-06
  on_device_inference:
    human_readable:
      batch_latency: 213.027 ms +/- 2.645 ms [208.756 ms, 220.147 ms]
      batches_per_second: 4.69 +/- 0.06 [4.54, 4.79]
    metrics:
      batches_per_second_max: 4.790271223452126
      batches_per_second_mean: 4.6949543490397385
      batches_per_second_min: 4.542411722341653
      batches_per_second_std: 0.05806752748986019
      seconds_per_batch_max: 0.22014737129211426
      seconds_per_batch_mean: 0.21302732944488526
      seconds_per_batch_min: 0.2087564468383789
      seconds_per_batch_std: 0.0026452183479881257
  total:
    human_readable:
      batch_latency: 213.287 ms +/- 2.651 ms [208.999 ms, 220.435 ms]
      batches_per_second: 4.69 +/- 0.06 [4.54, 4.78]
    metrics:
      batches_per_second_max: 4.784708295402952
      batches_per_second_mean: 4.68923313723658
      batches_per_second_min: 4.536481745066907
      batches_per_second_std: 0.058061420701196256
      seconds_per_batch_max: 0.22043514251708984
      seconds_per_batch_mean: 0.21328730821609498
      seconds_per_batch_min: 0.2089991569519043
      seconds_per_batch_std: 0.0026511221286147535

Warming up with batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=8:  10%|█         | 1/10 [00:00<00:05,  1.79it/s]Warming up with batch_size=8:  20%|██        | 2/10 [00:01<00:04,  1.82it/s]Warming up with batch_size=8:  30%|███       | 3/10 [00:01<00:03,  1.83it/s]Warming up with batch_size=8:  40%|████      | 4/10 [00:02<00:03,  1.83it/s]Warming up with batch_size=8:  50%|█████     | 5/10 [00:02<00:02,  1.84it/s]Warming up with batch_size=8:  60%|██████    | 6/10 [00:03<00:02,  1.84it/s]Warming up with batch_size=8:  70%|███████   | 7/10 [00:03<00:01,  1.83it/s]Warming up with batch_size=8:  80%|████████  | 8/10 [00:04<00:01,  1.83it/s]Warming up with batch_size=8:  90%|█████████ | 9/10 [00:04<00:00,  1.83it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:05<00:00,  1.83it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:05<00:00,  1.83it/s]
Measuring inference with batch_size=8:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=8:   1%|          | 1/100 [00:00<00:53,  1.84it/s]Measuring inference with batch_size=8:   2%|▏         | 2/100 [00:01<00:53,  1.83it/s]Measuring inference with batch_size=8:   3%|▎         | 3/100 [00:01<00:53,  1.83it/s]Measuring inference with batch_size=8:   4%|▍         | 4/100 [00:02<00:52,  1.83it/s]Measuring inference with batch_size=8:   5%|▌         | 5/100 [00:02<00:51,  1.84it/s]Measuring inference with batch_size=8:   6%|▌         | 6/100 [00:03<00:51,  1.84it/s]Measuring inference with batch_size=8:   7%|▋         | 7/100 [00:03<00:50,  1.83it/s]Measuring inference with batch_size=8:   8%|▊         | 8/100 [00:04<00:50,  1.83it/s]Measuring inference with batch_size=8:   9%|▉         | 9/100 [00:04<00:49,  1.83it/s]Measuring inference with batch_size=8:  10%|█         | 10/100 [00:05<00:49,  1.83it/s]Measuring inference with batch_size=8:  11%|█         | 11/100 [00:06<00:48,  1.83it/s]Measuring inference with batch_size=8:  12%|█▏        | 12/100 [00:06<00:47,  1.83it/s]Measuring inference with batch_size=8:  13%|█▎        | 13/100 [00:07<00:47,  1.83it/s]Measuring inference with batch_size=8:  14%|█▍        | 14/100 [00:07<00:47,  1.83it/s]Measuring inference with batch_size=8:  15%|█▌        | 15/100 [00:08<00:46,  1.82it/s]Measuring inference with batch_size=8:  16%|█▌        | 16/100 [00:08<00:45,  1.83it/s]Measuring inference with batch_size=8:  17%|█▋        | 17/100 [00:09<00:45,  1.83it/s]Measuring inference with batch_size=8:  18%|█▊        | 18/100 [00:09<00:44,  1.83it/s]Measuring inference with batch_size=8:  19%|█▉        | 19/100 [00:10<00:44,  1.84it/s]Measuring inference with batch_size=8:  20%|██        | 20/100 [00:10<00:43,  1.84it/s]Measuring inference with batch_size=8:  21%|██        | 21/100 [00:11<00:43,  1.84it/s]Measuring inference with batch_size=8:  22%|██▏       | 22/100 [00:12<00:42,  1.84it/s]Measuring inference with batch_size=8:  23%|██▎       | 23/100 [00:12<00:41,  1.84it/s]Measuring inference with batch_size=8:  24%|██▍       | 24/100 [00:13<00:41,  1.84it/s]Measuring inference with batch_size=8:  25%|██▌       | 25/100 [00:13<00:40,  1.84it/s]Measuring inference with batch_size=8:  26%|██▌       | 26/100 [00:14<00:40,  1.83it/s]Measuring inference with batch_size=8:  27%|██▋       | 27/100 [00:14<00:39,  1.83it/s]Measuring inference with batch_size=8:  28%|██▊       | 28/100 [00:15<00:39,  1.84it/s]Measuring inference with batch_size=8:  29%|██▉       | 29/100 [00:15<00:38,  1.84it/s]Measuring inference with batch_size=8:  30%|███       | 30/100 [00:16<00:38,  1.84it/s]Measuring inference with batch_size=8:  31%|███       | 31/100 [00:16<00:37,  1.84it/s]Measuring inference with batch_size=8:  32%|███▏      | 32/100 [00:17<00:37,  1.84it/s]Measuring inference with batch_size=8:  33%|███▎      | 33/100 [00:17<00:36,  1.83it/s]Measuring inference with batch_size=8:  34%|███▍      | 34/100 [00:18<00:36,  1.83it/s]Measuring inference with batch_size=8:  35%|███▌      | 35/100 [00:19<00:35,  1.83it/s]Measuring inference with batch_size=8:  36%|███▌      | 36/100 [00:19<00:34,  1.83it/s]Measuring inference with batch_size=8:  37%|███▋      | 37/100 [00:20<00:34,  1.83it/s]Measuring inference with batch_size=8:  38%|███▊      | 38/100 [00:20<00:33,  1.83it/s]Measuring inference with batch_size=8:  39%|███▉      | 39/100 [00:21<00:33,  1.84it/s]Measuring inference with batch_size=8:  40%|████      | 40/100 [00:21<00:32,  1.84it/s]Measuring inference with batch_size=8:  41%|████      | 41/100 [00:22<00:32,  1.84it/s]Measuring inference with batch_size=8:  42%|████▏     | 42/100 [00:22<00:31,  1.84it/s]Measuring inference with batch_size=8:  43%|████▎     | 43/100 [00:23<00:31,  1.84it/s]Measuring inference with batch_size=8:  44%|████▍     | 44/100 [00:23<00:30,  1.84it/s]Measuring inference with batch_size=8:  45%|████▌     | 45/100 [00:24<00:30,  1.83it/s]Measuring inference with batch_size=8:  46%|████▌     | 46/100 [00:25<00:29,  1.80it/s]Measuring inference with batch_size=8:  47%|████▋     | 47/100 [00:25<00:29,  1.81it/s]Measuring inference with batch_size=8:  48%|████▊     | 48/100 [00:26<00:28,  1.82it/s]Measuring inference with batch_size=8:  49%|████▉     | 49/100 [00:26<00:27,  1.82it/s]Measuring inference with batch_size=8:  50%|█████     | 50/100 [00:27<00:27,  1.83it/s]Measuring inference with batch_size=8:  51%|█████     | 51/100 [00:27<00:26,  1.83it/s]Measuring inference with batch_size=8:  52%|█████▏    | 52/100 [00:28<00:26,  1.83it/s]Measuring inference with batch_size=8:  53%|█████▎    | 53/100 [00:28<00:25,  1.83it/s]Measuring inference with batch_size=8:  54%|█████▍    | 54/100 [00:29<00:25,  1.83it/s]Measuring inference with batch_size=8:  55%|█████▌    | 55/100 [00:30<00:24,  1.84it/s]Measuring inference with batch_size=8:  56%|█████▌    | 56/100 [00:30<00:23,  1.84it/s]Measuring inference with batch_size=8:  57%|█████▋    | 57/100 [00:31<00:23,  1.84it/s]Measuring inference with batch_size=8:  58%|█████▊    | 58/100 [00:31<00:22,  1.84it/s]Measuring inference with batch_size=8:  59%|█████▉    | 59/100 [00:32<00:22,  1.84it/s]Measuring inference with batch_size=8:  60%|██████    | 60/100 [00:32<00:21,  1.84it/s]Measuring inference with batch_size=8:  61%|██████    | 61/100 [00:33<00:21,  1.84it/s]Measuring inference with batch_size=8:  62%|██████▏   | 62/100 [00:33<00:20,  1.84it/s]Measuring inference with batch_size=8:  63%|██████▎   | 63/100 [00:34<00:20,  1.84it/s]Measuring inference with batch_size=8:  64%|██████▍   | 64/100 [00:34<00:19,  1.83it/s]Measuring inference with batch_size=8:  65%|██████▌   | 65/100 [00:35<00:19,  1.83it/s]Measuring inference with batch_size=8:  66%|██████▌   | 66/100 [00:36<00:18,  1.83it/s]Measuring inference with batch_size=8:  67%|██████▋   | 67/100 [00:36<00:18,  1.82it/s]Measuring inference with batch_size=8:  68%|██████▊   | 68/100 [00:37<00:17,  1.82it/s]Measuring inference with batch_size=8:  69%|██████▉   | 69/100 [00:37<00:17,  1.82it/s]Measuring inference with batch_size=8:  70%|███████   | 70/100 [00:38<00:16,  1.82it/s]Measuring inference with batch_size=8:  71%|███████   | 71/100 [00:38<00:15,  1.82it/s]Measuring inference with batch_size=8:  72%|███████▏  | 72/100 [00:39<00:15,  1.83it/s]Measuring inference with batch_size=8:  73%|███████▎  | 73/100 [00:39<00:14,  1.83it/s]Measuring inference with batch_size=8:  74%|███████▍  | 74/100 [00:40<00:14,  1.83it/s]Measuring inference with batch_size=8:  75%|███████▌  | 75/100 [00:40<00:13,  1.82it/s]Measuring inference with batch_size=8:  76%|███████▌  | 76/100 [00:41<00:13,  1.83it/s]Measuring inference with batch_size=8:  77%|███████▋  | 77/100 [00:42<00:12,  1.83it/s]Measuring inference with batch_size=8:  78%|███████▊  | 78/100 [00:42<00:12,  1.83it/s]Measuring inference with batch_size=8:  79%|███████▉  | 79/100 [00:43<00:11,  1.83it/s]Measuring inference with batch_size=8:  80%|████████  | 80/100 [00:43<00:10,  1.83it/s]Measuring inference with batch_size=8:  81%|████████  | 81/100 [00:44<00:10,  1.83it/s]Measuring inference with batch_size=8:  82%|████████▏ | 82/100 [00:44<00:09,  1.83it/s]Measuring inference with batch_size=8:  83%|████████▎ | 83/100 [00:45<00:09,  1.84it/s]Measuring inference with batch_size=8:  84%|████████▍ | 84/100 [00:45<00:08,  1.84it/s]Measuring inference with batch_size=8:  85%|████████▌ | 85/100 [00:46<00:08,  1.83it/s]Measuring inference with batch_size=8:  86%|████████▌ | 86/100 [00:46<00:07,  1.83it/s]Measuring inference with batch_size=8:  87%|████████▋ | 87/100 [00:47<00:07,  1.84it/s]Measuring inference with batch_size=8:  88%|████████▊ | 88/100 [00:48<00:06,  1.84it/s]Measuring inference with batch_size=8:  89%|████████▉ | 89/100 [00:48<00:05,  1.84it/s]Measuring inference with batch_size=8:  90%|█████████ | 90/100 [00:49<00:05,  1.83it/s]Measuring inference with batch_size=8:  91%|█████████ | 91/100 [00:49<00:04,  1.83it/s]Measuring inference with batch_size=8:  92%|█████████▏| 92/100 [00:50<00:04,  1.84it/s]Measuring inference with batch_size=8:  93%|█████████▎| 93/100 [00:50<00:03,  1.84it/s]Measuring inference with batch_size=8:  94%|█████████▍| 94/100 [00:51<00:03,  1.84it/s]Measuring inference with batch_size=8:  95%|█████████▌| 95/100 [00:51<00:02,  1.84it/s]Measuring inference with batch_size=8:  96%|█████████▌| 96/100 [00:52<00:02,  1.84it/s]Measuring inference with batch_size=8:  97%|█████████▋| 97/100 [00:52<00:01,  1.84it/s]Measuring inference with batch_size=8:  98%|█████████▊| 98/100 [00:53<00:01,  1.83it/s]Measuring inference with batch_size=8:  99%|█████████▉| 99/100 [00:54<00:00,  1.84it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [00:54<00:00,  1.84it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [00:54<00:00,  1.83it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
Timing results (batch_size=8):
  cpu_to_gpu:
    human_readable:
      batch_latency: "976.732 \xB5s +/- 253.025 \xB5s [911.951 \xB5s, 3.483 ms]"
      batches_per_second: 1.04 K +/- 80.46 [287.12, 1.10 K]
    metrics:
      batches_per_second_max: 1096.5500653594772
      batches_per_second_mean: 1044.0877151582642
      batches_per_second_min: 287.1237677984666
      batches_per_second_std: 80.45991264556986
      seconds_per_batch_max: 0.003482818603515625
      seconds_per_batch_mean: 0.0009767317771911621
      seconds_per_batch_min: 0.0009119510650634766
      seconds_per_batch_std: 0.0002530249300461795
  gpu_to_cpu:
    human_readable:
      batch_latency: "630.076 \xB5s +/- 844.989 \xB5s [40.531 \xB5s, 5.212 ms]"
      batches_per_second: 9.58 K +/- 8.90 K [191.86, 24.67 K]
    metrics:
      batches_per_second_max: 24672.376470588235
      batches_per_second_mean: 9583.237543260704
      batches_per_second_min: 191.86240336672614
      batches_per_second_std: 8896.511117707763
      seconds_per_batch_max: 0.005212068557739258
      seconds_per_batch_mean: 0.0006300759315490723
      seconds_per_batch_min: 4.0531158447265625e-05
      seconds_per_batch_std: 0.0008449885988397704
  on_device_inference:
    human_readable:
      batch_latency: 544.076 ms +/- 4.099 ms [536.807 ms, 573.325 ms]
      batches_per_second: 1.84 +/- 0.01 [1.74, 1.86]
    metrics:
      batches_per_second_max: 1.862867533247377
      batches_per_second_mean: 1.8380803590556047
      batches_per_second_min: 1.7442101949473137
      batches_per_second_std: 0.01348041332183909
      seconds_per_batch_max: 0.5733253955841064
      seconds_per_batch_mean: 0.5440759062767029
      seconds_per_batch_min: 0.5368068218231201
      seconds_per_batch_std: 0.004098881255957599
  total:
    human_readable:
      batch_latency: 545.683 ms +/- 3.755 ms [541.555 ms, 574.393 ms]
      batches_per_second: 1.83 +/- 0.01 [1.74, 1.85]
    metrics:
      batches_per_second_max: 1.8465331365719135
      batches_per_second_mean: 1.8326508789890275
      batches_per_second_min: 1.7409681887481172
      batches_per_second_std: 0.01222962855701039
      seconds_per_batch_max: 0.5743930339813232
      seconds_per_batch_mean: 0.5456827139854431
      seconds_per_batch_min: 0.5415554046630859
      seconds_per_batch_std: 0.003754664165893723

learner.model.forward:
  device: cuda
  flops: 1032310310
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 52
        total: 104
      frequency: 2.10 GHz
      model: Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz
    gpus:
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    - memory: 11019.0 MB
      name: NVIDIA GeForce RTX 2080 Ti
    memory:
      available: 177.78 GB
      total: 187.56 GB
      used: 8.40 GB
    system:
      node: charybdis
      release: 4.15.0-167-generic
      system: Linux
  max_inference_memory: 8215507456
  params: 6153432
  post_inference_memory: 8203826176
  pre_inference_memory: 5484315136
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "223.773 \xB5s +/- 53.958 \xB5s [210.762 \xB5s, 538.588 \xB5\
            s]"
          batches_per_second: 4.58 K +/- 478.28 [1.86 K, 4.74 K]
        metrics:
          batches_per_second_max: 4744.68778280543
          batches_per_second_mean: 4583.7980730528225
          batches_per_second_min: 1856.7082779991147
          batches_per_second_std: 478.2809899050142
          seconds_per_batch_max: 0.0005385875701904297
          seconds_per_batch_mean: 0.00022377252578735353
          seconds_per_batch_min: 0.00021076202392578125
          seconds_per_batch_std: 5.395804093685597e-05
      gpu_to_cpu:
        human_readable:
          batch_latency: "36.206 \xB5s +/- 7.280 \xB5s [30.518 \xB5s, 68.426 \xB5\
            s]"
          batches_per_second: 28.40 K +/- 3.96 K [14.61 K, 32.77 K]
        metrics:
          batches_per_second_max: 32768.0
          batches_per_second_mean: 28395.497335002256
          batches_per_second_min: 14614.299651567944
          batches_per_second_std: 3963.2110097554964
          seconds_per_batch_max: 6.842613220214844e-05
          seconds_per_batch_mean: 3.6206245422363285e-05
          seconds_per_batch_min: 3.0517578125e-05
          seconds_per_batch_std: 7.280478964786163e-06
      on_device_inference:
        human_readable:
          batch_latency: 213.027 ms +/- 2.645 ms [208.756 ms, 220.147 ms]
          batches_per_second: 4.69 +/- 0.06 [4.54, 4.79]
        metrics:
          batches_per_second_max: 4.790271223452126
          batches_per_second_mean: 4.6949543490397385
          batches_per_second_min: 4.542411722341653
          batches_per_second_std: 0.05806752748986019
          seconds_per_batch_max: 0.22014737129211426
          seconds_per_batch_mean: 0.21302732944488526
          seconds_per_batch_min: 0.2087564468383789
          seconds_per_batch_std: 0.0026452183479881257
      total:
        human_readable:
          batch_latency: 213.287 ms +/- 2.651 ms [208.999 ms, 220.435 ms]
          batches_per_second: 4.69 +/- 0.06 [4.54, 4.78]
        metrics:
          batches_per_second_max: 4.784708295402952
          batches_per_second_mean: 4.68923313723658
          batches_per_second_min: 4.536481745066907
          batches_per_second_std: 0.058061420701196256
          seconds_per_batch_max: 0.22043514251708984
          seconds_per_batch_mean: 0.21328730821609498
          seconds_per_batch_min: 0.2089991569519043
          seconds_per_batch_std: 0.0026511221286147535
    batch_size_8:
      cpu_to_gpu:
        human_readable:
          batch_latency: "976.732 \xB5s +/- 253.025 \xB5s [911.951 \xB5s, 3.483 ms]"
          batches_per_second: 1.04 K +/- 80.46 [287.12, 1.10 K]
        metrics:
          batches_per_second_max: 1096.5500653594772
          batches_per_second_mean: 1044.0877151582642
          batches_per_second_min: 287.1237677984666
          batches_per_second_std: 80.45991264556986
          seconds_per_batch_max: 0.003482818603515625
          seconds_per_batch_mean: 0.0009767317771911621
          seconds_per_batch_min: 0.0009119510650634766
          seconds_per_batch_std: 0.0002530249300461795
      gpu_to_cpu:
        human_readable:
          batch_latency: "630.076 \xB5s +/- 844.989 \xB5s [40.531 \xB5s, 5.212 ms]"
          batches_per_second: 9.58 K +/- 8.90 K [191.86, 24.67 K]
        metrics:
          batches_per_second_max: 24672.376470588235
          batches_per_second_mean: 9583.237543260704
          batches_per_second_min: 191.86240336672614
          batches_per_second_std: 8896.511117707763
          seconds_per_batch_max: 0.005212068557739258
          seconds_per_batch_mean: 0.0006300759315490723
          seconds_per_batch_min: 4.0531158447265625e-05
          seconds_per_batch_std: 0.0008449885988397704
      on_device_inference:
        human_readable:
          batch_latency: 544.076 ms +/- 4.099 ms [536.807 ms, 573.325 ms]
          batches_per_second: 1.84 +/- 0.01 [1.74, 1.86]
        metrics:
          batches_per_second_max: 1.862867533247377
          batches_per_second_mean: 1.8380803590556047
          batches_per_second_min: 1.7442101949473137
          batches_per_second_std: 0.01348041332183909
          seconds_per_batch_max: 0.5733253955841064
          seconds_per_batch_mean: 0.5440759062767029
          seconds_per_batch_min: 0.5368068218231201
          seconds_per_batch_std: 0.004098881255957599
      total:
        human_readable:
          batch_latency: 545.683 ms +/- 3.755 ms [541.555 ms, 574.393 ms]
          batches_per_second: 1.83 +/- 0.01 [1.74, 1.85]
        metrics:
          batches_per_second_max: 1.8465331365719135
          batches_per_second_mean: 1.8326508789890275
          batches_per_second_min: 1.7409681887481172
          batches_per_second_std: 0.01222962855701039
          seconds_per_batch_max: 0.5743930339813232
          seconds_per_batch_mean: 0.5456827139854431
          seconds_per_batch_min: 0.5415554046630859
          seconds_per_batch_std: 0.003754664165893723

