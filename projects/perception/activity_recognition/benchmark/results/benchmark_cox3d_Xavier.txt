/home/maleci/.local/lib/python3.6/site-packages/pkg_resources/__init__.py:119: PkgResourcesDeprecationWarning: 0.18ubuntu0.18.04.1 is an invalid version and will not be supported in a future release
  PkgResourcesDeprecationWarning,
/home/maleci/.local/lib/python3.6/site-packages/torchvision-0.10.0-py3.6-linux-aarch64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The _functional_video module is deprecated. Please use the functional module instead.
  "The _functional_video module is deprecated. Please use the functional module instead."
/home/maleci/.local/lib/python3.6/site-packages/torchvision-0.10.0-py3.6-linux-aarch64.egg/torchvision/transforms/_transforms_video.py:26: UserWarning: The _transforms_video module is deprecated. Please use the transforms module instead.
  "The _transforms_video module is deprecated. Please use the transforms module instead."
WARNING:opendr.perception.activity_recognition.cox3d.algorithm.utils:Padding along the temporal dimension only affects the computation in `forward3d`. In `forward` it is omitted.
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
==== Benchmarking CoX3DLearner (xs) ====
== Benchmarking learner.infer ==
Machine info:
  cpu:
    architecture: aarch64
    cores:
      physical: 8
      total: 8
    frequency: 2.27 GHz
    model: ARMv8 Processor rev 0 (v8l)
  gpus: null
  memory:
    available: 25.27 GB
    total: 31.17 GB
    used: 8.88 GB
  system:
    node: xavier
    release: 4.9.140-tegra
    system: Linux

Model device: cuda:0
Allocated GPU memory prior to inference: 15445504 (14.73 MB)
Allocated GPU memory after to inference: 7504768512 (6.99 GB)
Max allocated GPU memory during inference: 7553511424 (7.03 GB)
Warming up with batch_size=128:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=128:  10%|█         | 1/10 [00:00<00:01,  8.21it/s]Warming up with batch_size=128:  20%|██        | 2/10 [00:00<00:00,  8.74it/s]Warming up with batch_size=128:  30%|███       | 3/10 [00:00<00:00,  8.84it/s]Warming up with batch_size=128:  40%|████      | 4/10 [00:00<00:00,  8.69it/s]Warming up with batch_size=128:  50%|█████     | 5/10 [00:00<00:00,  8.91it/s]Warming up with batch_size=128:  70%|███████   | 7/10 [00:00<00:00,  9.54it/s]Warming up with batch_size=128:  90%|█████████ | 9/10 [00:00<00:00, 10.01it/s]Warming up with batch_size=128: 100%|██████████| 10/10 [00:01<00:00,  9.77it/s]Warming up with batch_size=128: 100%|██████████| 10/10 [00:01<00:00,  9.42it/s]
Measuring inference with batch_size=128:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=128:   2%|▏         | 2/100 [00:00<00:08, 10.92it/s]Measuring inference with batch_size=128:   4%|▍         | 4/100 [00:00<00:08, 11.05it/s]Measuring inference with batch_size=128:   6%|▌         | 6/100 [00:00<00:08, 11.06it/s]Measuring inference with batch_size=128:   8%|▊         | 8/100 [00:00<00:08, 11.08it/s]Measuring inference with batch_size=128:  10%|█         | 10/100 [00:00<00:08, 10.35it/s]Measuring inference with batch_size=128:  12%|█▏        | 12/100 [00:01<00:08, 10.35it/s]Measuring inference with batch_size=128:  14%|█▍        | 14/100 [00:01<00:08, 10.26it/s]Measuring inference with batch_size=128:  16%|█▌        | 16/100 [00:01<00:08, 10.30it/s]Measuring inference with batch_size=128:  18%|█▊        | 18/100 [00:01<00:08, 10.24it/s]Measuring inference with batch_size=128:  20%|██        | 20/100 [00:01<00:07, 10.27it/s]Measuring inference with batch_size=128:  22%|██▏       | 22/100 [00:02<00:07, 10.24it/s]Measuring inference with batch_size=128:  24%|██▍       | 24/100 [00:02<00:07, 10.30it/s]Measuring inference with batch_size=128:  26%|██▌       | 26/100 [00:02<00:07, 10.16it/s]Measuring inference with batch_size=128:  28%|██▊       | 28/100 [00:02<00:07, 10.24it/s]Measuring inference with batch_size=128:  30%|███       | 30/100 [00:02<00:06, 10.20it/s]Measuring inference with batch_size=128:  32%|███▏      | 32/100 [00:03<00:06, 10.27it/s]Measuring inference with batch_size=128:  34%|███▍      | 34/100 [00:03<00:06, 10.46it/s]Measuring inference with batch_size=128:  36%|███▌      | 36/100 [00:03<00:06, 10.30it/s]Measuring inference with batch_size=128:  38%|███▊      | 38/100 [00:03<00:05, 10.36it/s]Measuring inference with batch_size=128:  40%|████      | 40/100 [00:03<00:05, 10.42it/s]Measuring inference with batch_size=128:  42%|████▏     | 42/100 [00:04<00:05, 10.28it/s]Measuring inference with batch_size=128:  44%|████▍     | 44/100 [00:04<00:05, 10.17it/s]Measuring inference with batch_size=128:  46%|████▌     | 46/100 [00:04<00:05, 10.09it/s]Measuring inference with batch_size=128:  48%|████▊     | 48/100 [00:04<00:05, 10.14it/s]Measuring inference with batch_size=128:  50%|█████     | 50/100 [00:04<00:04, 10.26it/s]Measuring inference with batch_size=128:  52%|█████▏    | 52/100 [00:05<00:04, 10.29it/s]Measuring inference with batch_size=128:  54%|█████▍    | 54/100 [00:05<00:04, 10.27it/s]Measuring inference with batch_size=128:  56%|█████▌    | 56/100 [00:05<00:04, 10.20it/s]Measuring inference with batch_size=128:  58%|█████▊    | 58/100 [00:05<00:04, 10.31it/s]Measuring inference with batch_size=128:  60%|██████    | 60/100 [00:05<00:03, 10.38it/s]Measuring inference with batch_size=128:  62%|██████▏   | 62/100 [00:05<00:03, 10.41it/s]Measuring inference with batch_size=128:  64%|██████▍   | 64/100 [00:06<00:03, 10.57it/s]Measuring inference with batch_size=128:  66%|██████▌   | 66/100 [00:06<00:03, 10.58it/s]Measuring inference with batch_size=128:  68%|██████▊   | 68/100 [00:06<00:03, 10.61it/s]Measuring inference with batch_size=128:  70%|███████   | 70/100 [00:06<00:02, 10.57it/s]Measuring inference with batch_size=128:  72%|███████▏  | 72/100 [00:06<00:02, 10.42it/s]Measuring inference with batch_size=128:  74%|███████▍  | 74/100 [00:07<00:02, 10.45it/s]Measuring inference with batch_size=128:  76%|███████▌  | 76/100 [00:07<00:02, 10.46it/s]Measuring inference with batch_size=128:  78%|███████▊  | 78/100 [00:07<00:02, 10.47it/s]Measuring inference with batch_size=128:  80%|████████  | 80/100 [00:07<00:01, 10.44it/s]Measuring inference with batch_size=128:  82%|████████▏ | 82/100 [00:07<00:01, 10.45it/s]Measuring inference with batch_size=128:  84%|████████▍ | 84/100 [00:08<00:01, 10.61it/s]Measuring inference with batch_size=128:  86%|████████▌ | 86/100 [00:08<00:01, 10.52it/s]Measuring inference with batch_size=128:  88%|████████▊ | 88/100 [00:08<00:01, 10.43it/s]Measuring inference with batch_size=128:  90%|█████████ | 90/100 [00:08<00:00, 10.34it/s]Measuring inference with batch_size=128:  92%|█████████▏| 92/100 [00:08<00:00, 10.20it/s]Measuring inference with batch_size=128:  94%|█████████▍| 94/100 [00:09<00:00, 10.15it/s]Measuring inference with batch_size=128:  96%|█████████▌| 96/100 [00:09<00:00, 10.12it/s]Measuring inference with batch_size=128:  98%|█████████▊| 98/100 [00:09<00:00, 10.16it/s]Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:09<00:00, 10.17it/s]Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:09<00:00, 10.35it/s]
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "7.076 \xB5s +/- 3.321 \xB5s [3.576 \xB5s, 13.828 \xB5s]"
      batches_per_second: 171.96 K +/- 68.09 K [72.32 K, 279.62 K]
    metrics:
      batches_per_second_max: 279620.26666666666
      batches_per_second_mean: 171962.63935087438
      batches_per_second_min: 72315.58620689655
      batches_per_second_std: 68093.06180326438
      seconds_per_batch_max: 1.3828277587890625e-05
      seconds_per_batch_mean: 7.076263427734375e-06
      seconds_per_batch_min: 3.5762786865234375e-06
      seconds_per_batch_std: 3.3214506331270908e-06
  gpu_to_cpu:
    human_readable:
      batch_latency: "217.080 \xB5s +/- 27.877 \xB5s [160.217 \xB5s, 348.806 \xB5\
        s]"
      batches_per_second: 4.67 K +/- 537.52 [2.87 K, 6.24 K]
    metrics:
      batches_per_second_max: 6241.523809523809
      batches_per_second_mean: 4674.046647103995
      batches_per_second_min: 2866.92002734108
      batches_per_second_std: 537.5151524439443
      seconds_per_batch_max: 0.00034880638122558594
      seconds_per_batch_mean: 0.00021708011627197266
      seconds_per_batch_min: 0.00016021728515625
      seconds_per_batch_std: 2.7876841936705584e-05
  on_device_inference:
    human_readable:
      batch_latency: 95.938 ms +/- 4.248 ms [87.365 ms, 113.047 ms]
      batches_per_second: 10.44 +/- 0.45 [8.85, 11.45]
    metrics:
      batches_per_second_max: 11.446211616762545
      batches_per_second_mean: 10.443342061089673
      batches_per_second_min: 8.845905954208776
      batches_per_second_std: 0.4524010027019305
      seconds_per_batch_max: 0.11304664611816406
      seconds_per_batch_mean: 0.09593834400177002
      seconds_per_batch_min: 0.08736515045166016
      seconds_per_batch_std: 0.00424792226370786
  total:
    human_readable:
      batch_latency: 96.163 ms +/- 4.258 ms [87.553 ms, 113.358 ms]
      batches_per_second: 10.42 +/- 0.45 [8.82, 11.42]
    metrics:
      batches_per_second_max: 11.421712206784996
      batches_per_second_mean: 10.418991737877702
      batches_per_second_min: 8.821644905101628
      batches_per_second_std: 0.4512337790355704
      seconds_per_batch_max: 0.1133575439453125
      seconds_per_batch_mean: 0.09616250038146973
      seconds_per_batch_min: 0.08755254745483398
      seconds_per_batch_std: 0.0042575483169278614

Energy results (batch_size=1):
  joules: 55.40220546237627
  kWh: 1.5389501517326743e-05

Warming up with batch_size=128:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=128:  10%|█         | 1/10 [00:02<00:18,  2.02s/it]Warming up with batch_size=128:  20%|██        | 2/10 [00:03<00:15,  1.99s/it]Warming up with batch_size=128:  30%|███       | 3/10 [00:05<00:13,  1.97s/it]Warming up with batch_size=128:  40%|████      | 4/10 [00:07<00:11,  1.96s/it]Warming up with batch_size=128:  50%|█████     | 5/10 [00:09<00:09,  1.95s/it]Warming up with batch_size=128:  60%|██████    | 6/10 [00:11<00:07,  1.95s/it]Warming up with batch_size=128:  70%|███████   | 7/10 [00:13<00:05,  1.96s/it]Warming up with batch_size=128:  80%|████████  | 8/10 [00:15<00:03,  1.96s/it]Warming up with batch_size=128:  90%|█████████ | 9/10 [00:17<00:01,  1.95s/it]Warming up with batch_size=128: 100%|██████████| 10/10 [00:19<00:00,  1.94s/it]Warming up with batch_size=128: 100%|██████████| 10/10 [00:19<00:00,  1.96s/it]
Measuring inference with batch_size=128:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=128:   1%|          | 1/100 [00:01<03:13,  1.95s/it]Measuring inference with batch_size=128:   2%|▏         | 2/100 [00:03<03:09,  1.94s/it]Measuring inference with batch_size=128:   3%|▎         | 3/100 [00:05<03:10,  1.96s/it]Measuring inference with batch_size=128:   4%|▍         | 4/100 [00:07<03:07,  1.95s/it]Measuring inference with batch_size=128:   5%|▌         | 5/100 [00:09<03:04,  1.94s/it]Measuring inference with batch_size=128:   6%|▌         | 6/100 [00:11<03:03,  1.96s/it]Measuring inference with batch_size=128:   7%|▋         | 7/100 [00:13<03:01,  1.95s/it]Measuring inference with batch_size=128:   8%|▊         | 8/100 [00:15<02:58,  1.94s/it]Measuring inference with batch_size=128:   9%|▉         | 9/100 [00:17<02:56,  1.94s/it]Measuring inference with batch_size=128:  10%|█         | 10/100 [00:19<02:55,  1.94s/it]Measuring inference with batch_size=128:  11%|█         | 11/100 [00:21<02:52,  1.94s/it]Measuring inference with batch_size=128:  12%|█▏        | 12/100 [00:23<02:50,  1.94s/it]Measuring inference with batch_size=128:  13%|█▎        | 13/100 [00:25<02:49,  1.95s/it]Measuring inference with batch_size=128:  14%|█▍        | 14/100 [00:27<02:47,  1.95s/it]Measuring inference with batch_size=128:  15%|█▌        | 15/100 [00:29<02:45,  1.94s/it]Measuring inference with batch_size=128:  16%|█▌        | 16/100 [00:31<02:44,  1.96s/it]Measuring inference with batch_size=128:  17%|█▋        | 17/100 [00:33<02:42,  1.95s/it]Measuring inference with batch_size=128:  18%|█▊        | 18/100 [00:35<02:41,  1.97s/it]Measuring inference with batch_size=128:  19%|█▉        | 19/100 [00:37<02:38,  1.96s/it]Measuring inference with batch_size=128:  20%|██        | 20/100 [00:39<02:36,  1.96s/it]Measuring inference with batch_size=128:  21%|██        | 21/100 [00:40<02:34,  1.96s/it]Measuring inference with batch_size=128:  22%|██▏       | 22/100 [00:42<02:32,  1.96s/it]Measuring inference with batch_size=128:  23%|██▎       | 23/100 [00:44<02:29,  1.95s/it]Measuring inference with batch_size=128:  24%|██▍       | 24/100 [00:46<02:27,  1.95s/it]Measuring inference with batch_size=128:  25%|██▌       | 25/100 [00:48<02:26,  1.96s/it]Measuring inference with batch_size=128:  26%|██▌       | 26/100 [00:50<02:24,  1.95s/it]Measuring inference with batch_size=128:  27%|██▋       | 27/100 [00:52<02:22,  1.95s/it]Measuring inference with batch_size=128:  28%|██▊       | 28/100 [00:54<02:20,  1.95s/it]Measuring inference with batch_size=128:  29%|██▉       | 29/100 [00:56<02:18,  1.94s/it]Measuring inference with batch_size=128:  30%|███       | 30/100 [00:58<02:16,  1.95s/it]Measuring inference with batch_size=128:  31%|███       | 31/100 [01:00<02:14,  1.95s/it]Measuring inference with batch_size=128:  32%|███▏      | 32/100 [01:02<02:12,  1.95s/it]Measuring inference with batch_size=128:  33%|███▎      | 33/100 [01:04<02:10,  1.95s/it]Measuring inference with batch_size=128:  34%|███▍      | 34/100 [01:06<02:08,  1.94s/it]Measuring inference with batch_size=128:  35%|███▌      | 35/100 [01:08<02:05,  1.94s/it]Measuring inference with batch_size=128:  36%|███▌      | 36/100 [01:10<02:04,  1.94s/it]Measuring inference with batch_size=128:  37%|███▋      | 37/100 [01:12<02:02,  1.94s/it]Measuring inference with batch_size=128:  38%|███▊      | 38/100 [01:14<02:00,  1.94s/it]Measuring inference with batch_size=128:  39%|███▉      | 39/100 [01:15<01:58,  1.94s/it]Measuring inference with batch_size=128:  40%|████      | 40/100 [01:17<01:56,  1.94s/it]Measuring inference with batch_size=128:  41%|████      | 41/100 [01:19<01:54,  1.94s/it]Measuring inference with batch_size=128:  42%|████▏     | 42/100 [01:21<01:52,  1.94s/it]Measuring inference with batch_size=128:  43%|████▎     | 43/100 [01:23<01:50,  1.94s/it]Measuring inference with batch_size=128:  44%|████▍     | 44/100 [01:25<01:51,  2.00s/it]Measuring inference with batch_size=128:  45%|████▌     | 45/100 [01:28<01:52,  2.04s/it]Measuring inference with batch_size=128:  46%|████▌     | 46/100 [01:30<01:54,  2.11s/it]Measuring inference with batch_size=128:  47%|████▋     | 47/100 [01:32<01:49,  2.06s/it]Measuring inference with batch_size=128:  48%|████▊     | 48/100 [01:34<01:44,  2.02s/it]Measuring inference with batch_size=128:  49%|████▉     | 49/100 [01:36<01:41,  1.99s/it]Measuring inference with batch_size=128:  50%|█████     | 50/100 [01:38<01:41,  2.04s/it]Measuring inference with batch_size=128:  51%|█████     | 51/100 [01:40<01:42,  2.10s/it]Measuring inference with batch_size=128:  52%|█████▏    | 52/100 [01:42<01:39,  2.08s/it]Measuring inference with batch_size=128:  53%|█████▎    | 53/100 [01:44<01:35,  2.04s/it]Measuring inference with batch_size=128:  54%|█████▍    | 54/100 [01:46<01:32,  2.01s/it]Measuring inference with batch_size=128:  55%|█████▌    | 55/100 [01:48<01:29,  2.00s/it]Measuring inference with batch_size=128:  56%|█████▌    | 56/100 [01:50<01:27,  1.98s/it]Measuring inference with batch_size=128:  57%|█████▋    | 57/100 [01:52<01:24,  1.97s/it]Measuring inference with batch_size=128:  58%|█████▊    | 58/100 [01:54<01:22,  1.96s/it]Measuring inference with batch_size=128:  59%|█████▉    | 59/100 [01:56<01:20,  1.95s/it]Measuring inference with batch_size=128:  60%|██████    | 60/100 [01:58<01:18,  1.95s/it]Measuring inference with batch_size=128:  61%|██████    | 61/100 [02:00<01:16,  1.95s/it]Measuring inference with batch_size=128:  62%|██████▏   | 62/100 [02:01<01:14,  1.95s/it]Measuring inference with batch_size=128:  63%|██████▎   | 63/100 [02:04<01:13,  1.98s/it]Measuring inference with batch_size=128:  64%|██████▍   | 64/100 [02:05<01:10,  1.96s/it]Measuring inference with batch_size=128:  65%|██████▌   | 65/100 [02:07<01:08,  1.96s/it]Measuring inference with batch_size=128:  66%|██████▌   | 66/100 [02:09<01:06,  1.95s/it]Measuring inference with batch_size=128:  67%|██████▋   | 67/100 [02:11<01:04,  1.94s/it]Measuring inference with batch_size=128:  68%|██████▊   | 68/100 [02:13<01:02,  1.94s/it]Measuring inference with batch_size=128:  69%|██████▉   | 69/100 [02:15<01:00,  1.94s/it]Measuring inference with batch_size=128:  70%|███████   | 70/100 [02:17<00:58,  1.94s/it]Measuring inference with batch_size=128:  71%|███████   | 71/100 [02:19<00:56,  1.94s/it]Measuring inference with batch_size=128:  72%|███████▏  | 72/100 [02:21<00:54,  1.93s/it]Measuring inference with batch_size=128:  73%|███████▎  | 73/100 [02:23<00:52,  1.95s/it]Measuring inference with batch_size=128:  74%|███████▍  | 74/100 [02:25<00:50,  1.94s/it]Measuring inference with batch_size=128:  75%|███████▌  | 75/100 [02:27<00:48,  1.95s/it]Measuring inference with batch_size=128:  76%|███████▌  | 76/100 [02:29<00:46,  1.95s/it]Measuring inference with batch_size=128:  77%|███████▋  | 77/100 [02:31<00:44,  1.95s/it]Measuring inference with batch_size=128:  78%|███████▊  | 78/100 [02:33<00:42,  1.94s/it]Measuring inference with batch_size=128:  79%|███████▉  | 79/100 [02:35<00:41,  1.97s/it]Measuring inference with batch_size=128:  80%|████████  | 80/100 [02:37<00:39,  1.96s/it]Measuring inference with batch_size=128:  81%|████████  | 81/100 [02:39<00:37,  1.96s/it]Measuring inference with batch_size=128:  82%|████████▏ | 82/100 [02:41<00:35,  1.96s/it]Measuring inference with batch_size=128:  83%|████████▎ | 83/100 [02:42<00:33,  1.96s/it]Measuring inference with batch_size=128:  84%|████████▍ | 84/100 [02:44<00:31,  1.96s/it]Measuring inference with batch_size=128:  85%|████████▌ | 85/100 [02:46<00:29,  1.96s/it]Measuring inference with batch_size=128:  86%|████████▌ | 86/100 [02:48<00:27,  1.95s/it]Measuring inference with batch_size=128:  87%|████████▋ | 87/100 [02:50<00:25,  1.96s/it]Measuring inference with batch_size=128:  88%|████████▊ | 88/100 [02:52<00:23,  1.95s/it]Measuring inference with batch_size=128:  89%|████████▉ | 89/100 [02:54<00:21,  1.94s/it]Measuring inference with batch_size=128:  90%|█████████ | 90/100 [02:56<00:19,  1.96s/it]Measuring inference with batch_size=128:  91%|█████████ | 91/100 [02:58<00:17,  1.95s/it]Measuring inference with batch_size=128:  92%|█████████▏| 92/100 [03:00<00:15,  1.96s/it]Measuring inference with batch_size=128:  93%|█████████▎| 93/100 [03:02<00:13,  1.96s/it]Measuring inference with batch_size=128:  94%|█████████▍| 94/100 [03:04<00:11,  1.96s/it]Measuring inference with batch_size=128:  95%|█████████▌| 95/100 [03:06<00:09,  1.95s/it]Measuring inference with batch_size=128:  96%|█████████▌| 96/100 [03:08<00:07,  1.95s/it]Measuring inference with batch_size=128:  97%|█████████▋| 97/100 [03:10<00:05,  1.95s/it]Measuring inference with batch_size=128:  98%|█████████▊| 98/100 [03:12<00:03,  1.95s/it]Measuring inference with batch_size=128:  99%|█████████▉| 99/100 [03:14<00:01,  1.95s/it]Measuring inference with batch_size=128: 100%|██████████| 100/100 [03:16<00:00,  1.98s/it]Measuring inference with batch_size=128: 100%|██████████| 100/100 [03:16<00:00,  1.96s/it]Timing results (batch_size=128):
  cpu_to_gpu:
    human_readable:
      batch_latency: "8.979 \xB5s +/- 4.400 \xB5s [5.960 \xB5s, 42.677 \xB5s]"
      batches_per_second: 123.72 K +/- 30.75 K [23.43 K, 167.77 K]
    metrics:
      batches_per_second_max: 167772.16
      batches_per_second_mean: 123719.82882000686
      batches_per_second_min: 23431.86592178771
      batches_per_second_std: 30751.442204678424
      seconds_per_batch_max: 4.267692565917969e-05
      seconds_per_batch_mean: 8.978843688964843e-06
      seconds_per_batch_min: 5.9604644775390625e-06
      seconds_per_batch_std: 4.39986872966453e-06
  gpu_to_cpu:
    human_readable:
      batch_latency: 8.438 ms +/- 1.006 ms [7.926 ms, 15.877 ms]
      batches_per_second: 119.59 +/- 9.24 [62.98, 126.16]
    metrics:
      batches_per_second_max: 126.15965830475847
      batches_per_second_mean: 119.59033087849004
      batches_per_second_min: 62.9841574940309
      batches_per_second_std: 9.237811693913265
      seconds_per_batch_max: 0.01587700843811035
      seconds_per_batch_mean: 0.008438131809234618
      seconds_per_batch_min: 0.007926464080810547
      seconds_per_batch_std: 0.0010058150183009085
  on_device_inference:
    human_readable:
      batch_latency: 1.953 s +/- 59.121 ms [1.912 s, 2.268 s]
      batches_per_second: 0.51 +/- 0.01 [0.44, 0.52]
    metrics:
      batches_per_second_max: 0.5228822963066666
      batches_per_second_mean: 0.5124263173682366
      batches_per_second_min: 0.4408823693575956
      batches_per_second_std: 0.014100391932909432
      seconds_per_batch_max: 2.268178701400757
      seconds_per_batch_mean: 1.9531249618530273
      seconds_per_batch_min: 1.9124763011932373
      seconds_per_batch_std: 0.059121381791960526
  total:
    human_readable:
      batch_latency: 1.962 s +/- 59.633 ms [1.921 s, 2.277 s]
      batches_per_second: 0.51 +/- 0.01 [0.44, 0.52]
    metrics:
      batches_per_second_max: 0.5205967736510241
      batches_per_second_mean: 0.510222905390508
      batches_per_second_min: 0.4391436001239229
      batches_per_second_std: 0.01408652589760567
      seconds_per_batch_max: 2.2771594524383545
      seconds_per_batch_mean: 1.961572072505951
      seconds_per_batch_min: 1.920872449874878
      seconds_per_batch_std: 0.059632726307386794

Energy results (batch_size=128):
  joules: 52.16743931533497
  kWh: 1.4490955365370824e-05

learner.infer:
  device: cuda
  energy:
    batch_size_1:
      joules: 55.40220546237627
      kWh: 1.5389501517326743e-05
    batch_size_128:
      joules: 52.16743931533497
      kWh: 1.4490955365370824e-05
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 25.27 GB
      total: 31.17 GB
      used: 8.88 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  max_inference_memory: 7553511424
  post_inference_memory: 7504768512
  pre_inference_memory: 15445504
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "7.076 \xB5s +/- 3.321 \xB5s [3.576 \xB5s, 13.828 \xB5s]"
          batches_per_second: 171.96 K +/- 68.09 K [72.32 K, 279.62 K]
        metrics:
          batches_per_second_max: 279620.26666666666
          batches_per_second_mean: 171962.63935087438
          batches_per_second_min: 72315.58620689655
          batches_per_second_std: 68093.06180326438
          seconds_per_batch_max: 1.3828277587890625e-05
          seconds_per_batch_mean: 7.076263427734375e-06
          seconds_per_batch_min: 3.5762786865234375e-06
          seconds_per_batch_std: 3.3214506331270908e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: "217.080 \xB5s +/- 27.877 \xB5s [160.217 \xB5s, 348.806 \xB5\
            s]"
          batches_per_second: 4.67 K +/- 537.52 [2.87 K, 6.24 K]
        metrics:
          batches_per_second_max: 6241.523809523809
          batches_per_second_mean: 4674.046647103995
          batches_per_second_min: 2866.92002734108
          batches_per_second_std: 537.5151524439443
          seconds_per_batch_max: 0.00034880638122558594
          seconds_per_batch_mean: 0.00021708011627197266
          seconds_per_batch_min: 0.00016021728515625
          seconds_per_batch_std: 2.7876841936705584e-05
      on_device_inference:
        human_readable:
          batch_latency: 95.938 ms +/- 4.248 ms [87.365 ms, 113.047 ms]
          batches_per_second: 10.44 +/- 0.45 [8.85, 11.45]
        metrics:
          batches_per_second_max: 11.446211616762545
          batches_per_second_mean: 10.443342061089673
          batches_per_second_min: 8.845905954208776
          batches_per_second_std: 0.4524010027019305
          seconds_per_batch_max: 0.11304664611816406
          seconds_per_batch_mean: 0.09593834400177002
          seconds_per_batch_min: 0.08736515045166016
          seconds_per_batch_std: 0.00424792226370786
      total:
        human_readable:
          batch_latency: 96.163 ms +/- 4.258 ms [87.553 ms, 113.358 ms]
          batches_per_second: 10.42 +/- 0.45 [8.82, 11.42]
        metrics:
          batches_per_second_max: 11.421712206784996
          batches_per_second_mean: 10.418991737877702
          batches_per_second_min: 8.821644905101628
          batches_per_second_std: 0.4512337790355704
          seconds_per_batch_max: 0.1133575439453125
          seconds_per_batch_mean: 0.09616250038146973
          seconds_per_batch_min: 0.08755254745483398
          seconds_per_batch_std: 0.0042575483169278614
    batch_size_128:
      cpu_to_gpu:
        human_readable:
          batch_latency: "8.979 \xB5s +/- 4.400 \xB5s [5.960 \xB5s, 42.677 \xB5s]"
          batches_per_second: 123.72 K +/- 30.75 K [23.43 K, 167.77 K]
        metrics:
          batches_per_second_max: 167772.16
          batches_per_second_mean: 123719.82882000686
          batches_per_second_min: 23431.86592178771
          batches_per_second_std: 30751.442204678424
          seconds_per_batch_max: 4.267692565917969e-05
          seconds_per_batch_mean: 8.978843688964843e-06
          seconds_per_batch_min: 5.9604644775390625e-06
          seconds_per_batch_std: 4.39986872966453e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: 8.438 ms +/- 1.006 ms [7.926 ms, 15.877 ms]
          batches_per_second: 119.59 +/- 9.24 [62.98, 126.16]
        metrics:
          batches_per_second_max: 126.15965830475847
          batches_per_second_mean: 119.59033087849004
          batches_per_second_min: 62.9841574940309
          batches_per_second_std: 9.237811693913265
          seconds_per_batch_max: 0.01587700843811035
          seconds_per_batch_mean: 0.008438131809234618
          seconds_per_batch_min: 0.007926464080810547
          seconds_per_batch_std: 0.0010058150183009085
      on_device_inference:
        human_readable:
          batch_latency: 1.953 s +/- 59.121 ms [1.912 s, 2.268 s]
          batches_per_second: 0.51 +/- 0.01 [0.44, 0.52]
        metrics:
          batches_per_second_max: 0.5228822963066666
          batches_per_second_mean: 0.5124263173682366
          batches_per_second_min: 0.4408823693575956
          batches_per_second_std: 0.014100391932909432
          seconds_per_batch_max: 2.268178701400757
          seconds_per_batch_mean: 1.9531249618530273
          seconds_per_batch_min: 1.9124763011932373
          seconds_per_batch_std: 0.059121381791960526
      total:
        human_readable:
          batch_latency: 1.962 s +/- 59.633 ms [1.921 s, 2.277 s]
          batches_per_second: 0.51 +/- 0.01 [0.44, 0.52]
        metrics:
          batches_per_second_max: 0.5205967736510241
          batches_per_second_mean: 0.510222905390508
          batches_per_second_min: 0.4391436001239229
          batches_per_second_std: 0.01408652589760567
          seconds_per_batch_max: 2.2771594524383545
          seconds_per_batch_mean: 1.961572072505951
          seconds_per_batch_min: 1.920872449874878
          seconds_per_batch_std: 0.059632726307386794

== Benchmarking model directly ==
Machine info:
  cpu:
    architecture: aarch64
    cores:
      physical: 8
      total: 8
    frequency: 2.27 GHz
    model: ARMv8 Processor rev 0 (v8l)
  gpus: null
  memory:
    available: 16.45 GB
    total: 31.17 GB
    used: 16.88 GB
  system:
    node: xavier
    release: 4.9.140-tegra
    system: Linux


Model device: cuda:0
Model parameters: 3794322 (3.79 M)
Model FLOPs: 138228272 (138.23 M)
Allocated GPU memory prior to inference: 73787904 (70.37 MB)
Allocated GPU memory after to inference: 7516507648 (7.00 GB)
Max allocated GPU memory during inference: 7566396928 (7.05 GB)
Warming up with batch_size=128:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=128:  10%|█         | 1/10 [00:00<00:01,  8.71it/s]Warming up with batch_size=128:  30%|███       | 3/10 [00:00<00:00, 10.51it/s]Warming up with batch_size=128:  50%|█████     | 5/10 [00:00<00:00, 10.90it/s]Warming up with batch_size=128:  70%|███████   | 7/10 [00:00<00:00, 10.77it/s]Warming up with batch_size=128:  90%|█████████ | 9/10 [00:00<00:00, 10.90it/s]Warming up with batch_size=128: 100%|██████████| 10/10 [00:00<00:00, 10.76it/s]
Measuring inference with batch_size=128:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=128:   1%|          | 1/100 [00:00<00:09,  9.92it/s]Measuring inference with batch_size=128:   3%|▎         | 3/100 [00:00<00:08, 10.94it/s]Measuring inference with batch_size=128:   5%|▌         | 5/100 [00:00<00:08, 10.82it/s]Measuring inference with batch_size=128:   7%|▋         | 7/100 [00:00<00:08, 10.96it/s]Measuring inference with batch_size=128:   9%|▉         | 9/100 [00:00<00:08, 10.89it/s]Measuring inference with batch_size=128:  11%|█         | 11/100 [00:01<00:08, 10.98it/s]Measuring inference with batch_size=128:  13%|█▎        | 13/100 [00:01<00:08, 10.69it/s]Measuring inference with batch_size=128:  15%|█▌        | 15/100 [00:01<00:07, 10.88it/s]Measuring inference with batch_size=128:  17%|█▋        | 17/100 [00:01<00:07, 11.06it/s]Measuring inference with batch_size=128:  19%|█▉        | 19/100 [00:01<00:07, 11.14it/s]Measuring inference with batch_size=128:  21%|██        | 21/100 [00:01<00:06, 11.34it/s]Measuring inference with batch_size=128:  23%|██▎       | 23/100 [00:02<00:06, 11.50it/s]Measuring inference with batch_size=128:  25%|██▌       | 25/100 [00:02<00:06, 11.61it/s]Measuring inference with batch_size=128:  27%|██▋       | 27/100 [00:02<00:06, 11.74it/s]Measuring inference with batch_size=128:  29%|██▉       | 29/100 [00:02<00:06, 11.78it/s]Measuring inference with batch_size=128:  31%|███       | 31/100 [00:02<00:05, 11.77it/s]Measuring inference with batch_size=128:  33%|███▎      | 33/100 [00:02<00:05, 11.86it/s]Measuring inference with batch_size=128:  35%|███▌      | 35/100 [00:03<00:05, 11.97it/s]Measuring inference with batch_size=128:  37%|███▋      | 37/100 [00:03<00:05, 11.95it/s]Measuring inference with batch_size=128:  39%|███▉      | 39/100 [00:03<00:05, 11.97it/s]Measuring inference with batch_size=128:  41%|████      | 41/100 [00:03<00:04, 12.02it/s]Measuring inference with batch_size=128:  43%|████▎     | 43/100 [00:03<00:04, 12.00it/s]Measuring inference with batch_size=128:  45%|████▌     | 45/100 [00:03<00:04, 11.99it/s]Measuring inference with batch_size=128:  47%|████▋     | 47/100 [00:04<00:04, 12.04it/s]Measuring inference with batch_size=128:  49%|████▉     | 49/100 [00:04<00:04, 12.06it/s]Measuring inference with batch_size=128:  51%|█████     | 51/100 [00:04<00:04, 12.09it/s]Measuring inference with batch_size=128:  53%|█████▎    | 53/100 [00:04<00:03, 12.12it/s]Measuring inference with batch_size=128:  55%|█████▌    | 55/100 [00:04<00:03, 12.12it/s]Measuring inference with batch_size=128:  57%|█████▋    | 57/100 [00:04<00:03, 12.16it/s]Measuring inference with batch_size=128:  59%|█████▉    | 59/100 [00:05<00:03, 12.19it/s]Measuring inference with batch_size=128:  61%|██████    | 61/100 [00:05<00:03, 12.14it/s]Measuring inference with batch_size=128:  63%|██████▎   | 63/100 [00:05<00:03, 12.13it/s]Measuring inference with batch_size=128:  65%|██████▌   | 65/100 [00:05<00:02, 12.16it/s]Measuring inference with batch_size=128:  67%|██████▋   | 67/100 [00:05<00:02, 12.12it/s]Measuring inference with batch_size=128:  69%|██████▉   | 69/100 [00:05<00:02, 12.17it/s]Measuring inference with batch_size=128:  71%|███████   | 71/100 [00:06<00:02, 12.22it/s]Measuring inference with batch_size=128:  73%|███████▎  | 73/100 [00:06<00:02, 12.17it/s]Measuring inference with batch_size=128:  75%|███████▌  | 75/100 [00:06<00:02, 12.13it/s]Measuring inference with batch_size=128:  77%|███████▋  | 77/100 [00:06<00:01, 12.17it/s]Measuring inference with batch_size=128:  79%|███████▉  | 79/100 [00:06<00:01, 12.12it/s]Measuring inference with batch_size=128:  81%|████████  | 81/100 [00:06<00:01, 12.13it/s]Measuring inference with batch_size=128:  83%|████████▎ | 83/100 [00:07<00:01, 12.14it/s]Measuring inference with batch_size=128:  85%|████████▌ | 85/100 [00:07<00:01, 12.10it/s]Measuring inference with batch_size=128:  87%|████████▋ | 87/100 [00:07<00:01, 12.08it/s]Measuring inference with batch_size=128:  89%|████████▉ | 89/100 [00:07<00:00, 12.08it/s]Measuring inference with batch_size=128:  91%|█████████ | 91/100 [00:07<00:00, 12.10it/s]Measuring inference with batch_size=128:  93%|█████████▎| 93/100 [00:07<00:00, 12.12it/s]Measuring inference with batch_size=128:  95%|█████████▌| 95/100 [00:08<00:00, 12.18it/s]Measuring inference with batch_size=128:  97%|█████████▋| 97/100 [00:08<00:00, 12.17it/s]Measuring inference with batch_size=128:  99%|█████████▉| 99/100 [00:08<00:00, 12.11it/s]Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:08<00:00, 11.85it/s]
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "182.989 \xB5s +/- 35.700 \xB5s [142.813 \xB5s, 327.587 \xB5\
        s]"
      batches_per_second: 5.63 K +/- 894.68 [3.05 K, 7.00 K]
    metrics:
      batches_per_second_max: 7002.176961602671
      batches_per_second_mean: 5634.751785412573
      batches_per_second_min: 3052.622998544396
      batches_per_second_std: 894.680284227401
      seconds_per_batch_max: 0.0003275871276855469
      seconds_per_batch_mean: 0.00018298864364624023
      seconds_per_batch_min: 0.00014281272888183594
      seconds_per_batch_std: 3.569951014668775e-05
  gpu_to_cpu:
    human_readable:
      batch_latency: "159.934 \xB5s +/- 28.570 \xB5s [132.799 \xB5s, 375.032 \xB5\
        s]"
      batches_per_second: 6.38 K +/- 773.00 [2.67 K, 7.53 K]
    metrics:
      batches_per_second_max: 7530.168761220826
      batches_per_second_mean: 6380.968235468852
      batches_per_second_min: 2666.4361093452003
      batches_per_second_std: 772.9992256342634
      seconds_per_batch_max: 0.0003750324249267578
      seconds_per_batch_mean: 0.00015993356704711913
      seconds_per_batch_min: 0.0001327991485595703
      seconds_per_batch_std: 2.8569524239696728e-05
  on_device_inference:
    human_readable:
      batch_latency: 83.567 ms +/- 4.018 ms [79.978 ms, 100.348 ms]
      batches_per_second: 11.99 +/- 0.52 [9.97, 12.50]
    metrics:
      batches_per_second_max: 12.503365588128327
      batches_per_second_mean: 11.991487612670687
      batches_per_second_min: 9.96534478211595
      batches_per_second_std: 0.5227106078439642
      seconds_per_batch_max: 0.10034775733947754
      seconds_per_batch_mean: 0.08356724500656128
      seconds_per_batch_min: 0.07997846603393555
      seconds_per_batch_std: 0.004017538715602513
  total:
    human_readable:
      batch_latency: 83.910 ms +/- 4.041 ms [80.308 ms, 100.774 ms]
      batches_per_second: 11.94 +/- 0.52 [9.92, 12.45]
    metrics:
      batches_per_second_max: 12.451991758649559
      batches_per_second_mean: 11.942557449344116
      batches_per_second_min: 9.923236529248241
      batches_per_second_std: 0.5213163742349076
      seconds_per_batch_max: 0.10077357292175293
      seconds_per_batch_mean: 0.08391016721725464
      seconds_per_batch_min: 0.08030843734741211
      seconds_per_batch_std: 0.004040639741900896

Energy results (batch_size=1):
  joules: 54.80376585629781
  kWh: 1.5223268293416058e-05

Warming up with batch_size=128:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=128:  10%|█         | 1/10 [00:01<00:16,  1.87s/it]Warming up with batch_size=128:  20%|██        | 2/10 [00:03<00:14,  1.87s/it]Warming up with batch_size=128:  30%|███       | 3/10 [00:05<00:13,  1.87s/it]Warming up with batch_size=128:  40%|████      | 4/10 [00:07<00:11,  1.87s/it]Warming up with batch_size=128:  50%|█████     | 5/10 [00:09<00:09,  1.87s/it]Warming up with batch_size=128:  60%|██████    | 6/10 [00:11<00:07,  1.87s/it]Warming up with batch_size=128:  70%|███████   | 7/10 [00:13<00:05,  1.87s/it]Warming up with batch_size=128:  80%|████████  | 8/10 [00:14<00:03,  1.87s/it]Warming up with batch_size=128:  90%|█████████ | 9/10 [00:16<00:01,  1.87s/it]Warming up with batch_size=128: 100%|██████████| 10/10 [00:18<00:00,  1.87s/it]Warming up with batch_size=128: 100%|██████████| 10/10 [00:18<00:00,  1.87s/it]
Measuring inference with batch_size=128:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=128:   1%|          | 1/100 [00:01<03:04,  1.87s/it]Measuring inference with batch_size=128:   2%|▏         | 2/100 [00:03<03:03,  1.87s/it]Measuring inference with batch_size=128:   3%|▎         | 3/100 [00:05<03:01,  1.87s/it]Measuring inference with batch_size=128:   4%|▍         | 4/100 [00:07<02:59,  1.87s/it]Measuring inference with batch_size=128:   5%|▌         | 5/100 [00:09<02:57,  1.87s/it]Measuring inference with batch_size=128:   6%|▌         | 6/100 [00:11<02:55,  1.87s/it]Measuring inference with batch_size=128:   7%|▋         | 7/100 [00:13<02:53,  1.87s/it]Measuring inference with batch_size=128:   8%|▊         | 8/100 [00:14<02:52,  1.87s/it]Measuring inference with batch_size=128:   9%|▉         | 9/100 [00:16<02:50,  1.87s/it]Measuring inference with batch_size=128:  10%|█         | 10/100 [00:18<02:48,  1.87s/it]Measuring inference with batch_size=128:  11%|█         | 11/100 [00:20<02:46,  1.87s/it]Measuring inference with batch_size=128:  12%|█▏        | 12/100 [00:22<02:44,  1.87s/it]Measuring inference with batch_size=128:  13%|█▎        | 13/100 [00:24<02:42,  1.87s/it]Measuring inference with batch_size=128:  14%|█▍        | 14/100 [00:26<02:40,  1.87s/it]Measuring inference with batch_size=128:  15%|█▌        | 15/100 [00:28<02:38,  1.87s/it]Measuring inference with batch_size=128:  16%|█▌        | 16/100 [00:29<02:37,  1.87s/it]Measuring inference with batch_size=128:  17%|█▋        | 17/100 [00:31<02:35,  1.87s/it]Measuring inference with batch_size=128:  18%|█▊        | 18/100 [00:33<02:33,  1.87s/it]Measuring inference with batch_size=128:  19%|█▉        | 19/100 [00:35<02:31,  1.87s/it]Measuring inference with batch_size=128:  20%|██        | 20/100 [00:37<02:29,  1.87s/it]Measuring inference with batch_size=128:  21%|██        | 21/100 [00:39<02:27,  1.87s/it]Measuring inference with batch_size=128:  22%|██▏       | 22/100 [00:41<02:26,  1.87s/it]Measuring inference with batch_size=128:  23%|██▎       | 23/100 [00:43<02:24,  1.87s/it]Measuring inference with batch_size=128:  24%|██▍       | 24/100 [00:44<02:22,  1.87s/it]Measuring inference with batch_size=128:  25%|██▌       | 25/100 [00:46<02:20,  1.87s/it]Measuring inference with batch_size=128:  26%|██▌       | 26/100 [00:48<02:18,  1.87s/it]Measuring inference with batch_size=128:  27%|██▋       | 27/100 [00:50<02:16,  1.87s/it]Measuring inference with batch_size=128:  28%|██▊       | 28/100 [00:52<02:14,  1.87s/it]Measuring inference with batch_size=128:  29%|██▉       | 29/100 [00:54<02:12,  1.87s/it]Measuring inference with batch_size=128:  30%|███       | 30/100 [00:56<02:10,  1.87s/it]Measuring inference with batch_size=128:  31%|███       | 31/100 [00:57<02:09,  1.87s/it]Measuring inference with batch_size=128:  32%|███▏      | 32/100 [00:59<02:07,  1.87s/it]Measuring inference with batch_size=128:  33%|███▎      | 33/100 [01:01<02:05,  1.87s/it]Measuring inference with batch_size=128:  34%|███▍      | 34/100 [01:03<02:03,  1.87s/it]Measuring inference with batch_size=128:  35%|███▌      | 35/100 [01:05<02:01,  1.87s/it]Measuring inference with batch_size=128:  36%|███▌      | 36/100 [01:07<01:59,  1.87s/it]Measuring inference with batch_size=128:  37%|███▋      | 37/100 [01:09<01:57,  1.87s/it]Measuring inference with batch_size=128:  38%|███▊      | 38/100 [01:11<01:55,  1.87s/it]Measuring inference with batch_size=128:  39%|███▉      | 39/100 [01:12<01:54,  1.87s/it]Measuring inference with batch_size=128:  40%|████      | 40/100 [01:14<01:52,  1.87s/it]Measuring inference with batch_size=128:  41%|████      | 41/100 [01:16<01:50,  1.87s/it]Measuring inference with batch_size=128:  42%|████▏     | 42/100 [01:18<01:48,  1.87s/it]Measuring inference with batch_size=128:  43%|████▎     | 43/100 [01:20<01:46,  1.87s/it]Measuring inference with batch_size=128:  44%|████▍     | 44/100 [01:22<01:44,  1.87s/it]Measuring inference with batch_size=128:  45%|████▌     | 45/100 [01:24<01:42,  1.87s/it]Measuring inference with batch_size=128:  46%|████▌     | 46/100 [01:26<01:40,  1.87s/it]Measuring inference with batch_size=128:  47%|████▋     | 47/100 [01:27<01:39,  1.87s/it]Measuring inference with batch_size=128:  48%|████▊     | 48/100 [01:29<01:37,  1.87s/it]Measuring inference with batch_size=128:  49%|████▉     | 49/100 [01:31<01:35,  1.87s/it]Measuring inference with batch_size=128:  50%|█████     | 50/100 [01:33<01:33,  1.87s/it]Measuring inference with batch_size=128:  51%|█████     | 51/100 [01:35<01:31,  1.87s/it]Measuring inference with batch_size=128:  52%|█████▏    | 52/100 [01:37<01:29,  1.87s/it]Measuring inference with batch_size=128:  53%|█████▎    | 53/100 [01:39<01:27,  1.87s/it]Measuring inference with batch_size=128:  54%|█████▍    | 54/100 [01:41<01:26,  1.87s/it]Measuring inference with batch_size=128:  55%|█████▌    | 55/100 [01:42<01:24,  1.87s/it]Measuring inference with batch_size=128:  56%|█████▌    | 56/100 [01:44<01:22,  1.87s/it]Measuring inference with batch_size=128:  57%|█████▋    | 57/100 [01:46<01:20,  1.87s/it]Measuring inference with batch_size=128:  58%|█████▊    | 58/100 [01:48<01:18,  1.87s/it]Measuring inference with batch_size=128:  59%|█████▉    | 59/100 [01:50<01:16,  1.87s/it]Measuring inference with batch_size=128:  60%|██████    | 60/100 [01:52<01:14,  1.87s/it]Measuring inference with batch_size=128:  61%|██████    | 61/100 [01:54<01:12,  1.87s/it]Measuring inference with batch_size=128:  62%|██████▏   | 62/100 [01:55<01:11,  1.87s/it]Measuring inference with batch_size=128:  63%|██████▎   | 63/100 [01:57<01:09,  1.87s/it]Measuring inference with batch_size=128:  64%|██████▍   | 64/100 [01:59<01:07,  1.87s/it]Measuring inference with batch_size=128:  65%|██████▌   | 65/100 [02:01<01:05,  1.87s/it]Measuring inference with batch_size=128:  66%|██████▌   | 66/100 [02:03<01:03,  1.87s/it]Measuring inference with batch_size=128:  67%|██████▋   | 67/100 [02:05<01:01,  1.87s/it]Measuring inference with batch_size=128:  68%|██████▊   | 68/100 [02:07<00:59,  1.87s/it]Measuring inference with batch_size=128:  69%|██████▉   | 69/100 [02:09<00:57,  1.87s/it]Measuring inference with batch_size=128:  70%|███████   | 70/100 [02:10<00:56,  1.87s/it]Measuring inference with batch_size=128:  71%|███████   | 71/100 [02:12<00:54,  1.87s/it]Measuring inference with batch_size=128:  72%|███████▏  | 72/100 [02:14<00:52,  1.87s/it]Measuring inference with batch_size=128:  73%|███████▎  | 73/100 [02:16<00:50,  1.87s/it]Measuring inference with batch_size=128:  74%|███████▍  | 74/100 [02:18<00:48,  1.87s/it]Measuring inference with batch_size=128:  75%|███████▌  | 75/100 [02:20<00:46,  1.87s/it]Measuring inference with batch_size=128:  76%|███████▌  | 76/100 [02:22<00:44,  1.87s/it]Measuring inference with batch_size=128:  77%|███████▋  | 77/100 [02:24<00:43,  1.87s/it]Measuring inference with batch_size=128:  78%|███████▊  | 78/100 [02:25<00:41,  1.87s/it]Measuring inference with batch_size=128:  79%|███████▉  | 79/100 [02:27<00:39,  1.87s/it]Measuring inference with batch_size=128:  80%|████████  | 80/100 [02:29<00:37,  1.87s/it]Measuring inference with batch_size=128:  81%|████████  | 81/100 [02:31<00:35,  1.87s/it]Measuring inference with batch_size=128:  82%|████████▏ | 82/100 [02:33<00:33,  1.87s/it]Measuring inference with batch_size=128:  83%|████████▎ | 83/100 [02:35<00:31,  1.87s/it]Measuring inference with batch_size=128:  84%|████████▍ | 84/100 [02:37<00:29,  1.87s/it]Measuring inference with batch_size=128:  85%|████████▌ | 85/100 [02:38<00:28,  1.87s/it]Measuring inference with batch_size=128:  86%|████████▌ | 86/100 [02:40<00:26,  1.87s/it]Measuring inference with batch_size=128:  87%|████████▋ | 87/100 [02:42<00:24,  1.87s/it]Measuring inference with batch_size=128:  88%|████████▊ | 88/100 [02:44<00:22,  1.87s/it]Measuring inference with batch_size=128:  89%|████████▉ | 89/100 [02:46<00:20,  1.87s/it]Measuring inference with batch_size=128:  90%|█████████ | 90/100 [02:48<00:18,  1.87s/it]Measuring inference with batch_size=128:  91%|█████████ | 91/100 [02:50<00:16,  1.87s/it]Measuring inference with batch_size=128:  92%|█████████▏| 92/100 [02:52<00:14,  1.87s/it]Measuring inference with batch_size=128:  93%|█████████▎| 93/100 [02:53<00:13,  1.87s/it]Measuring inference with batch_size=128:  94%|█████████▍| 94/100 [02:55<00:11,  1.87s/it]Measuring inference with batch_size=128:  95%|█████████▌| 95/100 [02:57<00:09,  1.87s/it]Measuring inference with batch_size=128:  96%|█████████▌| 96/100 [02:59<00:07,  1.87s/it]Measuring inference with batch_size=128:  97%|█████████▋| 97/100 [03:01<00:05,  1.87s/it]Measuring inference with batch_size=128:  98%|█████████▊| 98/100 [03:03<00:03,  1.87s/it]Measuring inference with batch_size=128:  99%|█████████▉| 99/100 [03:05<00:01,  1.87s/it]Measuring inference with batch_size=128: 100%|██████████| 100/100 [03:07<00:00,  1.87s/it]Measuring inference with batch_size=128: 100%|██████████| 100/100 [03:07<00:00,  1.87s/it]Timing results (batch_size=128):
  cpu_to_gpu:
    human_readable:
      batch_latency: "7.139 ms +/- 383.743 \xB5s [5.253 ms, 8.085 ms]"
      batches_per_second: 140.52 +/- 8.35 [123.68, 190.36]
    metrics:
      batches_per_second_max: 190.364634865883
      batches_per_second_mean: 140.52477511670315
      batches_per_second_min: 123.68200047180939
      batches_per_second_std: 8.349151815015759
      seconds_per_batch_max: 0.008085250854492188
      seconds_per_batch_mean: 0.007138748168945313
      seconds_per_batch_min: 0.0052530765533447266
      seconds_per_batch_std: 0.0003837428208825967
  gpu_to_cpu:
    human_readable:
      batch_latency: 89.012 ms +/- 1.463 ms [87.612 ms, 93.117 ms]
      batches_per_second: 11.24 +/- 0.18 [10.74, 11.41]
    metrics:
      batches_per_second_max: 11.414003864261028
      batches_per_second_mean: 11.237369080921685
      batches_per_second_min: 10.73923274076388
      batches_per_second_std: 0.18046059041487605
      seconds_per_batch_max: 0.09311652183532715
      seconds_per_batch_mean: 0.08901228666305543
      seconds_per_batch_min: 0.08761167526245117
      seconds_per_batch_std: 0.001462536416265145
  on_device_inference:
    human_readable:
      batch_latency: 1.773 s +/- 2.342 ms [1.769 s, 1.783 s]
      batches_per_second: 0.56 +/- 0.00 [0.56, 0.57]
    metrics:
      batches_per_second_max: 0.5654474881067807
      batches_per_second_mean: 0.5640242993921452
      batches_per_second_min: 0.5608767979117746
      batches_per_second_std: 0.0007441541191971278
      seconds_per_batch_max: 1.7829227447509766
      seconds_per_batch_mean: 1.772976348400116
      seconds_per_batch_min: 1.7685108184814453
      seconds_per_batch_std: 0.00234192224859872
  total:
    human_readable:
      batch_latency: 1.869 s +/- 2.100 ms [1.864 s, 1.877 s]
      batches_per_second: 0.54 +/- 0.00 [0.53, 0.54]
    metrics:
      batches_per_second_max: 0.536551350600404
      batches_per_second_mean: 0.5350096897318888
      batches_per_second_min: 0.532693030393434
      batches_per_second_std: 0.0006006776646385452
      seconds_per_batch_max: 1.877253770828247
      seconds_per_batch_mean: 1.8691273832321167
      seconds_per_batch_min: 1.8637545108795166
      seconds_per_batch_std: 0.0020999214416366364

Energy results (batch_size=128):
  joules: 50.93208313268028
  kWh: 1.4147800870188968e-05

learner.model.forward:
  device: cuda
  energy:
    batch_size_1:
      joules: 54.80376585629781
      kWh: 1.5223268293416058e-05
    batch_size_128:
      joules: 50.93208313268028
      kWh: 1.4147800870188968e-05
  flops: 138228272
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 16.45 GB
      total: 31.17 GB
      used: 16.88 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  max_inference_memory: 7566396928
  params: 3794322
  post_inference_memory: 7516507648
  pre_inference_memory: 73787904
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "182.989 \xB5s +/- 35.700 \xB5s [142.813 \xB5s, 327.587 \xB5\
            s]"
          batches_per_second: 5.63 K +/- 894.68 [3.05 K, 7.00 K]
        metrics:
          batches_per_second_max: 7002.176961602671
          batches_per_second_mean: 5634.751785412573
          batches_per_second_min: 3052.622998544396
          batches_per_second_std: 894.680284227401
          seconds_per_batch_max: 0.0003275871276855469
          seconds_per_batch_mean: 0.00018298864364624023
          seconds_per_batch_min: 0.00014281272888183594
          seconds_per_batch_std: 3.569951014668775e-05
      gpu_to_cpu:
        human_readable:
          batch_latency: "159.934 \xB5s +/- 28.570 \xB5s [132.799 \xB5s, 375.032 \xB5\
            s]"
          batches_per_second: 6.38 K +/- 773.00 [2.67 K, 7.53 K]
        metrics:
          batches_per_second_max: 7530.168761220826
          batches_per_second_mean: 6380.968235468852
          batches_per_second_min: 2666.4361093452003
          batches_per_second_std: 772.9992256342634
          seconds_per_batch_max: 0.0003750324249267578
          seconds_per_batch_mean: 0.00015993356704711913
          seconds_per_batch_min: 0.0001327991485595703
          seconds_per_batch_std: 2.8569524239696728e-05
      on_device_inference:
        human_readable:
          batch_latency: 83.567 ms +/- 4.018 ms [79.978 ms, 100.348 ms]
          batches_per_second: 11.99 +/- 0.52 [9.97, 12.50]
        metrics:
          batches_per_second_max: 12.503365588128327
          batches_per_second_mean: 11.991487612670687
          batches_per_second_min: 9.96534478211595
          batches_per_second_std: 0.5227106078439642
          seconds_per_batch_max: 0.10034775733947754
          seconds_per_batch_mean: 0.08356724500656128
          seconds_per_batch_min: 0.07997846603393555
          seconds_per_batch_std: 0.004017538715602513
      total:
        human_readable:
          batch_latency: 83.910 ms +/- 4.041 ms [80.308 ms, 100.774 ms]
          batches_per_second: 11.94 +/- 0.52 [9.92, 12.45]
        metrics:
          batches_per_second_max: 12.451991758649559
          batches_per_second_mean: 11.942557449344116
          batches_per_second_min: 9.923236529248241
          batches_per_second_std: 0.5213163742349076
          seconds_per_batch_max: 0.10077357292175293
          seconds_per_batch_mean: 0.08391016721725464
          seconds_per_batch_min: 0.08030843734741211
          seconds_per_batch_std: 0.004040639741900896
    batch_size_128:
      cpu_to_gpu:
        human_readable:
          batch_latency: "7.139 ms +/- 383.743 \xB5s [5.253 ms, 8.085 ms]"
          batches_per_second: 140.52 +/- 8.35 [123.68, 190.36]
        metrics:
          batches_per_second_max: 190.364634865883
          batches_per_second_mean: 140.52477511670315
          batches_per_second_min: 123.68200047180939
          batches_per_second_std: 8.349151815015759
          seconds_per_batch_max: 0.008085250854492188
          seconds_per_batch_mean: 0.007138748168945313
          seconds_per_batch_min: 0.0052530765533447266
          seconds_per_batch_std: 0.0003837428208825967
      gpu_to_cpu:
        human_readable:
          batch_latency: 89.012 ms +/- 1.463 ms [87.612 ms, 93.117 ms]
          batches_per_second: 11.24 +/- 0.18 [10.74, 11.41]
        metrics:
          batches_per_second_max: 11.414003864261028
          batches_per_second_mean: 11.237369080921685
          batches_per_second_min: 10.73923274076388
          batches_per_second_std: 0.18046059041487605
          seconds_per_batch_max: 0.09311652183532715
          seconds_per_batch_mean: 0.08901228666305543
          seconds_per_batch_min: 0.08761167526245117
          seconds_per_batch_std: 0.001462536416265145
      on_device_inference:
        human_readable:
          batch_latency: 1.773 s +/- 2.342 ms [1.769 s, 1.783 s]
          batches_per_second: 0.56 +/- 0.00 [0.56, 0.57]
        metrics:
          batches_per_second_max: 0.5654474881067807
          batches_per_second_mean: 0.5640242993921452
          batches_per_second_min: 0.5608767979117746
          batches_per_second_std: 0.0007441541191971278
          seconds_per_batch_max: 1.7829227447509766
          seconds_per_batch_mean: 1.772976348400116
          seconds_per_batch_min: 1.7685108184814453
          seconds_per_batch_std: 0.00234192224859872
      total:
        human_readable:
          batch_latency: 1.869 s +/- 2.100 ms [1.864 s, 1.877 s]
          batches_per_second: 0.54 +/- 0.00 [0.53, 0.54]
        metrics:
          batches_per_second_max: 0.536551350600404
          batches_per_second_mean: 0.5350096897318888
          batches_per_second_min: 0.532693030393434
          batches_per_second_std: 0.0006006776646385452
          seconds_per_batch_max: 1.877253770828247
          seconds_per_batch_mean: 1.8691273832321167
          seconds_per_batch_min: 1.8637545108795166
          seconds_per_batch_std: 0.0020999214416366364

==== Benchmarking CoX3DLearner (s) ====
== Benchmarking learner.infer ==
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'

Machine info:
  cpu:
    architecture: aarch64
    cores:
      physical: 8
      total: 8
    frequency: 2.27 GHz
    model: ARMv8 Processor rev 0 (v8l)
  gpus: null
  memory:
    available: 16.34 GB
    total: 31.17 GB
    used: 16.99 GB
  system:
    node: xavier
    release: 4.9.140-tegra
    system: Linux

Model device: cuda:0
Allocated GPU memory prior to inference: 7276731392 (6.78 GB)
Allocated GPU memory after to inference: 14766537728 (13.75 GB)
Max allocated GPU memory during inference: 14816001536 (13.80 GB)
Warming up with batch_size=128:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=128:  10%|█         | 1/10 [00:00<00:01,  8.15it/s]Warming up with batch_size=128:  20%|██        | 2/10 [00:00<00:00,  8.95it/s]Warming up with batch_size=128:  30%|███       | 3/10 [00:00<00:00,  8.90it/s]Warming up with batch_size=128:  40%|████      | 4/10 [00:00<00:00,  9.04it/s]Warming up with batch_size=128:  60%|██████    | 6/10 [00:00<00:00,  9.96it/s]Warming up with batch_size=128:  80%|████████  | 8/10 [00:00<00:00, 10.07it/s]Warming up with batch_size=128: 100%|██████████| 10/10 [00:01<00:00, 10.29it/s]Warming up with batch_size=128: 100%|██████████| 10/10 [00:01<00:00,  9.85it/s]
Measuring inference with batch_size=128:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=128:   2%|▏         | 2/100 [00:00<00:09, 10.87it/s]Measuring inference with batch_size=128:   4%|▍         | 4/100 [00:00<00:09, 10.42it/s]Measuring inference with batch_size=128:   6%|▌         | 6/100 [00:00<00:08, 10.57it/s]Measuring inference with batch_size=128:   8%|▊         | 8/100 [00:00<00:08, 10.75it/s]Measuring inference with batch_size=128:  10%|█         | 10/100 [00:00<00:08, 10.63it/s]Measuring inference with batch_size=128:  12%|█▏        | 12/100 [00:01<00:08, 10.67it/s]Measuring inference with batch_size=128:  14%|█▍        | 14/100 [00:01<00:08, 10.55it/s]Measuring inference with batch_size=128:  16%|█▌        | 16/100 [00:01<00:07, 10.57it/s]Measuring inference with batch_size=128:  18%|█▊        | 18/100 [00:01<00:07, 10.59it/s]Measuring inference with batch_size=128:  20%|██        | 20/100 [00:01<00:07, 10.60it/s]Measuring inference with batch_size=128:  22%|██▏       | 22/100 [00:02<00:07, 10.72it/s]Measuring inference with batch_size=128:  24%|██▍       | 24/100 [00:02<00:06, 10.92it/s]Measuring inference with batch_size=128:  26%|██▌       | 26/100 [00:02<00:06, 11.04it/s]Measuring inference with batch_size=128:  28%|██▊       | 28/100 [00:02<00:06, 11.13it/s]Measuring inference with batch_size=128:  30%|███       | 30/100 [00:02<00:06, 10.90it/s]Measuring inference with batch_size=128:  32%|███▏      | 32/100 [00:02<00:06, 10.99it/s]Measuring inference with batch_size=128:  34%|███▍      | 34/100 [00:03<00:06, 10.95it/s]Measuring inference with batch_size=128:  36%|███▌      | 36/100 [00:03<00:05, 10.88it/s]Measuring inference with batch_size=128:  38%|███▊      | 38/100 [00:03<00:05, 10.97it/s]Measuring inference with batch_size=128:  40%|████      | 40/100 [00:03<00:05, 11.07it/s]Measuring inference with batch_size=128:  42%|████▏     | 42/100 [00:03<00:05, 10.95it/s]Measuring inference with batch_size=128:  44%|████▍     | 44/100 [00:04<00:05, 10.81it/s]Measuring inference with batch_size=128:  46%|████▌     | 46/100 [00:04<00:04, 10.86it/s]Measuring inference with batch_size=128:  48%|████▊     | 48/100 [00:04<00:04, 10.90it/s]Measuring inference with batch_size=128:  50%|█████     | 50/100 [00:04<00:04, 10.88it/s]Measuring inference with batch_size=128:  52%|█████▏    | 52/100 [00:04<00:04, 10.93it/s]Measuring inference with batch_size=128:  54%|█████▍    | 54/100 [00:04<00:04, 10.91it/s]Measuring inference with batch_size=128:  56%|█████▌    | 56/100 [00:05<00:04, 10.68it/s]Measuring inference with batch_size=128:  58%|█████▊    | 58/100 [00:05<00:03, 10.61it/s]Measuring inference with batch_size=128:  60%|██████    | 60/100 [00:05<00:03, 10.50it/s]Measuring inference with batch_size=128:  62%|██████▏   | 62/100 [00:05<00:03, 10.44it/s]Measuring inference with batch_size=128:  64%|██████▍   | 64/100 [00:05<00:03, 10.29it/s]Measuring inference with batch_size=128:  66%|██████▌   | 66/100 [00:06<00:03, 10.40it/s]Measuring inference with batch_size=128:  68%|██████▊   | 68/100 [00:06<00:03, 10.44it/s]Measuring inference with batch_size=128:  70%|███████   | 70/100 [00:06<00:02, 10.33it/s]Measuring inference with batch_size=128:  72%|███████▏  | 72/100 [00:06<00:02, 10.25it/s]Measuring inference with batch_size=128:  74%|███████▍  | 74/100 [00:06<00:02, 10.21it/s]Measuring inference with batch_size=128:  76%|███████▌  | 76/100 [00:07<00:02, 10.30it/s]Measuring inference with batch_size=128:  78%|███████▊  | 78/100 [00:07<00:02, 10.54it/s]Measuring inference with batch_size=128:  80%|████████  | 80/100 [00:07<00:01, 10.60it/s]Measuring inference with batch_size=128:  82%|████████▏ | 82/100 [00:07<00:01, 10.68it/s]Measuring inference with batch_size=128:  84%|████████▍ | 84/100 [00:07<00:01, 10.65it/s]Measuring inference with batch_size=128:  86%|████████▌ | 86/100 [00:08<00:01, 10.65it/s]Measuring inference with batch_size=128:  88%|████████▊ | 88/100 [00:08<00:01, 10.59it/s]Measuring inference with batch_size=128:  90%|█████████ | 90/100 [00:08<00:00, 10.56it/s]Measuring inference with batch_size=128:  92%|█████████▏| 92/100 [00:08<00:00, 10.59it/s]Measuring inference with batch_size=128:  94%|█████████▍| 94/100 [00:08<00:00, 10.41it/s]Measuring inference with batch_size=128:  96%|█████████▌| 96/100 [00:09<00:00, 10.52it/s]Measuring inference with batch_size=128:  98%|█████████▊| 98/100 [00:09<00:00, 10.57it/s]Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:09<00:00, 10.73it/s]Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:09<00:00, 10.67it/s]
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "8.042 \xB5s +/- 7.063 \xB5s [3.338 \xB5s, 61.750 \xB5s]"
      batches_per_second: 170.05 K +/- 71.31 K [16.19 K, 299.59 K]
    metrics:
      batches_per_second_max: 299593.14285714284
      batches_per_second_mean: 170050.19286137927
      batches_per_second_min: 16194.223938223939
      batches_per_second_std: 71306.54304233234
      seconds_per_batch_max: 6.175041198730469e-05
      seconds_per_batch_mean: 8.041858673095702e-06
      seconds_per_batch_min: 3.337860107421875e-06
      seconds_per_batch_std: 7.062653330924686e-06
  gpu_to_cpu:
    human_readable:
      batch_latency: "212.686 \xB5s +/- 21.563 \xB5s [179.052 \xB5s, 289.440 \xB5\
        s]"
      batches_per_second: 4.75 K +/- 441.19 [3.45 K, 5.58 K]
    metrics:
      batches_per_second_max: 5584.9587217043945
      batches_per_second_mean: 4746.137047268147
      batches_per_second_min: 3454.9456342668864
      batches_per_second_std: 441.186980862726
      seconds_per_batch_max: 0.0002894401550292969
      seconds_per_batch_mean: 0.00021268606185913087
      seconds_per_batch_min: 0.00017905235290527344
      seconds_per_batch_std: 2.156293415610299e-05
  on_device_inference:
    human_readable:
      batch_latency: 93.073 ms +/- 4.201 ms [85.857 ms, 106.580 ms]
      batches_per_second: 10.77 +/- 0.47 [9.38, 11.65]
    metrics:
      batches_per_second_max: 11.647220864619896
      batches_per_second_mean: 10.765442342163194
      batches_per_second_min: 9.382642697996546
      batches_per_second_std: 0.4708478218637512
      seconds_per_batch_max: 0.10657978057861328
      seconds_per_batch_mean: 0.09307321786880493
      seconds_per_batch_min: 0.08585739135742188
      seconds_per_batch_std: 0.004201194914245017
  total:
    human_readable:
      batch_latency: 93.294 ms +/- 4.206 ms [86.054 ms, 106.787 ms]
      batches_per_second: 10.74 +/- 0.47 [9.36, 11.62]
    metrics:
      batches_per_second_max: 11.620630804353127
      batches_per_second_mean: 10.739923039633297
      batches_per_second_min: 9.364438699075903
      batches_per_second_std: 0.4692338801355308
      seconds_per_batch_max: 0.10678696632385254
      seconds_per_batch_mean: 0.09329394578933715
      seconds_per_batch_min: 0.08605384826660156
      seconds_per_batch_std: 0.004205867432342956

Energy results (batch_size=1):
  joules: 56.20074994572005
  kWh: 1.561131942936668e-05

Warming up with batch_size=128:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=128:  10%|█         | 1/10 [00:02<00:18,  2.02s/it]Warming up with batch_size=128:  20%|██        | 2/10 [00:03<00:15,  1.97s/it]Warming up with batch_size=128:  30%|███       | 3/10 [00:05<00:13,  1.97s/it]Warming up with batch_size=128:  40%|████      | 4/10 [00:07<00:11,  1.96s/it]Warming up with batch_size=128:  50%|█████     | 5/10 [00:09<00:09,  1.96s/it]Warming up with batch_size=128:  60%|██████    | 6/10 [00:11<00:07,  1.95s/it]Warming up with batch_size=128:  70%|███████   | 7/10 [00:13<00:05,  1.95s/it]Warming up with batch_size=128:  80%|████████  | 8/10 [00:15<00:03,  1.98s/it]Warming up with batch_size=128:  90%|█████████ | 9/10 [00:17<00:01,  1.96s/it]Warming up with batch_size=128: 100%|██████████| 10/10 [00:19<00:00,  1.96s/it]Warming up with batch_size=128: 100%|██████████| 10/10 [00:19<00:00,  1.96s/it]
Measuring inference with batch_size=128:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=128:   1%|          | 1/100 [00:01<03:13,  1.96s/it]Measuring inference with batch_size=128:   2%|▏         | 2/100 [00:03<03:10,  1.95s/it]Measuring inference with batch_size=128:   3%|▎         | 3/100 [00:05<03:08,  1.94s/it]Measuring inference with batch_size=128:   4%|▍         | 4/100 [00:07<03:07,  1.95s/it]Measuring inference with batch_size=128:   5%|▌         | 5/100 [00:09<03:05,  1.95s/it]Measuring inference with batch_size=128:   6%|▌         | 6/100 [00:11<03:03,  1.95s/it]Measuring inference with batch_size=128:   7%|▋         | 7/100 [00:13<03:01,  1.95s/it]Measuring inference with batch_size=128:   8%|▊         | 8/100 [00:15<02:59,  1.95s/it]Measuring inference with batch_size=128:   9%|▉         | 9/100 [00:17<02:56,  1.94s/it]Measuring inference with batch_size=128:  10%|█         | 10/100 [00:19<02:56,  1.96s/it]Measuring inference with batch_size=128:  11%|█         | 11/100 [00:21<02:54,  1.96s/it]Measuring inference with batch_size=128:  12%|█▏        | 12/100 [00:23<02:52,  1.96s/it]Measuring inference with batch_size=128:  13%|█▎        | 13/100 [00:25<02:50,  1.96s/it]Measuring inference with batch_size=128:  14%|█▍        | 14/100 [00:27<02:48,  1.96s/it]Measuring inference with batch_size=128:  15%|█▌        | 15/100 [00:29<02:46,  1.95s/it]Measuring inference with batch_size=128:  16%|█▌        | 16/100 [00:31<02:43,  1.95s/it]Measuring inference with batch_size=128:  17%|█▋        | 17/100 [00:33<02:43,  1.97s/it]Measuring inference with batch_size=128:  18%|█▊        | 18/100 [00:35<02:40,  1.95s/it]Measuring inference with batch_size=128:  19%|█▉        | 19/100 [00:37<02:38,  1.96s/it]Measuring inference with batch_size=128:  20%|██        | 20/100 [00:39<02:36,  1.96s/it]Measuring inference with batch_size=128:  21%|██        | 21/100 [00:41<02:35,  1.97s/it]Measuring inference with batch_size=128:  22%|██▏       | 22/100 [00:43<02:32,  1.96s/it]Measuring inference with batch_size=128:  23%|██▎       | 23/100 [00:45<02:31,  1.96s/it]Measuring inference with batch_size=128:  24%|██▍       | 24/100 [00:46<02:28,  1.95s/it]Measuring inference with batch_size=128:  25%|██▌       | 25/100 [00:48<02:26,  1.95s/it]Measuring inference with batch_size=128:  26%|██▌       | 26/100 [00:50<02:24,  1.95s/it]Measuring inference with batch_size=128:  27%|██▋       | 27/100 [00:52<02:22,  1.95s/it]Measuring inference with batch_size=128:  28%|██▊       | 28/100 [00:54<02:21,  1.96s/it]Measuring inference with batch_size=128:  29%|██▉       | 29/100 [00:56<02:20,  1.98s/it]Measuring inference with batch_size=128:  30%|███       | 30/100 [00:58<02:17,  1.97s/it]Measuring inference with batch_size=128:  31%|███       | 31/100 [01:00<02:15,  1.97s/it]Measuring inference with batch_size=128:  32%|███▏      | 32/100 [01:02<02:13,  1.96s/it]Measuring inference with batch_size=128:  33%|███▎      | 33/100 [01:04<02:10,  1.95s/it]Measuring inference with batch_size=128:  34%|███▍      | 34/100 [01:06<02:09,  1.96s/it]Measuring inference with batch_size=128:  35%|███▌      | 35/100 [01:08<02:07,  1.96s/it]Measuring inference with batch_size=128:  36%|███▌      | 36/100 [01:10<02:05,  1.96s/it]Measuring inference with batch_size=128:  37%|███▋      | 37/100 [01:12<02:02,  1.95s/it]Measuring inference with batch_size=128:  38%|███▊      | 38/100 [01:14<02:00,  1.95s/it]Measuring inference with batch_size=128:  39%|███▉      | 39/100 [01:16<01:59,  1.95s/it]Measuring inference with batch_size=128:  40%|████      | 40/100 [01:18<01:56,  1.95s/it]Measuring inference with batch_size=128:  41%|████      | 41/100 [01:20<01:55,  1.96s/it]Measuring inference with batch_size=128:  42%|████▏     | 42/100 [01:22<01:53,  1.95s/it]Measuring inference with batch_size=128:  43%|████▎     | 43/100 [01:24<01:51,  1.96s/it]Measuring inference with batch_size=128:  44%|████▍     | 44/100 [01:26<01:49,  1.95s/it]Measuring inference with batch_size=128:  45%|████▌     | 45/100 [01:27<01:47,  1.95s/it]Measuring inference with batch_size=128:  46%|████▌     | 46/100 [01:29<01:45,  1.95s/it]Measuring inference with batch_size=128:  47%|████▋     | 47/100 [01:31<01:43,  1.94s/it]Measuring inference with batch_size=128:  48%|████▊     | 48/100 [01:33<01:41,  1.94s/it]Measuring inference with batch_size=128:  49%|████▉     | 49/100 [01:35<01:39,  1.96s/it]Measuring inference with batch_size=128:  50%|█████     | 50/100 [01:37<01:38,  1.97s/it]Measuring inference with batch_size=128:  51%|█████     | 51/100 [01:39<01:37,  1.99s/it]Measuring inference with batch_size=128:  52%|█████▏    | 52/100 [01:41<01:34,  1.97s/it]Measuring inference with batch_size=128:  53%|█████▎    | 53/100 [01:43<01:32,  1.96s/it]Measuring inference with batch_size=128:  54%|█████▍    | 54/100 [01:45<01:29,  1.96s/it]Measuring inference with batch_size=128:  55%|█████▌    | 55/100 [01:47<01:27,  1.95s/it]Measuring inference with batch_size=128:  56%|█████▌    | 56/100 [01:49<01:25,  1.95s/it]Measuring inference with batch_size=128:  57%|█████▋    | 57/100 [01:51<01:23,  1.95s/it]Measuring inference with batch_size=128:  58%|█████▊    | 58/100 [01:53<01:21,  1.95s/it]Measuring inference with batch_size=128:  59%|█████▉    | 59/100 [01:55<01:19,  1.94s/it]Measuring inference with batch_size=128:  60%|██████    | 60/100 [01:57<01:17,  1.94s/it]Measuring inference with batch_size=128:  61%|██████    | 61/100 [01:59<01:16,  1.96s/it]Measuring inference with batch_size=128:  62%|██████▏   | 62/100 [02:01<01:14,  1.97s/it]Measuring inference with batch_size=128:  63%|██████▎   | 63/100 [02:03<01:13,  1.99s/it]Measuring inference with batch_size=128:  64%|██████▍   | 64/100 [02:05<01:11,  1.98s/it]Measuring inference with batch_size=128:  65%|██████▌   | 65/100 [02:07<01:09,  1.97s/it]Measuring inference with batch_size=128:  66%|██████▌   | 66/100 [02:09<01:07,  1.98s/it]Measuring inference with batch_size=128:  67%|██████▋   | 67/100 [02:11<01:04,  1.97s/it]Measuring inference with batch_size=128:  68%|██████▊   | 68/100 [02:13<01:02,  1.95s/it]Measuring inference with batch_size=128:  69%|██████▉   | 69/100 [02:15<01:00,  1.95s/it]Measuring inference with batch_size=128:  70%|███████   | 70/100 [02:17<00:58,  1.95s/it]Measuring inference with batch_size=128:  71%|███████   | 71/100 [02:18<00:56,  1.95s/it]Measuring inference with batch_size=128:  72%|███████▏  | 72/100 [02:20<00:54,  1.96s/it]Measuring inference with batch_size=128:  73%|███████▎  | 73/100 [02:22<00:52,  1.95s/it]Measuring inference with batch_size=128:  74%|███████▍  | 74/100 [02:24<00:50,  1.95s/it]Measuring inference with batch_size=128:  75%|███████▌  | 75/100 [02:26<00:49,  1.96s/it]Measuring inference with batch_size=128:  76%|███████▌  | 76/100 [02:28<00:47,  1.97s/it]Measuring inference with batch_size=128:  77%|███████▋  | 77/100 [02:30<00:45,  1.96s/it]Measuring inference with batch_size=128:  78%|███████▊  | 78/100 [02:32<00:42,  1.95s/it]Measuring inference with batch_size=128:  79%|███████▉  | 79/100 [02:34<00:41,  1.95s/it]Measuring inference with batch_size=128:  80%|████████  | 80/100 [02:36<00:38,  1.95s/it]Measuring inference with batch_size=128:  81%|████████  | 81/100 [02:38<00:38,  2.01s/it]Measuring inference with batch_size=128:  82%|████████▏ | 82/100 [02:40<00:36,  2.04s/it]Measuring inference with batch_size=128:  83%|████████▎ | 83/100 [02:43<00:36,  2.15s/it]Measuring inference with batch_size=128:  84%|████████▍ | 84/100 [02:45<00:35,  2.24s/it]Measuring inference with batch_size=128:  85%|████████▌ | 85/100 [02:48<00:33,  2.27s/it]Measuring inference with batch_size=128:  86%|████████▌ | 86/100 [02:50<00:30,  2.20s/it]Measuring inference with batch_size=128:  87%|████████▋ | 87/100 [02:52<00:28,  2.21s/it]Measuring inference with batch_size=128:  88%|████████▊ | 88/100 [02:54<00:25,  2.12s/it]Measuring inference with batch_size=128:  89%|████████▉ | 89/100 [02:56<00:22,  2.07s/it]Measuring inference with batch_size=128:  90%|█████████ | 90/100 [02:58<00:20,  2.04s/it]Measuring inference with batch_size=128:  91%|█████████ | 91/100 [03:00<00:18,  2.01s/it]Measuring inference with batch_size=128:  92%|█████████▏| 92/100 [03:02<00:16,  2.00s/it]Measuring inference with batch_size=128:  93%|█████████▎| 93/100 [03:03<00:13,  1.98s/it]Measuring inference with batch_size=128:  94%|█████████▍| 94/100 [03:05<00:11,  1.97s/it]Measuring inference with batch_size=128:  95%|█████████▌| 95/100 [03:07<00:09,  1.96s/it]Measuring inference with batch_size=128:  96%|█████████▌| 96/100 [03:09<00:07,  1.95s/it]Measuring inference with batch_size=128:  97%|█████████▋| 97/100 [03:11<00:05,  1.96s/it]Measuring inference with batch_size=128:  98%|█████████▊| 98/100 [03:13<00:03,  1.95s/it]Measuring inference with batch_size=128:  99%|█████████▉| 99/100 [03:15<00:01,  1.96s/it]Measuring inference with batch_size=128: 100%|██████████| 100/100 [03:17<00:00,  1.96s/it]Measuring inference with batch_size=128: 100%|██████████| 100/100 [03:17<00:00,  1.98s/it]Timing results (batch_size=128):
  cpu_to_gpu:
    human_readable:
      batch_latency: "8.705 \xB5s +/- 2.986 \xB5s [5.722 \xB5s, 28.849 \xB5s]"
      batches_per_second: 123.25 K +/- 27.35 K [34.66 K, 174.76 K]
    metrics:
      batches_per_second_max: 174762.66666666666
      batches_per_second_mean: 123246.04719556282
      batches_per_second_min: 34663.669421487604
      batches_per_second_std: 27351.51685342682
      seconds_per_batch_max: 2.8848648071289062e-05
      seconds_per_batch_mean: 8.704662322998046e-06
      seconds_per_batch_min: 5.7220458984375e-06
      seconds_per_batch_std: 2.985757474343594e-06
  gpu_to_cpu:
    human_readable:
      batch_latency: "8.406 ms +/- 883.041 \xB5s [7.943 ms, 13.815 ms]"
      batches_per_second: 119.90 +/- 9.04 [72.39, 125.90]
    metrics:
      batches_per_second_max: 125.89836410025514
      batches_per_second_mean: 119.90412833193572
      batches_per_second_min: 72.38547563164434
      batches_per_second_std: 9.03734472587961
      seconds_per_batch_max: 0.013814926147460938
      seconds_per_batch_mean: 0.008405842781066895
      seconds_per_batch_min: 0.007942914962768555
      seconds_per_batch_std: 0.0008830410112722455
  on_device_inference:
    human_readable:
      batch_latency: 1.967 s +/- 86.658 ms [1.903 s, 2.443 s]
      batches_per_second: 0.51 +/- 0.02 [0.41, 0.53]
    metrics:
      batches_per_second_max: 0.5253982321392894
      batches_per_second_mean: 0.5092572239814982
      batches_per_second_min: 0.4094014562859457
      batches_per_second_std: 0.019334846812777168
      seconds_per_batch_max: 2.4425902366638184
      seconds_per_batch_mean: 1.9669264936447144
      seconds_per_batch_min: 1.903318166732788
      seconds_per_batch_std: 0.08665774957139104
  total:
    human_readable:
      batch_latency: 1.975 s +/- 87.031 ms [1.912 s, 2.452 s]
      batches_per_second: 0.51 +/- 0.02 [0.41, 0.52]
    metrics:
      batches_per_second_max: 0.5231126965490285
      batches_per_second_mean: 0.5070880113355863
      batches_per_second_min: 0.40786874852007565
      batches_per_second_std: 0.019254548954601765
      seconds_per_batch_max: 2.4517691135406494
      seconds_per_batch_mean: 1.9753410410881043
      seconds_per_batch_min: 1.9116339683532715
      seconds_per_batch_std: 0.08703119848813509

Energy results (batch_size=128):
  joules: 49.74806922686895
  kWh: 1.3818908118574708e-05

learner.infer:
  device: cuda
  energy:
    batch_size_1:
      joules: 56.20074994572005
      kWh: 1.561131942936668e-05
    batch_size_128:
      joules: 49.74806922686895
      kWh: 1.3818908118574708e-05
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 16.34 GB
      total: 31.17 GB
      used: 16.99 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  max_inference_memory: 14816001536
  post_inference_memory: 14766537728
  pre_inference_memory: 7276731392
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "8.042 \xB5s +/- 7.063 \xB5s [3.338 \xB5s, 61.750 \xB5s]"
          batches_per_second: 170.05 K +/- 71.31 K [16.19 K, 299.59 K]
        metrics:
          batches_per_second_max: 299593.14285714284
          batches_per_second_mean: 170050.19286137927
          batches_per_second_min: 16194.223938223939
          batches_per_second_std: 71306.54304233234
          seconds_per_batch_max: 6.175041198730469e-05
          seconds_per_batch_mean: 8.041858673095702e-06
          seconds_per_batch_min: 3.337860107421875e-06
          seconds_per_batch_std: 7.062653330924686e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: "212.686 \xB5s +/- 21.563 \xB5s [179.052 \xB5s, 289.440 \xB5\
            s]"
          batches_per_second: 4.75 K +/- 441.19 [3.45 K, 5.58 K]
        metrics:
          batches_per_second_max: 5584.9587217043945
          batches_per_second_mean: 4746.137047268147
          batches_per_second_min: 3454.9456342668864
          batches_per_second_std: 441.186980862726
          seconds_per_batch_max: 0.0002894401550292969
          seconds_per_batch_mean: 0.00021268606185913087
          seconds_per_batch_min: 0.00017905235290527344
          seconds_per_batch_std: 2.156293415610299e-05
      on_device_inference:
        human_readable:
          batch_latency: 93.073 ms +/- 4.201 ms [85.857 ms, 106.580 ms]
          batches_per_second: 10.77 +/- 0.47 [9.38, 11.65]
        metrics:
          batches_per_second_max: 11.647220864619896
          batches_per_second_mean: 10.765442342163194
          batches_per_second_min: 9.382642697996546
          batches_per_second_std: 0.4708478218637512
          seconds_per_batch_max: 0.10657978057861328
          seconds_per_batch_mean: 0.09307321786880493
          seconds_per_batch_min: 0.08585739135742188
          seconds_per_batch_std: 0.004201194914245017
      total:
        human_readable:
          batch_latency: 93.294 ms +/- 4.206 ms [86.054 ms, 106.787 ms]
          batches_per_second: 10.74 +/- 0.47 [9.36, 11.62]
        metrics:
          batches_per_second_max: 11.620630804353127
          batches_per_second_mean: 10.739923039633297
          batches_per_second_min: 9.364438699075903
          batches_per_second_std: 0.4692338801355308
          seconds_per_batch_max: 0.10678696632385254
          seconds_per_batch_mean: 0.09329394578933715
          seconds_per_batch_min: 0.08605384826660156
          seconds_per_batch_std: 0.004205867432342956
    batch_size_128:
      cpu_to_gpu:
        human_readable:
          batch_latency: "8.705 \xB5s +/- 2.986 \xB5s [5.722 \xB5s, 28.849 \xB5s]"
          batches_per_second: 123.25 K +/- 27.35 K [34.66 K, 174.76 K]
        metrics:
          batches_per_second_max: 174762.66666666666
          batches_per_second_mean: 123246.04719556282
          batches_per_second_min: 34663.669421487604
          batches_per_second_std: 27351.51685342682
          seconds_per_batch_max: 2.8848648071289062e-05
          seconds_per_batch_mean: 8.704662322998046e-06
          seconds_per_batch_min: 5.7220458984375e-06
          seconds_per_batch_std: 2.985757474343594e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: "8.406 ms +/- 883.041 \xB5s [7.943 ms, 13.815 ms]"
          batches_per_second: 119.90 +/- 9.04 [72.39, 125.90]
        metrics:
          batches_per_second_max: 125.89836410025514
          batches_per_second_mean: 119.90412833193572
          batches_per_second_min: 72.38547563164434
          batches_per_second_std: 9.03734472587961
          seconds_per_batch_max: 0.013814926147460938
          seconds_per_batch_mean: 0.008405842781066895
          seconds_per_batch_min: 0.007942914962768555
          seconds_per_batch_std: 0.0008830410112722455
      on_device_inference:
        human_readable:
          batch_latency: 1.967 s +/- 86.658 ms [1.903 s, 2.443 s]
          batches_per_second: 0.51 +/- 0.02 [0.41, 0.53]
        metrics:
          batches_per_second_max: 0.5253982321392894
          batches_per_second_mean: 0.5092572239814982
          batches_per_second_min: 0.4094014562859457
          batches_per_second_std: 0.019334846812777168
          seconds_per_batch_max: 2.4425902366638184
          seconds_per_batch_mean: 1.9669264936447144
          seconds_per_batch_min: 1.903318166732788
          seconds_per_batch_std: 0.08665774957139104
      total:
        human_readable:
          batch_latency: 1.975 s +/- 87.031 ms [1.912 s, 2.452 s]
          batches_per_second: 0.51 +/- 0.02 [0.41, 0.52]
        metrics:
          batches_per_second_max: 0.5231126965490285
          batches_per_second_mean: 0.5070880113355863
          batches_per_second_min: 0.40786874852007565
          batches_per_second_std: 0.019254548954601765
          seconds_per_batch_max: 2.4517691135406494
          seconds_per_batch_mean: 1.9753410410881043
          seconds_per_batch_min: 1.9116339683532715
          seconds_per_batch_std: 0.08703119848813509

== Benchmarking model directly ==
Machine info:
  cpu:
    architecture: aarch64
    cores:
      physical: 8
      total: 8
    frequency: 2.27 GHz
    model: ARMv8 Processor rev 0 (v8l)
  gpus: null
  memory:
    available: 9.44 GB
    total: 31.17 GB
    used: 23.52 GB
  system:
    node: xavier
    release: 4.9.140-tegra
    system: Linux


Model device: cuda:0
Model parameters: 3794322 (3.79 M)
Model FLOPs: 138232160 (138.23 M)
Allocated GPU memory prior to inference: 7335638016 (6.83 GB)
Allocated GPU memory after to inference: 14778309632 (13.76 GB)
Max allocated GPU memory during inference: 14827641856 (13.81 GB)
Warming up with batch_size=128:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=128:  10%|█         | 1/10 [00:00<00:00,  9.16it/s]Warming up with batch_size=128:  30%|███       | 3/10 [00:00<00:00, 10.75it/s]Warming up with batch_size=128:  50%|█████     | 5/10 [00:00<00:00, 10.97it/s]Warming up with batch_size=128:  70%|███████   | 7/10 [00:00<00:00, 11.50it/s]Warming up with batch_size=128:  90%|█████████ | 9/10 [00:00<00:00, 11.78it/s]Warming up with batch_size=128: 100%|██████████| 10/10 [00:00<00:00, 11.23it/s]
Measuring inference with batch_size=128:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=128:   2%|▏         | 2/100 [00:00<00:09, 10.87it/s]Measuring inference with batch_size=128:   4%|▍         | 4/100 [00:00<00:08, 11.42it/s]Measuring inference with batch_size=128:   6%|▌         | 6/100 [00:00<00:08, 11.25it/s]Measuring inference with batch_size=128:   8%|▊         | 8/100 [00:00<00:08, 11.36it/s]Measuring inference with batch_size=128:  10%|█         | 10/100 [00:00<00:07, 11.36it/s]Measuring inference with batch_size=128:  12%|█▏        | 12/100 [00:01<00:07, 11.40it/s]Measuring inference with batch_size=128:  14%|█▍        | 14/100 [00:01<00:07, 11.23it/s]Measuring inference with batch_size=128:  16%|█▌        | 16/100 [00:01<00:07, 11.22it/s]Measuring inference with batch_size=128:  18%|█▊        | 18/100 [00:01<00:07, 11.40it/s]Measuring inference with batch_size=128:  20%|██        | 20/100 [00:01<00:06, 11.57it/s]Measuring inference with batch_size=128:  22%|██▏       | 22/100 [00:01<00:06, 11.72it/s]Measuring inference with batch_size=128:  24%|██▍       | 24/100 [00:02<00:06, 11.82it/s]Measuring inference with batch_size=128:  26%|██▌       | 26/100 [00:02<00:06, 11.95it/s]Measuring inference with batch_size=128:  28%|██▊       | 28/100 [00:02<00:06, 11.99it/s]Measuring inference with batch_size=128:  30%|███       | 30/100 [00:02<00:05, 12.06it/s]Measuring inference with batch_size=128:  32%|███▏      | 32/100 [00:02<00:05, 12.14it/s]Measuring inference with batch_size=128:  34%|███▍      | 34/100 [00:02<00:05, 12.18it/s]Measuring inference with batch_size=128:  36%|███▌      | 36/100 [00:03<00:05, 12.23it/s]Measuring inference with batch_size=128:  38%|███▊      | 38/100 [00:03<00:05, 12.30it/s]Measuring inference with batch_size=128:  40%|████      | 40/100 [00:03<00:04, 12.33it/s]Measuring inference with batch_size=128:  42%|████▏     | 42/100 [00:03<00:04, 12.39it/s]Measuring inference with batch_size=128:  44%|████▍     | 44/100 [00:03<00:04, 12.41it/s]Measuring inference with batch_size=128:  46%|████▌     | 46/100 [00:03<00:04, 12.38it/s]Measuring inference with batch_size=128:  48%|████▊     | 48/100 [00:04<00:04, 12.15it/s]Measuring inference with batch_size=128:  50%|█████     | 50/100 [00:04<00:04, 11.77it/s]Measuring inference with batch_size=128:  52%|█████▏    | 52/100 [00:04<00:04, 11.79it/s]Measuring inference with batch_size=128:  54%|█████▍    | 54/100 [00:04<00:03, 12.00it/s]Measuring inference with batch_size=128:  56%|█████▌    | 56/100 [00:04<00:03, 12.20it/s]Measuring inference with batch_size=128:  58%|█████▊    | 58/100 [00:04<00:03, 12.35it/s]Measuring inference with batch_size=128:  60%|██████    | 60/100 [00:05<00:03, 12.45it/s]Measuring inference with batch_size=128:  62%|██████▏   | 62/100 [00:05<00:03, 12.22it/s]Measuring inference with batch_size=128:  64%|██████▍   | 64/100 [00:05<00:03, 11.76it/s]Measuring inference with batch_size=128:  66%|██████▌   | 66/100 [00:05<00:02, 11.73it/s]Measuring inference with batch_size=128:  68%|██████▊   | 68/100 [00:05<00:02, 11.88it/s]Measuring inference with batch_size=128:  70%|███████   | 70/100 [00:05<00:02, 12.04it/s]Measuring inference with batch_size=128:  72%|███████▏  | 72/100 [00:06<00:02, 11.90it/s]Measuring inference with batch_size=128:  74%|███████▍  | 74/100 [00:06<00:02, 11.86it/s]Measuring inference with batch_size=128:  76%|███████▌  | 76/100 [00:06<00:02, 11.58it/s]Measuring inference with batch_size=128:  78%|███████▊  | 78/100 [00:06<00:01, 11.71it/s]Measuring inference with batch_size=128:  80%|████████  | 80/100 [00:06<00:01, 11.63it/s]Measuring inference with batch_size=128:  82%|████████▏ | 82/100 [00:06<00:01, 11.68it/s]Measuring inference with batch_size=128:  84%|████████▍ | 84/100 [00:07<00:01, 11.68it/s]Measuring inference with batch_size=128:  86%|████████▌ | 86/100 [00:07<00:01, 11.57it/s]Measuring inference with batch_size=128:  88%|████████▊ | 88/100 [00:07<00:01, 11.43it/s]Measuring inference with batch_size=128:  90%|█████████ | 90/100 [00:07<00:00, 11.64it/s]Measuring inference with batch_size=128:  92%|█████████▏| 92/100 [00:07<00:00, 11.92it/s]Measuring inference with batch_size=128:  94%|█████████▍| 94/100 [00:07<00:00, 12.12it/s]Measuring inference with batch_size=128:  96%|█████████▌| 96/100 [00:08<00:00, 12.27it/s]Measuring inference with batch_size=128:  98%|█████████▊| 98/100 [00:08<00:00, 12.08it/s]Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:08<00:00, 11.77it/s]Measuring inference with batch_size=128: 100%|██████████| 100/100 [00:08<00:00, 11.86it/s]
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "190.709 \xB5s +/- 48.017 \xB5s [123.978 \xB5s, 325.918 \xB5\
        s]"
      batches_per_second: 5.54 K +/- 1.21 K [3.07 K, 8.07 K]
    metrics:
      batches_per_second_max: 8065.969230769231
      batches_per_second_mean: 5542.371765664944
      batches_per_second_min: 3068.2545720555963
      batches_per_second_std: 1211.1923571751852
      seconds_per_batch_max: 0.00032591819763183594
      seconds_per_batch_mean: 0.00019070863723754882
      seconds_per_batch_min: 0.0001239776611328125
      seconds_per_batch_std: 4.80166245634181e-05
  gpu_to_cpu:
    human_readable:
      batch_latency: "157.993 \xB5s +/- 22.834 \xB5s [125.408 \xB5s, 222.921 \xB5\
        s]"
      batches_per_second: 6.46 K +/- 911.21 [4.49 K, 7.97 K]
    metrics:
      batches_per_second_max: 7973.961977186312
      batches_per_second_mean: 6459.613116833283
      batches_per_second_min: 4485.886631016043
      batches_per_second_std: 911.208544022827
      seconds_per_batch_max: 0.00022292137145996094
      seconds_per_batch_mean: 0.0001579928398132324
      seconds_per_batch_min: 0.00012540817260742188
      seconds_per_batch_std: 2.2833506896389548e-05
  on_device_inference:
    human_readable:
      batch_latency: 83.514 ms +/- 4.298 ms [77.589 ms, 94.632 ms]
      batches_per_second: 12.00 +/- 0.60 [10.57, 12.89]
    metrics:
      batches_per_second_max: 12.888458690167809
      batches_per_second_mean: 12.004749677743964
      batches_per_second_min: 10.567286616244324
      batches_per_second_std: 0.5984903017020714
      seconds_per_batch_max: 0.09463167190551758
      seconds_per_batch_mean: 0.08351434946060181
      seconds_per_batch_min: 0.07758879661560059
      seconds_per_batch_std: 0.004298378536264893
  total:
    human_readable:
      batch_latency: 83.863 ms +/- 4.333 ms [77.915 ms, 95.006 ms]
      batches_per_second: 11.96 +/- 0.60 [10.53, 12.83]
    metrics:
      batches_per_second_max: 12.834506837536223
      batches_per_second_mean: 11.955076043589521
      batches_per_second_min: 10.525599393705175
      batches_per_second_std: 0.5983511019731392
      seconds_per_batch_max: 0.09500646591186523
      seconds_per_batch_mean: 0.08386305093765259
      seconds_per_batch_min: 0.07791495323181152
      seconds_per_batch_std: 0.004333378937381598

Energy results (batch_size=1):
  joules: 51.53619978780746
  kWh: 1.431561105216874e-05

Warming up with batch_size=128:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=128:  10%|█         | 1/10 [00:01<00:16,  1.85s/it]Warming up with batch_size=128:  20%|██        | 2/10 [00:03<00:14,  1.85s/it]Warming up with batch_size=128:  30%|███       | 3/10 [00:05<00:12,  1.86s/it]Warming up with batch_size=128:  40%|████      | 4/10 [00:07<00:11,  1.86s/it]Warming up with batch_size=128:  50%|█████     | 5/10 [00:09<00:09,  1.86s/it]Warming up with batch_size=128:  60%|██████    | 6/10 [00:11<00:07,  1.86s/it]Warming up with batch_size=128:  70%|███████   | 7/10 [00:12<00:05,  1.86s/it]Warming up with batch_size=128:  80%|████████  | 8/10 [00:14<00:03,  1.86s/it]Warming up with batch_size=128:  90%|█████████ | 9/10 [00:16<00:01,  1.86s/it]Warming up with batch_size=128: 100%|██████████| 10/10 [00:18<00:00,  1.86s/it]Warming up with batch_size=128: 100%|██████████| 10/10 [00:18<00:00,  1.86s/it]
Measuring inference with batch_size=128:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=128:   1%|          | 1/100 [00:01<03:03,  1.85s/it]Measuring inference with batch_size=128:   2%|▏         | 2/100 [00:03<03:01,  1.85s/it]Measuring inference with batch_size=128:   3%|▎         | 3/100 [00:05<02:59,  1.85s/it]Measuring inference with batch_size=128:   4%|▍         | 4/100 [00:07<02:58,  1.86s/it]Measuring inference with batch_size=128:   5%|▌         | 5/100 [00:09<02:56,  1.86s/it]Measuring inference with batch_size=128:   6%|▌         | 6/100 [00:11<02:54,  1.86s/it]Measuring inference with batch_size=128:   7%|▋         | 7/100 [00:12<02:52,  1.86s/it]Measuring inference with batch_size=128:   8%|▊         | 8/100 [00:14<02:50,  1.86s/it]Measuring inference with batch_size=128:   9%|▉         | 9/100 [00:16<02:48,  1.86s/it]Measuring inference with batch_size=128:  10%|█         | 10/100 [00:18<02:46,  1.86s/it]Measuring inference with batch_size=128:  11%|█         | 11/100 [00:20<02:45,  1.86s/it]Measuring inference with batch_size=128:  12%|█▏        | 12/100 [00:22<02:43,  1.86s/it]Measuring inference with batch_size=128:  13%|█▎        | 13/100 [00:24<02:41,  1.85s/it]Measuring inference with batch_size=128:  14%|█▍        | 14/100 [00:25<02:39,  1.86s/it]Measuring inference with batch_size=128:  15%|█▌        | 15/100 [00:27<02:37,  1.86s/it]Measuring inference with batch_size=128:  16%|█▌        | 16/100 [00:29<02:35,  1.86s/it]Measuring inference with batch_size=128:  17%|█▋        | 17/100 [00:31<02:33,  1.86s/it]Measuring inference with batch_size=128:  18%|█▊        | 18/100 [00:33<02:32,  1.86s/it]Measuring inference with batch_size=128:  19%|█▉        | 19/100 [00:35<02:30,  1.86s/it]Measuring inference with batch_size=128:  20%|██        | 20/100 [00:37<02:28,  1.86s/it]Measuring inference with batch_size=128:  21%|██        | 21/100 [00:38<02:26,  1.86s/it]Measuring inference with batch_size=128:  22%|██▏       | 22/100 [00:40<02:24,  1.86s/it]Measuring inference with batch_size=128:  23%|██▎       | 23/100 [00:42<02:22,  1.86s/it]Measuring inference with batch_size=128:  24%|██▍       | 24/100 [00:44<02:21,  1.86s/it]Measuring inference with batch_size=128:  25%|██▌       | 25/100 [00:46<02:19,  1.86s/it]Measuring inference with batch_size=128:  26%|██▌       | 26/100 [00:48<02:17,  1.86s/it]Measuring inference with batch_size=128:  27%|██▋       | 27/100 [00:50<02:15,  1.86s/it]Measuring inference with batch_size=128:  28%|██▊       | 28/100 [00:51<02:13,  1.86s/it]Measuring inference with batch_size=128:  29%|██▉       | 29/100 [00:53<02:11,  1.86s/it]Measuring inference with batch_size=128:  30%|███       | 30/100 [00:55<02:09,  1.86s/it]Measuring inference with batch_size=128:  31%|███       | 31/100 [00:57<02:08,  1.86s/it]Measuring inference with batch_size=128:  32%|███▏      | 32/100 [00:59<02:06,  1.86s/it]Measuring inference with batch_size=128:  33%|███▎      | 33/100 [01:01<02:04,  1.86s/it]Measuring inference with batch_size=128:  34%|███▍      | 34/100 [01:03<02:02,  1.86s/it]Measuring inference with batch_size=128:  35%|███▌      | 35/100 [01:04<02:00,  1.86s/it]Measuring inference with batch_size=128:  36%|███▌      | 36/100 [01:06<01:58,  1.86s/it]Measuring inference with batch_size=128:  37%|███▋      | 37/100 [01:08<01:56,  1.86s/it]Measuring inference with batch_size=128:  38%|███▊      | 38/100 [01:10<01:55,  1.85s/it]Measuring inference with batch_size=128:  39%|███▉      | 39/100 [01:12<01:53,  1.86s/it]Measuring inference with batch_size=128:  40%|████      | 40/100 [01:14<01:51,  1.86s/it]Measuring inference with batch_size=128:  41%|████      | 41/100 [01:16<01:49,  1.86s/it]Measuring inference with batch_size=128:  42%|████▏     | 42/100 [01:17<01:47,  1.86s/it]Measuring inference with batch_size=128:  43%|████▎     | 43/100 [01:19<01:45,  1.86s/it]Measuring inference with batch_size=128:  44%|████▍     | 44/100 [01:21<01:43,  1.86s/it]Measuring inference with batch_size=128:  45%|████▌     | 45/100 [01:23<01:42,  1.86s/it]Measuring inference with batch_size=128:  46%|████▌     | 46/100 [01:25<01:40,  1.86s/it]Measuring inference with batch_size=128:  47%|████▋     | 47/100 [01:27<01:38,  1.86s/it]Measuring inference with batch_size=128:  48%|████▊     | 48/100 [01:29<01:36,  1.86s/it]Measuring inference with batch_size=128:  49%|████▉     | 49/100 [01:30<01:34,  1.86s/it]Measuring inference with batch_size=128:  50%|█████     | 50/100 [01:32<01:32,  1.86s/it]Measuring inference with batch_size=128:  51%|█████     | 51/100 [01:34<01:30,  1.86s/it]Measuring inference with batch_size=128:  52%|█████▏    | 52/100 [01:36<01:30,  1.89s/it]Measuring inference with batch_size=128:  53%|█████▎    | 53/100 [01:38<01:31,  1.95s/it]Measuring inference with batch_size=128:  54%|█████▍    | 54/100 [01:40<01:29,  1.95s/it]Measuring inference with batch_size=128:  55%|█████▌    | 55/100 [01:42<01:32,  2.05s/it]Measuring inference with batch_size=128:  56%|█████▌    | 56/100 [01:44<01:28,  2.00s/it]Measuring inference with batch_size=128:  57%|█████▋    | 57/100 [01:46<01:25,  2.00s/it]Measuring inference with batch_size=128:  58%|█████▊    | 58/100 [01:48<01:22,  1.97s/it]Measuring inference with batch_size=128:  59%|█████▉    | 59/100 [01:50<01:19,  1.94s/it]Measuring inference with batch_size=128:  60%|██████    | 60/100 [01:52<01:17,  1.93s/it]Measuring inference with batch_size=128:  61%|██████    | 61/100 [01:54<01:14,  1.92s/it]Measuring inference with batch_size=128:  62%|██████▏   | 62/100 [01:56<01:12,  1.90s/it]Measuring inference with batch_size=128:  63%|██████▎   | 63/100 [01:58<01:10,  1.91s/it]Measuring inference with batch_size=128:  64%|██████▍   | 64/100 [02:00<01:08,  1.91s/it]Measuring inference with batch_size=128:  65%|██████▌   | 65/100 [02:01<01:06,  1.90s/it]Measuring inference with batch_size=128:  66%|██████▌   | 66/100 [02:03<01:04,  1.89s/it]Measuring inference with batch_size=128:  67%|██████▋   | 67/100 [02:05<01:02,  1.88s/it]Measuring inference with batch_size=128:  68%|██████▊   | 68/100 [02:07<01:00,  1.88s/it]Measuring inference with batch_size=128:  69%|██████▉   | 69/100 [02:09<00:58,  1.88s/it]Measuring inference with batch_size=128:  70%|███████   | 70/100 [02:11<00:57,  1.93s/it]Measuring inference with batch_size=128:  71%|███████   | 71/100 [02:13<00:57,  1.98s/it]Measuring inference with batch_size=128:  72%|███████▏  | 72/100 [02:15<00:54,  1.95s/it]Measuring inference with batch_size=128:  73%|███████▎  | 73/100 [02:17<00:52,  1.94s/it]Measuring inference with batch_size=128:  74%|███████▍  | 74/100 [02:19<00:49,  1.92s/it]Measuring inference with batch_size=128:  75%|███████▌  | 75/100 [02:21<00:47,  1.91s/it]Measuring inference with batch_size=128:  76%|███████▌  | 76/100 [02:22<00:45,  1.90s/it]Measuring inference with batch_size=128:  77%|███████▋  | 77/100 [02:24<00:43,  1.90s/it]Measuring inference with batch_size=128:  78%|███████▊  | 78/100 [02:26<00:41,  1.91s/it]Measuring inference with batch_size=128:  79%|███████▉  | 79/100 [02:28<00:41,  1.95s/it]Measuring inference with batch_size=128:  80%|████████  | 80/100 [02:30<00:38,  1.95s/it]Measuring inference with batch_size=128:  81%|████████  | 81/100 [02:32<00:36,  1.93s/it]Measuring inference with batch_size=128:  82%|████████▏ | 82/100 [02:34<00:34,  1.92s/it]Measuring inference with batch_size=128:  83%|████████▎ | 83/100 [02:36<00:32,  1.92s/it]Measuring inference with batch_size=128:  84%|████████▍ | 84/100 [02:38<00:30,  1.93s/it]Measuring inference with batch_size=128:  85%|████████▌ | 85/100 [02:40<00:29,  1.99s/it]Measuring inference with batch_size=128:  86%|████████▌ | 86/100 [02:42<00:28,  2.04s/it]Measuring inference with batch_size=128:  87%|████████▋ | 87/100 [02:44<00:25,  1.99s/it]Measuring inference with batch_size=128:  88%|████████▊ | 88/100 [02:46<00:23,  1.96s/it]Measuring inference with batch_size=128:  89%|████████▉ | 89/100 [02:48<00:21,  1.95s/it]Measuring inference with batch_size=128:  90%|█████████ | 90/100 [02:50<00:19,  1.94s/it]Measuring inference with batch_size=128:  91%|█████████ | 91/100 [02:52<00:17,  1.93s/it]Measuring inference with batch_size=128:  92%|█████████▏| 92/100 [02:54<00:15,  1.92s/it]Measuring inference with batch_size=128:  93%|█████████▎| 93/100 [02:56<00:13,  1.92s/it]Measuring inference with batch_size=128:  94%|█████████▍| 94/100 [02:57<00:11,  1.92s/it]Measuring inference with batch_size=128:  95%|█████████▌| 95/100 [02:59<00:09,  1.91s/it]Measuring inference with batch_size=128:  96%|█████████▌| 96/100 [03:01<00:07,  1.90s/it]Measuring inference with batch_size=128:  97%|█████████▋| 97/100 [03:03<00:05,  1.90s/it]Measuring inference with batch_size=128:  98%|█████████▊| 98/100 [03:05<00:03,  1.89s/it]Measuring inference with batch_size=128:  99%|█████████▉| 99/100 [03:07<00:01,  1.89s/it]Measuring inference with batch_size=128: 100%|██████████| 100/100 [03:09<00:00,  1.89s/it]Measuring inference with batch_size=128: 100%|██████████| 100/100 [03:09<00:00,  1.89s/it]Timing results (batch_size=128):
  cpu_to_gpu:
    human_readable:
      batch_latency: "6.944 ms +/- 598.850 \xB5s [5.203 ms, 8.031 ms]"
      batches_per_second: 145.15 +/- 13.29 [124.52, 192.21]
    metrics:
      batches_per_second_max: 192.20529740628723
      batches_per_second_mean: 145.14953361516427
      batches_per_second_min: 124.5154816683984
      batches_per_second_std: 13.292609572392331
      seconds_per_batch_max: 0.008031129837036133
      seconds_per_batch_mean: 0.006943755149841309
      seconds_per_batch_min: 0.005202770233154297
      seconds_per_batch_std: 0.0005988497971864009
  gpu_to_cpu:
    human_readable:
      batch_latency: 89.949 ms +/- 4.434 ms [87.315 ms, 108.825 ms]
      batches_per_second: 11.14 +/- 0.48 [9.19, 11.45]
    metrics:
      batches_per_second_max: 11.452775062393167
      batches_per_second_mean: 11.141082874654677
      batches_per_second_min: 9.189027422679962
      batches_per_second_std: 0.4804668761414786
      seconds_per_batch_max: 0.1088254451751709
      seconds_per_batch_mean: 0.08994884729385376
      seconds_per_batch_min: 0.08731508255004883
      seconds_per_batch_std: 0.00443367931525834
  on_device_inference:
    human_readable:
      batch_latency: 1.795 s +/- 69.217 ms [1.757 s, 2.165 s]
      batches_per_second: 0.56 +/- 0.02 [0.46, 0.57]
    metrics:
      batches_per_second_max: 0.5692164693847113
      batches_per_second_mean: 0.5579092145524175
      batches_per_second_min: 0.46182079194653486
      batches_per_second_std: 0.019339639782278123
      seconds_per_batch_max: 2.165342092514038
      seconds_per_batch_mean: 1.7948013854026794
      seconds_per_batch_min: 1.756800889968872
      seconds_per_batch_std: 0.06921707859098689
  total:
    human_readable:
      batch_latency: 1.892 s +/- 71.815 ms [1.851 s, 2.275 s]
      batches_per_second: 0.53 +/- 0.02 [0.44, 0.54]
    metrics:
      batches_per_second_max: 0.5401284830546649
      batches_per_second_mean: 0.5293135736347937
      batches_per_second_min: 0.43953600769268497
      batches_per_second_std: 0.018123516885438257
      seconds_per_batch_max: 2.2751264572143555
      seconds_per_batch_mean: 1.8916939878463745
      seconds_per_batch_min: 1.8514113426208496
      seconds_per_batch_std: 0.07181490642893877

Energy results (batch_size=128):
  joules: 51.5554264837424
  kWh: 1.4320951801039556e-05

learner.model.forward:
  device: cuda
  energy:
    batch_size_1:
      joules: 51.53619978780746
      kWh: 1.431561105216874e-05
    batch_size_128:
      joules: 51.5554264837424
      kWh: 1.4320951801039556e-05
  flops: 138232160
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 9.44 GB
      total: 31.17 GB
      used: 23.52 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  max_inference_memory: 14827641856
  params: 3794322
  post_inference_memory: 14778309632
  pre_inference_memory: 7335638016
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "190.709 \xB5s +/- 48.017 \xB5s [123.978 \xB5s, 325.918 \xB5\
            s]"
          batches_per_second: 5.54 K +/- 1.21 K [3.07 K, 8.07 K]
        metrics:
          batches_per_second_max: 8065.969230769231
          batches_per_second_mean: 5542.371765664944
          batches_per_second_min: 3068.2545720555963
          batches_per_second_std: 1211.1923571751852
          seconds_per_batch_max: 0.00032591819763183594
          seconds_per_batch_mean: 0.00019070863723754882
          seconds_per_batch_min: 0.0001239776611328125
          seconds_per_batch_std: 4.80166245634181e-05
      gpu_to_cpu:
        human_readable:
          batch_latency: "157.993 \xB5s +/- 22.834 \xB5s [125.408 \xB5s, 222.921 \xB5\
            s]"
          batches_per_second: 6.46 K +/- 911.21 [4.49 K, 7.97 K]
        metrics:
          batches_per_second_max: 7973.961977186312
          batches_per_second_mean: 6459.613116833283
          batches_per_second_min: 4485.886631016043
          batches_per_second_std: 911.208544022827
          seconds_per_batch_max: 0.00022292137145996094
          seconds_per_batch_mean: 0.0001579928398132324
          seconds_per_batch_min: 0.00012540817260742188
          seconds_per_batch_std: 2.2833506896389548e-05
      on_device_inference:
        human_readable:
          batch_latency: 83.514 ms +/- 4.298 ms [77.589 ms, 94.632 ms]
          batches_per_second: 12.00 +/- 0.60 [10.57, 12.89]
        metrics:
          batches_per_second_max: 12.888458690167809
          batches_per_second_mean: 12.004749677743964
          batches_per_second_min: 10.567286616244324
          batches_per_second_std: 0.5984903017020714
          seconds_per_batch_max: 0.09463167190551758
          seconds_per_batch_mean: 0.08351434946060181
          seconds_per_batch_min: 0.07758879661560059
          seconds_per_batch_std: 0.004298378536264893
      total:
        human_readable:
          batch_latency: 83.863 ms +/- 4.333 ms [77.915 ms, 95.006 ms]
          batches_per_second: 11.96 +/- 0.60 [10.53, 12.83]
        metrics:
          batches_per_second_max: 12.834506837536223
          batches_per_second_mean: 11.955076043589521
          batches_per_second_min: 10.525599393705175
          batches_per_second_std: 0.5983511019731392
          seconds_per_batch_max: 0.09500646591186523
          seconds_per_batch_mean: 0.08386305093765259
          seconds_per_batch_min: 0.07791495323181152
          seconds_per_batch_std: 0.004333378937381598
    batch_size_128:
      cpu_to_gpu:
        human_readable:
          batch_latency: "6.944 ms +/- 598.850 \xB5s [5.203 ms, 8.031 ms]"
          batches_per_second: 145.15 +/- 13.29 [124.52, 192.21]
        metrics:
          batches_per_second_max: 192.20529740628723
          batches_per_second_mean: 145.14953361516427
          batches_per_second_min: 124.5154816683984
          batches_per_second_std: 13.292609572392331
          seconds_per_batch_max: 0.008031129837036133
          seconds_per_batch_mean: 0.006943755149841309
          seconds_per_batch_min: 0.005202770233154297
          seconds_per_batch_std: 0.0005988497971864009
      gpu_to_cpu:
        human_readable:
          batch_latency: 89.949 ms +/- 4.434 ms [87.315 ms, 108.825 ms]
          batches_per_second: 11.14 +/- 0.48 [9.19, 11.45]
        metrics:
          batches_per_second_max: 11.452775062393167
          batches_per_second_mean: 11.141082874654677
          batches_per_second_min: 9.189027422679962
          batches_per_second_std: 0.4804668761414786
          seconds_per_batch_max: 0.1088254451751709
          seconds_per_batch_mean: 0.08994884729385376
          seconds_per_batch_min: 0.08731508255004883
          seconds_per_batch_std: 0.00443367931525834
      on_device_inference:
        human_readable:
          batch_latency: 1.795 s +/- 69.217 ms [1.757 s, 2.165 s]
          batches_per_second: 0.56 +/- 0.02 [0.46, 0.57]
        metrics:
          batches_per_second_max: 0.5692164693847113
          batches_per_second_mean: 0.5579092145524175
          batches_per_second_min: 0.46182079194653486
          batches_per_second_std: 0.019339639782278123
          seconds_per_batch_max: 2.165342092514038
          seconds_per_batch_mean: 1.7948013854026794
          seconds_per_batch_min: 1.756800889968872
          seconds_per_batch_std: 0.06921707859098689
      total:
        human_readable:
          batch_latency: 1.892 s +/- 71.815 ms [1.851 s, 2.275 s]
          batches_per_second: 0.53 +/- 0.02 [0.44, 0.54]
        metrics:
          batches_per_second_max: 0.5401284830546649
          batches_per_second_mean: 0.5293135736347937
          batches_per_second_min: 0.43953600769268497
          batches_per_second_std: 0.018123516885438257
          seconds_per_batch_max: 2.2751264572143555
          seconds_per_batch_mean: 1.8916939878463745
          seconds_per_batch_min: 1.8514113426208496
          seconds_per_batch_std: 0.07181490642893877

==== Benchmarking CoX3DLearner (m) ====
== Benchmarking learner.infer ==
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'

Machine info:
  cpu:
    architecture: aarch64
    cores:
      physical: 8
      total: 8
    frequency: 2.27 GHz
    model: ARMv8 Processor rev 0 (v8l)
  gpus: null
  memory:
    available: 9.32 GB
    total: 31.17 GB
    used: 23.61 GB
  system:
    node: xavier
    release: 4.9.140-tegra
    system: Linux

Model device: cuda:0
Allocated GPU memory prior to inference: 14540483072 (13.54 GB)
Allocated GPU memory after to inference: 21859699200 (20.36 GB)
Max allocated GPU memory during inference: 21906668544 (20.40 GB)
Warming up with batch_size=64:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=64:  10%|█         | 1/10 [00:00<00:01,  7.46it/s]Warming up with batch_size=64:  20%|██        | 2/10 [00:00<00:01,  7.51it/s]Warming up with batch_size=64:  30%|███       | 3/10 [00:00<00:00,  7.73it/s]Warming up with batch_size=64:  40%|████      | 4/10 [00:00<00:00,  8.36it/s]Warming up with batch_size=64:  50%|█████     | 5/10 [00:00<00:00,  8.81it/s]Warming up with batch_size=64:  60%|██████    | 6/10 [00:00<00:00,  9.09it/s]Warming up with batch_size=64:  70%|███████   | 7/10 [00:00<00:00,  9.36it/s]Warming up with batch_size=64:  80%|████████  | 8/10 [00:00<00:00,  9.54it/s]Warming up with batch_size=64:  90%|█████████ | 9/10 [00:01<00:00,  9.67it/s]Warming up with batch_size=64: 100%|██████████| 10/10 [00:01<00:00,  9.07it/s]
Measuring inference with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=64:   1%|          | 1/100 [00:00<00:10,  9.03it/s]Measuring inference with batch_size=64:   2%|▏         | 2/100 [00:00<00:10,  9.14it/s]Measuring inference with batch_size=64:   3%|▎         | 3/100 [00:00<00:10,  9.37it/s]Measuring inference with batch_size=64:   4%|▍         | 4/100 [00:00<00:10,  9.57it/s]Measuring inference with batch_size=64:   6%|▌         | 6/100 [00:00<00:09,  9.87it/s]Measuring inference with batch_size=64:   7%|▋         | 7/100 [00:00<00:09,  9.78it/s]Measuring inference with batch_size=64:   8%|▊         | 8/100 [00:00<00:09,  9.22it/s]Measuring inference with batch_size=64:   9%|▉         | 9/100 [00:00<00:10,  9.02it/s]Measuring inference with batch_size=64:  11%|█         | 11/100 [00:01<00:09,  9.61it/s]Measuring inference with batch_size=64:  12%|█▏        | 12/100 [00:01<00:09,  9.55it/s]Measuring inference with batch_size=64:  13%|█▎        | 13/100 [00:01<00:09,  9.34it/s]Measuring inference with batch_size=64:  14%|█▍        | 14/100 [00:01<00:09,  9.15it/s]Measuring inference with batch_size=64:  15%|█▌        | 15/100 [00:01<00:09,  9.05it/s]Measuring inference with batch_size=64:  17%|█▋        | 17/100 [00:01<00:08,  9.72it/s]Measuring inference with batch_size=64:  18%|█▊        | 18/100 [00:01<00:08,  9.70it/s]Measuring inference with batch_size=64:  20%|██        | 20/100 [00:02<00:07, 10.11it/s]Measuring inference with batch_size=64:  22%|██▏       | 22/100 [00:02<00:07, 10.18it/s]Measuring inference with batch_size=64:  24%|██▍       | 24/100 [00:02<00:07, 10.16it/s]Measuring inference with batch_size=64:  26%|██▌       | 26/100 [00:02<00:07, 10.04it/s]Measuring inference with batch_size=64:  28%|██▊       | 28/100 [00:02<00:07, 10.15it/s]Measuring inference with batch_size=64:  30%|███       | 30/100 [00:03<00:06, 10.28it/s]Measuring inference with batch_size=64:  32%|███▏      | 32/100 [00:03<00:06, 10.17it/s]Measuring inference with batch_size=64:  34%|███▍      | 34/100 [00:03<00:06, 10.19it/s]Measuring inference with batch_size=64:  36%|███▌      | 36/100 [00:03<00:06, 10.25it/s]Measuring inference with batch_size=64:  38%|███▊      | 38/100 [00:03<00:05, 10.33it/s]Measuring inference with batch_size=64:  40%|████      | 40/100 [00:04<00:05, 10.38it/s]Measuring inference with batch_size=64:  42%|████▏     | 42/100 [00:04<00:05, 10.45it/s]Measuring inference with batch_size=64:  44%|████▍     | 44/100 [00:04<00:05, 10.38it/s]Measuring inference with batch_size=64:  46%|████▌     | 46/100 [00:04<00:05, 10.38it/s]Measuring inference with batch_size=64:  48%|████▊     | 48/100 [00:04<00:05, 10.37it/s]Measuring inference with batch_size=64:  50%|█████     | 50/100 [00:04<00:04, 10.35it/s]Measuring inference with batch_size=64:  52%|█████▏    | 52/100 [00:05<00:04, 10.21it/s]Measuring inference with batch_size=64:  54%|█████▍    | 54/100 [00:05<00:04,  9.94it/s]Measuring inference with batch_size=64:  55%|█████▌    | 55/100 [00:05<00:04,  9.86it/s]Measuring inference with batch_size=64:  56%|█████▌    | 56/100 [00:05<00:04,  9.86it/s]Measuring inference with batch_size=64:  57%|█████▋    | 57/100 [00:05<00:04,  9.65it/s]Measuring inference with batch_size=64:  58%|█████▊    | 58/100 [00:05<00:04,  9.62it/s]Measuring inference with batch_size=64:  59%|█████▉    | 59/100 [00:05<00:04,  9.61it/s]Measuring inference with batch_size=64:  61%|██████    | 61/100 [00:06<00:03, 10.02it/s]Measuring inference with batch_size=64:  62%|██████▏   | 62/100 [00:06<00:03, 10.01it/s]Measuring inference with batch_size=64:  64%|██████▍   | 64/100 [00:06<00:03, 10.30it/s]Measuring inference with batch_size=64:  66%|██████▌   | 66/100 [00:06<00:03, 10.00it/s]Measuring inference with batch_size=64:  68%|██████▊   | 68/100 [00:06<00:03,  9.80it/s]Measuring inference with batch_size=64:  69%|██████▉   | 69/100 [00:06<00:03,  9.64it/s]Measuring inference with batch_size=64:  70%|███████   | 70/100 [00:07<00:03,  9.59it/s]Measuring inference with batch_size=64:  71%|███████   | 71/100 [00:07<00:03,  9.50it/s]Measuring inference with batch_size=64:  72%|███████▏  | 72/100 [00:07<00:03,  9.31it/s]Measuring inference with batch_size=64:  73%|███████▎  | 73/100 [00:07<00:02,  9.46it/s]Measuring inference with batch_size=64:  74%|███████▍  | 74/100 [00:07<00:02,  9.26it/s]Measuring inference with batch_size=64:  75%|███████▌  | 75/100 [00:07<00:02,  9.29it/s]Measuring inference with batch_size=64:  76%|███████▌  | 76/100 [00:07<00:02,  9.48it/s]Measuring inference with batch_size=64:  77%|███████▋  | 77/100 [00:07<00:02,  9.53it/s]Measuring inference with batch_size=64:  78%|███████▊  | 78/100 [00:07<00:02,  9.33it/s]Measuring inference with batch_size=64:  79%|███████▉  | 79/100 [00:08<00:02,  9.41it/s]Measuring inference with batch_size=64:  81%|████████  | 81/100 [00:08<00:01,  9.92it/s]Measuring inference with batch_size=64:  83%|████████▎ | 83/100 [00:08<00:01,  9.96it/s]Measuring inference with batch_size=64:  84%|████████▍ | 84/100 [00:08<00:01,  9.94it/s]Measuring inference with batch_size=64:  86%|████████▌ | 86/100 [00:08<00:01,  9.60it/s]Measuring inference with batch_size=64:  87%|████████▋ | 87/100 [00:08<00:01,  9.35it/s]Measuring inference with batch_size=64:  88%|████████▊ | 88/100 [00:08<00:01,  8.41it/s]Measuring inference with batch_size=64:  89%|████████▉ | 89/100 [00:09<00:01,  8.21it/s]Measuring inference with batch_size=64:  90%|█████████ | 90/100 [00:09<00:01,  8.25it/s]Measuring inference with batch_size=64:  91%|█████████ | 91/100 [00:09<00:01,  8.47it/s]Measuring inference with batch_size=64:  92%|█████████▏| 92/100 [00:09<00:00,  8.82it/s]Measuring inference with batch_size=64:  93%|█████████▎| 93/100 [00:09<00:00,  9.01it/s]Measuring inference with batch_size=64:  94%|█████████▍| 94/100 [00:09<00:00,  9.06it/s]Measuring inference with batch_size=64:  95%|█████████▌| 95/100 [00:09<00:00,  9.15it/s]Measuring inference with batch_size=64:  96%|█████████▌| 96/100 [00:09<00:00,  9.21it/s]Measuring inference with batch_size=64:  97%|█████████▋| 97/100 [00:10<00:00,  8.90it/s]Measuring inference with batch_size=64:  98%|█████████▊| 98/100 [00:10<00:00,  8.28it/s]Measuring inference with batch_size=64:  99%|█████████▉| 99/100 [00:10<00:00,  8.07it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:10<00:00,  7.99it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:10<00:00,  9.61it/s]
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "8.445 \xB5s +/- 3.605 \xB5s [3.338 \xB5s, 16.451 \xB5s]"
      batches_per_second: 142.96 K +/- 62.16 K [60.79 K, 299.59 K]
    metrics:
      batches_per_second_max: 299593.14285714284
      batches_per_second_mean: 142958.17492521158
      batches_per_second_min: 60787.014492753624
      batches_per_second_std: 62163.21037011968
      seconds_per_batch_max: 1.6450881958007812e-05
      seconds_per_batch_mean: 8.444786071777344e-06
      seconds_per_batch_min: 3.337860107421875e-06
      seconds_per_batch_std: 3.605119127663196e-06
  gpu_to_cpu:
    human_readable:
      batch_latency: "241.718 \xB5s +/- 157.202 \xB5s [172.853 \xB5s, 1.408 ms]"
      batches_per_second: 4.60 K +/- 856.61 [710.18, 5.79 K]
    metrics:
      batches_per_second_max: 5785.246896551724
      batches_per_second_mean: 4597.3563332719295
      batches_per_second_min: 710.176769387064
      batches_per_second_std: 856.6128644202996
      seconds_per_batch_max: 0.0014081001281738281
      seconds_per_batch_mean: 0.00024171829223632813
      seconds_per_batch_min: 0.0001728534698486328
      seconds_per_batch_std: 0.00015720247108190613
  on_device_inference:
    human_readable:
      batch_latency: 103.206 ms +/- 10.941 ms [88.813 ms, 156.441 ms]
      batches_per_second: 9.78 +/- 0.90 [6.39, 11.26]
    metrics:
      batches_per_second_max: 11.259634692409291
      batches_per_second_mean: 9.783359021194439
      batches_per_second_min: 6.392167799769265
      batches_per_second_std: 0.8992965251776086
      seconds_per_batch_max: 0.15644145011901855
      seconds_per_batch_mean: 0.10320646047592164
      seconds_per_batch_min: 0.08881282806396484
      seconds_per_batch_std: 0.0109409669595604
  total:
    human_readable:
      batch_latency: 103.457 ms +/- 10.959 ms [89.017 ms, 156.679 ms]
      batches_per_second: 9.76 +/- 0.90 [6.38, 11.23]
    metrics:
      batches_per_second_max: 11.233820079064934
      batches_per_second_mean: 9.759637271777715
      batches_per_second_min: 6.382489446982309
      batches_per_second_std: 0.8971293417201438
      seconds_per_batch_max: 0.1566786766052246
      seconds_per_batch_mean: 0.10345662355422974
      seconds_per_batch_min: 0.08901691436767578
      seconds_per_batch_std: 0.010959312588913694

Energy results (batch_size=1):
  joules: 48.856648278141016
  kWh: 1.3571291188372505e-05

Warming up with batch_size=64:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=64:  10%|█         | 1/10 [00:01<00:17,  1.99s/it]Warming up with batch_size=64:  20%|██        | 2/10 [00:03<00:13,  1.71s/it]Warming up with batch_size=64:  30%|███       | 3/10 [00:04<00:11,  1.60s/it]Warming up with batch_size=64:  40%|████      | 4/10 [00:06<00:09,  1.54s/it]Warming up with batch_size=64:  50%|█████     | 5/10 [00:07<00:07,  1.51s/it]Warming up with batch_size=64:  60%|██████    | 6/10 [00:09<00:05,  1.48s/it]Warming up with batch_size=64:  70%|███████   | 7/10 [00:10<00:04,  1.50s/it]Warming up with batch_size=64:  80%|████████  | 8/10 [00:12<00:02,  1.47s/it]Warming up with batch_size=64:  90%|█████████ | 9/10 [00:13<00:01,  1.50s/it]Warming up with batch_size=64: 100%|██████████| 10/10 [00:15<00:00,  1.50s/it]Warming up with batch_size=64: 100%|██████████| 10/10 [00:15<00:00,  1.53s/it]
Measuring inference with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=64:   1%|          | 1/100 [00:01<02:19,  1.41s/it]Measuring inference with batch_size=64:   2%|▏         | 2/100 [00:02<02:19,  1.42s/it]Measuring inference with batch_size=64:   3%|▎         | 3/100 [00:04<02:18,  1.43s/it]Measuring inference with batch_size=64:   4%|▍         | 4/100 [00:05<02:16,  1.42s/it]Measuring inference with batch_size=64:   5%|▌         | 5/100 [00:07<02:15,  1.43s/it]Measuring inference with batch_size=64:   6%|▌         | 6/100 [00:08<02:15,  1.44s/it]Measuring inference with batch_size=64:   7%|▋         | 7/100 [00:09<02:12,  1.43s/it]Measuring inference with batch_size=64:   8%|▊         | 8/100 [00:11<02:11,  1.43s/it]Measuring inference with batch_size=64:   9%|▉         | 9/100 [00:12<02:09,  1.43s/it]Measuring inference with batch_size=64:  10%|█         | 10/100 [00:14<02:08,  1.43s/it]Measuring inference with batch_size=64:  11%|█         | 11/100 [00:15<02:06,  1.43s/it]Measuring inference with batch_size=64:  12%|█▏        | 12/100 [00:17<02:05,  1.43s/it]Measuring inference with batch_size=64:  13%|█▎        | 13/100 [00:18<02:03,  1.42s/it]Measuring inference with batch_size=64:  14%|█▍        | 14/100 [00:19<02:01,  1.42s/it]Measuring inference with batch_size=64:  15%|█▌        | 15/100 [00:21<02:00,  1.42s/it]Measuring inference with batch_size=64:  16%|█▌        | 16/100 [00:22<01:58,  1.42s/it]Measuring inference with batch_size=64:  17%|█▋        | 17/100 [00:24<01:58,  1.43s/it]Measuring inference with batch_size=64:  18%|█▊        | 18/100 [00:25<01:56,  1.43s/it]Measuring inference with batch_size=64:  19%|█▉        | 19/100 [00:27<01:55,  1.43s/it]Measuring inference with batch_size=64:  20%|██        | 20/100 [00:28<01:54,  1.43s/it]Measuring inference with batch_size=64:  21%|██        | 21/100 [00:29<01:52,  1.42s/it]Measuring inference with batch_size=64:  22%|██▏       | 22/100 [00:31<01:50,  1.42s/it]Measuring inference with batch_size=64:  23%|██▎       | 23/100 [00:32<01:49,  1.42s/it]Measuring inference with batch_size=64:  24%|██▍       | 24/100 [00:34<01:47,  1.42s/it]Measuring inference with batch_size=64:  25%|██▌       | 25/100 [00:35<01:46,  1.42s/it]Measuring inference with batch_size=64:  26%|██▌       | 26/100 [00:36<01:44,  1.42s/it]Measuring inference with batch_size=64:  27%|██▋       | 27/100 [00:38<01:43,  1.42s/it]Measuring inference with batch_size=64:  28%|██▊       | 28/100 [00:39<01:41,  1.41s/it]Measuring inference with batch_size=64:  29%|██▉       | 29/100 [00:41<01:41,  1.42s/it]Measuring inference with batch_size=64:  30%|███       | 30/100 [00:42<01:39,  1.42s/it]Measuring inference with batch_size=64:  31%|███       | 31/100 [00:44<01:38,  1.42s/it]Measuring inference with batch_size=64:  32%|███▏      | 32/100 [00:45<01:37,  1.43s/it]Measuring inference with batch_size=64:  33%|███▎      | 33/100 [00:46<01:35,  1.42s/it]Measuring inference with batch_size=64:  34%|███▍      | 34/100 [00:48<01:34,  1.43s/it]Measuring inference with batch_size=64:  35%|███▌      | 35/100 [00:49<01:32,  1.42s/it]Measuring inference with batch_size=64:  36%|███▌      | 36/100 [00:51<01:31,  1.43s/it]Measuring inference with batch_size=64:  37%|███▋      | 37/100 [00:52<01:30,  1.43s/it]Measuring inference with batch_size=64:  38%|███▊      | 38/100 [00:54<01:28,  1.43s/it]Measuring inference with batch_size=64:  39%|███▉      | 39/100 [00:55<01:27,  1.43s/it]Measuring inference with batch_size=64:  40%|████      | 40/100 [00:56<01:26,  1.44s/it]Measuring inference with batch_size=64:  41%|████      | 41/100 [00:58<01:24,  1.43s/it]Measuring inference with batch_size=64:  42%|████▏     | 42/100 [00:59<01:23,  1.43s/it]Measuring inference with batch_size=64:  43%|████▎     | 43/100 [01:01<01:21,  1.43s/it]Measuring inference with batch_size=64:  44%|████▍     | 44/100 [01:02<01:20,  1.43s/it]Measuring inference with batch_size=64:  45%|████▌     | 45/100 [01:04<01:18,  1.43s/it]Measuring inference with batch_size=64:  46%|████▌     | 46/100 [01:05<01:16,  1.42s/it]Measuring inference with batch_size=64:  47%|████▋     | 47/100 [01:06<01:15,  1.42s/it]Measuring inference with batch_size=64:  48%|████▊     | 48/100 [01:08<01:14,  1.42s/it]Measuring inference with batch_size=64:  49%|████▉     | 49/100 [01:09<01:12,  1.43s/it]Measuring inference with batch_size=64:  50%|█████     | 50/100 [01:11<01:11,  1.42s/it]Measuring inference with batch_size=64:  51%|█████     | 51/100 [01:12<01:09,  1.42s/it]Measuring inference with batch_size=64:  52%|█████▏    | 52/100 [01:14<01:07,  1.42s/it]Measuring inference with batch_size=64:  53%|█████▎    | 53/100 [01:15<01:06,  1.42s/it]Measuring inference with batch_size=64:  54%|█████▍    | 54/100 [01:16<01:06,  1.44s/it]Measuring inference with batch_size=64:  55%|█████▌    | 55/100 [01:18<01:06,  1.47s/it]Measuring inference with batch_size=64:  56%|█████▌    | 56/100 [01:20<01:05,  1.48s/it]Measuring inference with batch_size=64:  57%|█████▋    | 57/100 [01:21<01:05,  1.53s/it]Measuring inference with batch_size=64:  58%|█████▊    | 58/100 [01:23<01:06,  1.58s/it]Measuring inference with batch_size=64:  59%|█████▉    | 59/100 [01:24<01:04,  1.57s/it]Measuring inference with batch_size=64:  60%|██████    | 60/100 [01:26<01:03,  1.58s/it]Measuring inference with batch_size=64:  61%|██████    | 61/100 [01:28<01:02,  1.60s/it]Measuring inference with batch_size=64:  62%|██████▏   | 62/100 [01:29<01:00,  1.59s/it]Measuring inference with batch_size=64:  63%|██████▎   | 63/100 [01:31<00:59,  1.61s/it]Measuring inference with batch_size=64:  64%|██████▍   | 64/100 [01:33<00:58,  1.61s/it]Measuring inference with batch_size=64:  65%|██████▌   | 65/100 [01:34<00:55,  1.60s/it]Measuring inference with batch_size=64:  66%|██████▌   | 66/100 [01:36<00:54,  1.59s/it]Measuring inference with batch_size=64:  67%|██████▋   | 67/100 [01:37<00:51,  1.57s/it]Measuring inference with batch_size=64:  68%|██████▊   | 68/100 [01:39<00:49,  1.53s/it]Measuring inference with batch_size=64:  69%|██████▉   | 69/100 [01:40<00:47,  1.54s/it]Measuring inference with batch_size=64:  70%|███████   | 70/100 [01:42<00:45,  1.51s/it]Measuring inference with batch_size=64:  71%|███████   | 71/100 [01:43<00:43,  1.49s/it]Measuring inference with batch_size=64:  72%|███████▏  | 72/100 [01:44<00:41,  1.47s/it]Measuring inference with batch_size=64:  73%|███████▎  | 73/100 [01:46<00:39,  1.46s/it]Measuring inference with batch_size=64:  74%|███████▍  | 74/100 [01:47<00:37,  1.45s/it]Measuring inference with batch_size=64:  75%|███████▌  | 75/100 [01:49<00:36,  1.45s/it]Measuring inference with batch_size=64:  76%|███████▌  | 76/100 [01:50<00:34,  1.44s/it]Measuring inference with batch_size=64:  77%|███████▋  | 77/100 [01:52<00:33,  1.45s/it]Measuring inference with batch_size=64:  78%|███████▊  | 78/100 [01:53<00:32,  1.46s/it]Measuring inference with batch_size=64:  79%|███████▉  | 79/100 [01:55<00:31,  1.49s/it]Measuring inference with batch_size=64:  80%|████████  | 80/100 [01:56<00:29,  1.47s/it]Measuring inference with batch_size=64:  81%|████████  | 81/100 [01:58<00:29,  1.53s/it]Measuring inference with batch_size=64:  82%|████████▏ | 82/100 [01:59<00:27,  1.53s/it]Measuring inference with batch_size=64:  83%|████████▎ | 83/100 [02:01<00:26,  1.54s/it]Measuring inference with batch_size=64:  84%|████████▍ | 84/100 [02:03<00:25,  1.60s/it]Measuring inference with batch_size=64:  85%|████████▌ | 85/100 [02:04<00:23,  1.55s/it]Measuring inference with batch_size=64:  86%|████████▌ | 86/100 [02:05<00:21,  1.50s/it]Measuring inference with batch_size=64:  87%|████████▋ | 87/100 [02:07<00:19,  1.47s/it]Measuring inference with batch_size=64:  88%|████████▊ | 88/100 [02:08<00:17,  1.45s/it]Measuring inference with batch_size=64:  89%|████████▉ | 89/100 [02:10<00:15,  1.44s/it]Measuring inference with batch_size=64:  90%|█████████ | 90/100 [02:11<00:14,  1.43s/it]Measuring inference with batch_size=64:  91%|█████████ | 91/100 [02:12<00:12,  1.42s/it]Measuring inference with batch_size=64:  92%|█████████▏| 92/100 [02:14<00:11,  1.42s/it]Measuring inference with batch_size=64:  93%|█████████▎| 93/100 [02:15<00:09,  1.42s/it]Measuring inference with batch_size=64:  94%|█████████▍| 94/100 [02:17<00:08,  1.42s/it]Measuring inference with batch_size=64:  95%|█████████▌| 95/100 [02:18<00:07,  1.42s/it]Measuring inference with batch_size=64:  96%|█████████▌| 96/100 [02:20<00:05,  1.41s/it]Measuring inference with batch_size=64:  97%|█████████▋| 97/100 [02:21<00:04,  1.41s/it]Measuring inference with batch_size=64:  98%|█████████▊| 98/100 [02:22<00:02,  1.41s/it]Measuring inference with batch_size=64:  99%|█████████▉| 99/100 [02:24<00:01,  1.41s/it]Measuring inference with batch_size=64: 100%|██████████| 100/100 [02:25<00:00,  1.41s/it]Measuring inference with batch_size=64: 100%|██████████| 100/100 [02:25<00:00,  1.46s/it]Timing results (batch_size=64):
  cpu_to_gpu:
    human_readable:
      batch_latency: "10.755 \xB5s +/- 6.017 \xB5s [5.484 \xB5s, 44.823 \xB5s]"
      batches_per_second: 109.64 K +/- 37.05 K [22.31 K, 182.36 K]
    metrics:
      batches_per_second_max: 182361.04347826086
      batches_per_second_mean: 109637.84463468254
      batches_per_second_min: 22310.127659574468
      batches_per_second_std: 37054.32291743276
      seconds_per_batch_max: 4.482269287109375e-05
      seconds_per_batch_mean: 1.0755062103271484e-05
      seconds_per_batch_min: 5.4836273193359375e-06
      seconds_per_batch_std: 6.0174974292917074e-06
  gpu_to_cpu:
    human_readable:
      batch_latency: "4.769 ms +/- 599.908 \xB5s [4.097 ms, 8.339 ms]"
      batches_per_second: 212.15 +/- 20.00 [119.92, 244.11]
    metrics:
      batches_per_second_max: 244.1103480386451
      batches_per_second_mean: 212.148953639831
      batches_per_second_min: 119.91948764867338
      batches_per_second_std: 19.996273228625128
      seconds_per_batch_max: 0.00833892822265625
      seconds_per_batch_mean: 0.00476895809173584
      seconds_per_batch_min: 0.004096508026123047
      seconds_per_batch_std: 0.0005999078721229161
  on_device_inference:
    human_readable:
      batch_latency: 1.451 s +/- 73.744 ms [1.392 s, 1.735 s]
      batches_per_second: 0.69 +/- 0.03 [0.58, 0.72]
    metrics:
      batches_per_second_max: 0.7183726151680732
      batches_per_second_mean: 0.6909012972050138
      batches_per_second_min: 0.5764409946006209
      batches_per_second_std: 0.03203484162287109
      seconds_per_batch_max: 1.7347829341888428
      seconds_per_batch_mean: 1.4507990717887878
      seconds_per_batch_min: 1.3920352458953857
      seconds_per_batch_std: 0.07374449446246208
  total:
    human_readable:
      batch_latency: 1.456 s +/- 73.953 ms [1.396 s, 1.740 s]
      batches_per_second: 0.69 +/- 0.03 [0.57, 0.72]
    metrics:
      batches_per_second_max: 0.7160807374584834
      batches_per_second_mean: 0.68863120210933
      batches_per_second_min: 0.5747251617924535
      batches_per_second_std: 0.03191731170696952
      seconds_per_batch_max: 1.739962100982666
      seconds_per_batch_mean: 1.455578784942627
      seconds_per_batch_min: 1.3964905738830566
      seconds_per_batch_std: 0.07395316357266102

Energy results (batch_size=64):
  joules: 34.672139797274276
  kWh: 9.631149943687298e-06

learner.infer:
  device: cuda
  energy:
    batch_size_1:
      joules: 48.856648278141016
      kWh: 1.3571291188372505e-05
    batch_size_64:
      joules: 34.672139797274276
      kWh: 9.631149943687298e-06
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 9.32 GB
      total: 31.17 GB
      used: 23.61 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  max_inference_memory: 21906668544
  post_inference_memory: 21859699200
  pre_inference_memory: 14540483072
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "8.445 \xB5s +/- 3.605 \xB5s [3.338 \xB5s, 16.451 \xB5s]"
          batches_per_second: 142.96 K +/- 62.16 K [60.79 K, 299.59 K]
        metrics:
          batches_per_second_max: 299593.14285714284
          batches_per_second_mean: 142958.17492521158
          batches_per_second_min: 60787.014492753624
          batches_per_second_std: 62163.21037011968
          seconds_per_batch_max: 1.6450881958007812e-05
          seconds_per_batch_mean: 8.444786071777344e-06
          seconds_per_batch_min: 3.337860107421875e-06
          seconds_per_batch_std: 3.605119127663196e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: "241.718 \xB5s +/- 157.202 \xB5s [172.853 \xB5s, 1.408 ms]"
          batches_per_second: 4.60 K +/- 856.61 [710.18, 5.79 K]
        metrics:
          batches_per_second_max: 5785.246896551724
          batches_per_second_mean: 4597.3563332719295
          batches_per_second_min: 710.176769387064
          batches_per_second_std: 856.6128644202996
          seconds_per_batch_max: 0.0014081001281738281
          seconds_per_batch_mean: 0.00024171829223632813
          seconds_per_batch_min: 0.0001728534698486328
          seconds_per_batch_std: 0.00015720247108190613
      on_device_inference:
        human_readable:
          batch_latency: 103.206 ms +/- 10.941 ms [88.813 ms, 156.441 ms]
          batches_per_second: 9.78 +/- 0.90 [6.39, 11.26]
        metrics:
          batches_per_second_max: 11.259634692409291
          batches_per_second_mean: 9.783359021194439
          batches_per_second_min: 6.392167799769265
          batches_per_second_std: 0.8992965251776086
          seconds_per_batch_max: 0.15644145011901855
          seconds_per_batch_mean: 0.10320646047592164
          seconds_per_batch_min: 0.08881282806396484
          seconds_per_batch_std: 0.0109409669595604
      total:
        human_readable:
          batch_latency: 103.457 ms +/- 10.959 ms [89.017 ms, 156.679 ms]
          batches_per_second: 9.76 +/- 0.90 [6.38, 11.23]
        metrics:
          batches_per_second_max: 11.233820079064934
          batches_per_second_mean: 9.759637271777715
          batches_per_second_min: 6.382489446982309
          batches_per_second_std: 0.8971293417201438
          seconds_per_batch_max: 0.1566786766052246
          seconds_per_batch_mean: 0.10345662355422974
          seconds_per_batch_min: 0.08901691436767578
          seconds_per_batch_std: 0.010959312588913694
    batch_size_64:
      cpu_to_gpu:
        human_readable:
          batch_latency: "10.755 \xB5s +/- 6.017 \xB5s [5.484 \xB5s, 44.823 \xB5s]"
          batches_per_second: 109.64 K +/- 37.05 K [22.31 K, 182.36 K]
        metrics:
          batches_per_second_max: 182361.04347826086
          batches_per_second_mean: 109637.84463468254
          batches_per_second_min: 22310.127659574468
          batches_per_second_std: 37054.32291743276
          seconds_per_batch_max: 4.482269287109375e-05
          seconds_per_batch_mean: 1.0755062103271484e-05
          seconds_per_batch_min: 5.4836273193359375e-06
          seconds_per_batch_std: 6.0174974292917074e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: "4.769 ms +/- 599.908 \xB5s [4.097 ms, 8.339 ms]"
          batches_per_second: 212.15 +/- 20.00 [119.92, 244.11]
        metrics:
          batches_per_second_max: 244.1103480386451
          batches_per_second_mean: 212.148953639831
          batches_per_second_min: 119.91948764867338
          batches_per_second_std: 19.996273228625128
          seconds_per_batch_max: 0.00833892822265625
          seconds_per_batch_mean: 0.00476895809173584
          seconds_per_batch_min: 0.004096508026123047
          seconds_per_batch_std: 0.0005999078721229161
      on_device_inference:
        human_readable:
          batch_latency: 1.451 s +/- 73.744 ms [1.392 s, 1.735 s]
          batches_per_second: 0.69 +/- 0.03 [0.58, 0.72]
        metrics:
          batches_per_second_max: 0.7183726151680732
          batches_per_second_mean: 0.6909012972050138
          batches_per_second_min: 0.5764409946006209
          batches_per_second_std: 0.03203484162287109
          seconds_per_batch_max: 1.7347829341888428
          seconds_per_batch_mean: 1.4507990717887878
          seconds_per_batch_min: 1.3920352458953857
          seconds_per_batch_std: 0.07374449446246208
      total:
        human_readable:
          batch_latency: 1.456 s +/- 73.953 ms [1.396 s, 1.740 s]
          batches_per_second: 0.69 +/- 0.03 [0.57, 0.72]
        metrics:
          batches_per_second_max: 0.7160807374584834
          batches_per_second_mean: 0.68863120210933
          batches_per_second_min: 0.5747251617924535
          batches_per_second_std: 0.03191731170696952
          seconds_per_batch_max: 1.739962100982666
          seconds_per_batch_mean: 1.455578784942627
          seconds_per_batch_min: 1.3964905738830566
          seconds_per_batch_std: 0.07395316357266102

== Benchmarking model directly ==
Machine info:
  cpu:
    architecture: aarch64
    cores:
      physical: 8
      total: 8
    frequency: 2.27 GHz
    model: ARMv8 Processor rev 0 (v8l)
  gpus: null
  memory:
    available: 2.71 GB
    total: 31.17 GB
    used: 27.98 GB
  system:
    node: xavier
    release: 4.9.140-tegra
    system: Linux


Model device: cuda:0
Model parameters: 3794322 (3.79 M)
Model FLOPs: 269136368 (269.14 M)
Allocated GPU memory prior to inference: 14655741952 (13.65 GB)
Allocated GPU memory after to inference: 21871864320 (20.37 GB)
Max allocated GPU memory during inference: 21919934976 (20.41 GB)
Warming up with batch_size=64:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=64:  10%|█         | 1/10 [00:00<00:00,  9.20it/s]Warming up with batch_size=64:  30%|███       | 3/10 [00:00<00:00, 11.12it/s]Warming up with batch_size=64:  50%|█████     | 5/10 [00:00<00:00, 11.16it/s]Warming up with batch_size=64:  70%|███████   | 7/10 [00:00<00:00, 11.21it/s]Warming up with batch_size=64:  90%|█████████ | 9/10 [00:00<00:00, 11.21it/s]Warming up with batch_size=64: 100%|██████████| 10/10 [00:00<00:00, 11.06it/s]
Measuring inference with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=64:   2%|▏         | 2/100 [00:00<00:08, 11.38it/s]Measuring inference with batch_size=64:   4%|▍         | 4/100 [00:00<00:08, 11.47it/s]Measuring inference with batch_size=64:   6%|▌         | 6/100 [00:00<00:08, 11.09it/s]Measuring inference with batch_size=64:   8%|▊         | 8/100 [00:00<00:08, 11.19it/s]Measuring inference with batch_size=64:  10%|█         | 10/100 [00:00<00:08, 11.18it/s]Measuring inference with batch_size=64:  12%|█▏        | 12/100 [00:01<00:07, 11.13it/s]Measuring inference with batch_size=64:  14%|█▍        | 14/100 [00:01<00:07, 11.16it/s]Measuring inference with batch_size=64:  16%|█▌        | 16/100 [00:01<00:07, 11.28it/s]Measuring inference with batch_size=64:  18%|█▊        | 18/100 [00:01<00:07, 11.32it/s]Measuring inference with batch_size=64:  20%|██        | 20/100 [00:01<00:07, 11.41it/s]Measuring inference with batch_size=64:  22%|██▏       | 22/100 [00:01<00:06, 11.48it/s]Measuring inference with batch_size=64:  24%|██▍       | 24/100 [00:02<00:06, 11.53it/s]Measuring inference with batch_size=64:  26%|██▌       | 26/100 [00:02<00:06, 11.67it/s]Measuring inference with batch_size=64:  28%|██▊       | 28/100 [00:02<00:06, 11.78it/s]Measuring inference with batch_size=64:  30%|███       | 30/100 [00:02<00:05, 11.88it/s]Measuring inference with batch_size=64:  32%|███▏      | 32/100 [00:02<00:05, 11.97it/s]Measuring inference with batch_size=64:  34%|███▍      | 34/100 [00:02<00:05, 12.01it/s]Measuring inference with batch_size=64:  36%|███▌      | 36/100 [00:03<00:05, 12.00it/s]Measuring inference with batch_size=64:  38%|███▊      | 38/100 [00:03<00:05, 12.04it/s]Measuring inference with batch_size=64:  40%|████      | 40/100 [00:03<00:04, 12.05it/s]Measuring inference with batch_size=64:  42%|████▏     | 42/100 [00:03<00:04, 12.10it/s]Measuring inference with batch_size=64:  44%|████▍     | 44/100 [00:03<00:04, 12.13it/s]Measuring inference with batch_size=64:  46%|████▌     | 46/100 [00:03<00:04, 12.16it/s]Measuring inference with batch_size=64:  48%|████▊     | 48/100 [00:04<00:04, 12.20it/s]Measuring inference with batch_size=64:  50%|█████     | 50/100 [00:04<00:04, 12.21it/s]Measuring inference with batch_size=64:  52%|█████▏    | 52/100 [00:04<00:03, 12.20it/s]Measuring inference with batch_size=64:  54%|█████▍    | 54/100 [00:04<00:03, 12.19it/s]Measuring inference with batch_size=64:  56%|█████▌    | 56/100 [00:04<00:03, 12.19it/s]Measuring inference with batch_size=64:  58%|█████▊    | 58/100 [00:04<00:03, 12.20it/s]Measuring inference with batch_size=64:  60%|██████    | 60/100 [00:05<00:03, 12.22it/s]Measuring inference with batch_size=64:  62%|██████▏   | 62/100 [00:05<00:03, 12.22it/s]Measuring inference with batch_size=64:  64%|██████▍   | 64/100 [00:05<00:03, 11.84it/s]Measuring inference with batch_size=64:  66%|██████▌   | 66/100 [00:05<00:02, 11.70it/s]Measuring inference with batch_size=64:  68%|██████▊   | 68/100 [00:05<00:02, 11.61it/s]Measuring inference with batch_size=64:  70%|███████   | 70/100 [00:05<00:02, 11.63it/s]Measuring inference with batch_size=64:  72%|███████▏  | 72/100 [00:06<00:02, 11.64it/s]Measuring inference with batch_size=64:  74%|███████▍  | 74/100 [00:06<00:02, 11.68it/s]Measuring inference with batch_size=64:  76%|███████▌  | 76/100 [00:06<00:02, 11.69it/s]Measuring inference with batch_size=64:  78%|███████▊  | 78/100 [00:06<00:01, 11.74it/s]Measuring inference with batch_size=64:  80%|████████  | 80/100 [00:06<00:01, 11.83it/s]Measuring inference with batch_size=64:  82%|████████▏ | 82/100 [00:06<00:01, 11.88it/s]Measuring inference with batch_size=64:  84%|████████▍ | 84/100 [00:07<00:01, 11.95it/s]Measuring inference with batch_size=64:  86%|████████▌ | 86/100 [00:07<00:01, 11.99it/s]Measuring inference with batch_size=64:  88%|████████▊ | 88/100 [00:07<00:00, 12.02it/s]Measuring inference with batch_size=64:  90%|█████████ | 90/100 [00:07<00:00, 12.07it/s]Measuring inference with batch_size=64:  92%|█████████▏| 92/100 [00:07<00:00, 12.07it/s]Measuring inference with batch_size=64:  94%|█████████▍| 94/100 [00:07<00:00, 12.11it/s]Measuring inference with batch_size=64:  96%|█████████▌| 96/100 [00:08<00:00, 12.15it/s]Measuring inference with batch_size=64:  98%|█████████▊| 98/100 [00:08<00:00, 12.14it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:08<00:00, 12.18it/s]Measuring inference with batch_size=64: 100%|██████████| 100/100 [00:08<00:00, 11.85it/s]
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "217.140 \xB5s +/- 43.922 \xB5s [171.900 \xB5s, 352.144 \xB5\
        s]"
      batches_per_second: 4.76 K +/- 796.27 [2.84 K, 5.82 K]
    metrics:
      batches_per_second_max: 5817.342579750347
      batches_per_second_mean: 4763.083010570756
      batches_per_second_min: 2839.7454299255246
      batches_per_second_std: 796.2709978189779
      seconds_per_batch_max: 0.0003521442413330078
      seconds_per_batch_mean: 0.00021713972091674805
      seconds_per_batch_min: 0.00017189979553222656
      seconds_per_batch_std: 4.392172890008347e-05
  gpu_to_cpu:
    human_readable:
      batch_latency: "168.941 \xB5s +/- 30.889 \xB5s [150.681 \xB5s, 447.273 \xB5\
        s]"
      batches_per_second: 6.02 K +/- 576.82 [2.24 K, 6.64 K]
    metrics:
      batches_per_second_max: 6636.556962025316
      batches_per_second_mean: 6015.622707171527
      batches_per_second_min: 2235.769722814499
      batches_per_second_std: 576.8227280946057
      seconds_per_batch_max: 0.00044727325439453125
      seconds_per_batch_mean: 0.00016894102096557618
      seconds_per_batch_min: 0.0001506805419921875
      seconds_per_batch_std: 3.088935312532219e-05
  on_device_inference:
    human_readable:
      batch_latency: 83.567 ms +/- 3.049 ms [80.335 ms, 94.604 ms]
      batches_per_second: 11.98 +/- 0.42 [10.57, 12.45]
    metrics:
      batches_per_second_max: 12.447889739990325
      batches_per_second_mean: 11.9817189731647
      batches_per_second_min: 10.570402498002757
      batches_per_second_std: 0.41989212627134065
      seconds_per_batch_max: 0.0946037769317627
      seconds_per_batch_mean: 0.08356725215911866
      seconds_per_batch_min: 0.08033490180969238
      seconds_per_batch_std: 0.003049407740026341
  total:
    human_readable:
      batch_latency: 83.953 ms +/- 3.076 ms [80.693 ms, 95.119 ms]
      batches_per_second: 11.93 +/- 0.42 [10.51, 12.39]
    metrics:
      batches_per_second_max: 12.392647680166405
      batches_per_second_mean: 11.926743214505581
      batches_per_second_min: 10.513094044515741
      batches_per_second_std: 0.4196615826987162
      seconds_per_batch_max: 0.09511947631835938
      seconds_per_batch_mean: 0.08395333290100097
      seconds_per_batch_min: 0.08069300651550293
      seconds_per_batch_std: 0.003076207627231075

Energy results (batch_size=1):
  joules: 37.44373313250543
  kWh: 1.0401036981251507e-05

Warming up with batch_size=64:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=64:  10%|█         | 1/10 [00:01<00:12,  1.35s/it]Warming up with batch_size=64:  20%|██        | 2/10 [00:02<00:10,  1.35s/it]Warming up with batch_size=64:  30%|███       | 3/10 [00:04<00:09,  1.35s/it]Warming up with batch_size=64:  40%|████      | 4/10 [00:05<00:08,  1.35s/it]Warming up with batch_size=64:  50%|█████     | 5/10 [00:06<00:06,  1.35s/it]Warming up with batch_size=64:  60%|██████    | 6/10 [00:08<00:05,  1.35s/it]Warming up with batch_size=64:  70%|███████   | 7/10 [00:09<00:04,  1.35s/it]Warming up with batch_size=64:  80%|████████  | 8/10 [00:10<00:02,  1.35s/it]Warming up with batch_size=64:  90%|█████████ | 9/10 [00:12<00:01,  1.35s/it]Warming up with batch_size=64: 100%|██████████| 10/10 [00:13<00:00,  1.35s/it]Warming up with batch_size=64: 100%|██████████| 10/10 [00:13<00:00,  1.35s/it]
Measuring inference with batch_size=64:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=64:   1%|          | 1/100 [00:01<02:13,  1.35s/it]Measuring inference with batch_size=64:   2%|▏         | 2/100 [00:02<02:12,  1.35s/it]Measuring inference with batch_size=64:   3%|▎         | 3/100 [00:04<02:11,  1.35s/it]Measuring inference with batch_size=64:   4%|▍         | 4/100 [00:05<02:10,  1.35s/it]Measuring inference with batch_size=64:   5%|▌         | 5/100 [00:06<02:08,  1.35s/it]Measuring inference with batch_size=64:   6%|▌         | 6/100 [00:08<02:07,  1.35s/it]Measuring inference with batch_size=64:   7%|▋         | 7/100 [00:09<02:05,  1.35s/it]Measuring inference with batch_size=64:   8%|▊         | 8/100 [00:10<02:04,  1.35s/it]Measuring inference with batch_size=64:   9%|▉         | 9/100 [00:12<02:03,  1.35s/it]Measuring inference with batch_size=64:  10%|█         | 10/100 [00:13<02:01,  1.35s/it]Measuring inference with batch_size=64:  11%|█         | 11/100 [00:14<02:00,  1.35s/it]Measuring inference with batch_size=64:  12%|█▏        | 12/100 [00:16<01:59,  1.35s/it]Measuring inference with batch_size=64:  13%|█▎        | 13/100 [00:17<01:57,  1.35s/it]Measuring inference with batch_size=64:  14%|█▍        | 14/100 [00:18<01:56,  1.35s/it]Measuring inference with batch_size=64:  15%|█▌        | 15/100 [00:20<01:55,  1.35s/it]Measuring inference with batch_size=64:  16%|█▌        | 16/100 [00:21<01:53,  1.35s/it]Measuring inference with batch_size=64:  17%|█▋        | 17/100 [00:23<01:52,  1.35s/it]Measuring inference with batch_size=64:  18%|█▊        | 18/100 [00:24<01:51,  1.35s/it]Measuring inference with batch_size=64:  19%|█▉        | 19/100 [00:25<01:49,  1.35s/it]Measuring inference with batch_size=64:  20%|██        | 20/100 [00:27<01:48,  1.35s/it]Measuring inference with batch_size=64:  21%|██        | 21/100 [00:28<01:46,  1.35s/it]Measuring inference with batch_size=64:  22%|██▏       | 22/100 [00:29<01:45,  1.35s/it]Measuring inference with batch_size=64:  23%|██▎       | 23/100 [00:31<01:44,  1.35s/it]Measuring inference with batch_size=64:  24%|██▍       | 24/100 [00:32<01:42,  1.35s/it]Measuring inference with batch_size=64:  25%|██▌       | 25/100 [00:33<01:41,  1.35s/it]Measuring inference with batch_size=64:  26%|██▌       | 26/100 [00:35<01:40,  1.35s/it]Measuring inference with batch_size=64:  27%|██▋       | 27/100 [00:36<01:38,  1.35s/it]Measuring inference with batch_size=64:  28%|██▊       | 28/100 [00:37<01:37,  1.35s/it]Measuring inference with batch_size=64:  29%|██▉       | 29/100 [00:39<01:36,  1.35s/it]Measuring inference with batch_size=64:  30%|███       | 30/100 [00:40<01:34,  1.35s/it]Measuring inference with batch_size=64:  31%|███       | 31/100 [00:41<01:33,  1.35s/it]Measuring inference with batch_size=64:  32%|███▏      | 32/100 [00:43<01:32,  1.35s/it]Measuring inference with batch_size=64:  33%|███▎      | 33/100 [00:44<01:30,  1.35s/it]Measuring inference with batch_size=64:  34%|███▍      | 34/100 [00:46<01:29,  1.35s/it]Measuring inference with batch_size=64:  35%|███▌      | 35/100 [00:47<01:28,  1.35s/it]Measuring inference with batch_size=64:  36%|███▌      | 36/100 [00:48<01:26,  1.35s/it]Measuring inference with batch_size=64:  37%|███▋      | 37/100 [00:50<01:25,  1.35s/it]Measuring inference with batch_size=64:  38%|███▊      | 38/100 [00:51<01:23,  1.35s/it]Measuring inference with batch_size=64:  39%|███▉      | 39/100 [00:52<01:22,  1.35s/it]Measuring inference with batch_size=64:  40%|████      | 40/100 [00:54<01:21,  1.35s/it]Measuring inference with batch_size=64:  41%|████      | 41/100 [00:55<01:19,  1.35s/it]Measuring inference with batch_size=64:  42%|████▏     | 42/100 [00:56<01:18,  1.36s/it]Measuring inference with batch_size=64:  43%|████▎     | 43/100 [00:58<01:17,  1.36s/it]Measuring inference with batch_size=64:  44%|████▍     | 44/100 [00:59<01:15,  1.35s/it]Measuring inference with batch_size=64:  45%|████▌     | 45/100 [01:00<01:14,  1.35s/it]Measuring inference with batch_size=64:  46%|████▌     | 46/100 [01:02<01:13,  1.36s/it]Measuring inference with batch_size=64:  47%|████▋     | 47/100 [01:03<01:11,  1.36s/it]Measuring inference with batch_size=64:  48%|████▊     | 48/100 [01:05<01:10,  1.36s/it]Measuring inference with batch_size=64:  49%|████▉     | 49/100 [01:06<01:09,  1.35s/it]Measuring inference with batch_size=64:  50%|█████     | 50/100 [01:07<01:07,  1.35s/it]Measuring inference with batch_size=64:  51%|█████     | 51/100 [01:09<01:06,  1.36s/it]Measuring inference with batch_size=64:  52%|█████▏    | 52/100 [01:10<01:05,  1.35s/it]Measuring inference with batch_size=64:  53%|█████▎    | 53/100 [01:11<01:03,  1.35s/it]Measuring inference with batch_size=64:  54%|█████▍    | 54/100 [01:13<01:02,  1.35s/it]Measuring inference with batch_size=64:  55%|█████▌    | 55/100 [01:14<01:00,  1.35s/it]Measuring inference with batch_size=64:  56%|█████▌    | 56/100 [01:15<00:59,  1.35s/it]Measuring inference with batch_size=64:  57%|█████▋    | 57/100 [01:17<00:58,  1.35s/it]Measuring inference with batch_size=64:  58%|█████▊    | 58/100 [01:18<00:56,  1.35s/it]Measuring inference with batch_size=64:  59%|█████▉    | 59/100 [01:19<00:55,  1.35s/it]Measuring inference with batch_size=64:  60%|██████    | 60/100 [01:21<00:54,  1.35s/it]Measuring inference with batch_size=64:  61%|██████    | 61/100 [01:22<00:52,  1.35s/it]Measuring inference with batch_size=64:  62%|██████▏   | 62/100 [01:23<00:51,  1.35s/it]Measuring inference with batch_size=64:  63%|██████▎   | 63/100 [01:25<00:50,  1.35s/it]Measuring inference with batch_size=64:  64%|██████▍   | 64/100 [01:26<00:48,  1.35s/it]Measuring inference with batch_size=64:  65%|██████▌   | 65/100 [01:28<00:47,  1.35s/it]Measuring inference with batch_size=64:  66%|██████▌   | 66/100 [01:29<00:46,  1.35s/it]Measuring inference with batch_size=64:  67%|██████▋   | 67/100 [01:30<00:44,  1.35s/it]Measuring inference with batch_size=64:  68%|██████▊   | 68/100 [01:32<00:43,  1.35s/it]Measuring inference with batch_size=64:  69%|██████▉   | 69/100 [01:33<00:41,  1.35s/it]Measuring inference with batch_size=64:  70%|███████   | 70/100 [01:34<00:40,  1.35s/it]Measuring inference with batch_size=64:  71%|███████   | 71/100 [01:36<00:39,  1.35s/it]Measuring inference with batch_size=64:  72%|███████▏  | 72/100 [01:37<00:37,  1.35s/it]Measuring inference with batch_size=64:  73%|███████▎  | 73/100 [01:38<00:36,  1.35s/it]Measuring inference with batch_size=64:  74%|███████▍  | 74/100 [01:40<00:35,  1.35s/it]Measuring inference with batch_size=64:  75%|███████▌  | 75/100 [01:41<00:33,  1.35s/it]Measuring inference with batch_size=64:  76%|███████▌  | 76/100 [01:42<00:32,  1.35s/it]Measuring inference with batch_size=64:  77%|███████▋  | 77/100 [01:44<00:31,  1.35s/it]Measuring inference with batch_size=64:  78%|███████▊  | 78/100 [01:45<00:29,  1.35s/it]Measuring inference with batch_size=64:  79%|███████▉  | 79/100 [01:47<00:29,  1.41s/it]Measuring inference with batch_size=64:  80%|████████  | 80/100 [01:48<00:28,  1.42s/it]Measuring inference with batch_size=64:  81%|████████  | 81/100 [01:50<00:28,  1.48s/it]Measuring inference with batch_size=64:  82%|████████▏ | 82/100 [01:51<00:27,  1.54s/it]Measuring inference with batch_size=64:  83%|████████▎ | 83/100 [01:53<00:26,  1.54s/it]Measuring inference with batch_size=64:  84%|████████▍ | 84/100 [01:54<00:24,  1.52s/it]Measuring inference with batch_size=64:  85%|████████▌ | 85/100 [01:56<00:22,  1.50s/it]Measuring inference with batch_size=64:  86%|████████▌ | 86/100 [01:57<00:20,  1.48s/it]Measuring inference with batch_size=64:  87%|████████▋ | 87/100 [01:59<00:18,  1.45s/it]Measuring inference with batch_size=64:  88%|████████▊ | 88/100 [02:00<00:17,  1.43s/it]Measuring inference with batch_size=64:  89%|████████▉ | 89/100 [02:02<00:15,  1.44s/it]Measuring inference with batch_size=64:  90%|█████████ | 90/100 [02:03<00:14,  1.42s/it]Measuring inference with batch_size=64:  91%|█████████ | 91/100 [02:04<00:12,  1.41s/it]Measuring inference with batch_size=64:  92%|█████████▏| 92/100 [02:06<00:11,  1.40s/it]Measuring inference with batch_size=64:  93%|█████████▎| 93/100 [02:07<00:09,  1.39s/it]Measuring inference with batch_size=64:  94%|█████████▍| 94/100 [02:08<00:08,  1.38s/it]Measuring inference with batch_size=64:  95%|█████████▌| 95/100 [02:10<00:07,  1.46s/it]Measuring inference with batch_size=64:  96%|█████████▌| 96/100 [02:12<00:05,  1.48s/it]Measuring inference with batch_size=64:  97%|█████████▋| 97/100 [02:13<00:04,  1.49s/it]Measuring inference with batch_size=64:  98%|█████████▊| 98/100 [02:14<00:02,  1.45s/it]Measuring inference with batch_size=64:  99%|█████████▉| 99/100 [02:16<00:01,  1.42s/it]Measuring inference with batch_size=64: 100%|██████████| 100/100 [02:17<00:00,  1.41s/it]Measuring inference with batch_size=64: 100%|██████████| 100/100 [02:17<00:00,  1.38s/it]Timing results (batch_size=64):
  cpu_to_gpu:
    human_readable:
      batch_latency: "6.864 ms +/- 455.624 \xB5s [5.088 ms, 7.936 ms]"
      batches_per_second: 146.40 +/- 10.76 [126.00, 196.53]
    metrics:
      batches_per_second_max: 196.52816043482335
      batches_per_second_mean: 146.3972824253526
      batches_per_second_min: 126.0042659296422
      batches_per_second_std: 10.763631500596755
      seconds_per_batch_max: 0.007936239242553711
      seconds_per_batch_mean: 0.006863775253295898
      seconds_per_batch_min: 0.005088329315185547
      seconds_per_batch_std: 0.0004556240966671552
  gpu_to_cpu:
    human_readable:
      batch_latency: 84.244 ms +/- 4.883 ms [82.220 ms, 107.100 ms]
      batches_per_second: 11.90 +/- 0.58 [9.34, 12.16]
    metrics:
      batches_per_second_max: 12.162444143514557
      batches_per_second_mean: 11.903752090957214
      batches_per_second_min: 9.33710886716176
      batches_per_second_std: 0.5770264619108062
      seconds_per_batch_max: 0.10709953308105469
      seconds_per_batch_mean: 0.08424356937408448
      seconds_per_batch_min: 0.08222031593322754
      seconds_per_batch_std: 0.004883231413976559
  on_device_inference:
    human_readable:
      batch_latency: 1.282 s +/- 50.520 ms [1.261 s, 1.530 s]
      batches_per_second: 0.78 +/- 0.03 [0.65, 0.79]
    metrics:
      batches_per_second_max: 0.7931859786078458
      batches_per_second_mean: 0.7814071797609634
      batches_per_second_min: 0.6536227931700278
      batches_per_second_std: 0.02737430655155078
      seconds_per_batch_max: 1.5299344062805176
      seconds_per_batch_mean: 1.2815102982521056
      seconds_per_batch_min: 1.2607383728027344
      seconds_per_batch_std: 0.05051963052906258
  total:
    human_readable:
      batch_latency: 1.373 s +/- 53.520 ms [1.351 s, 1.625 s]
      batches_per_second: 0.73 +/- 0.03 [0.62, 0.74]
    metrics:
      batches_per_second_max: 0.740387095179801
      batches_per_second_mean: 0.7295237363292736
      batches_per_second_min: 0.6153412798640421
      batches_per_second_std: 0.025385483584982296
      seconds_per_batch_max: 1.6251144409179688
      seconds_per_batch_mean: 1.372617642879486
      seconds_per_batch_min: 1.350644826889038
      seconds_per_batch_std: 0.05352048931173408

Energy results (batch_size=64):
  joules: 35.84831698082289
  kWh: 9.957865828006357e-06

learner.model.forward:
  device: cuda
  energy:
    batch_size_1:
      joules: 37.44373313250543
      kWh: 1.0401036981251507e-05
    batch_size_64:
      joules: 35.84831698082289
      kWh: 9.957865828006357e-06
  flops: 269136368
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 2.71 GB
      total: 31.17 GB
      used: 27.98 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  max_inference_memory: 21919934976
  params: 3794322
  post_inference_memory: 21871864320
  pre_inference_memory: 14655741952
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "217.140 \xB5s +/- 43.922 \xB5s [171.900 \xB5s, 352.144 \xB5\
            s]"
          batches_per_second: 4.76 K +/- 796.27 [2.84 K, 5.82 K]
        metrics:
          batches_per_second_max: 5817.342579750347
          batches_per_second_mean: 4763.083010570756
          batches_per_second_min: 2839.7454299255246
          batches_per_second_std: 796.2709978189779
          seconds_per_batch_max: 0.0003521442413330078
          seconds_per_batch_mean: 0.00021713972091674805
          seconds_per_batch_min: 0.00017189979553222656
          seconds_per_batch_std: 4.392172890008347e-05
      gpu_to_cpu:
        human_readable:
          batch_latency: "168.941 \xB5s +/- 30.889 \xB5s [150.681 \xB5s, 447.273 \xB5\
            s]"
          batches_per_second: 6.02 K +/- 576.82 [2.24 K, 6.64 K]
        metrics:
          batches_per_second_max: 6636.556962025316
          batches_per_second_mean: 6015.622707171527
          batches_per_second_min: 2235.769722814499
          batches_per_second_std: 576.8227280946057
          seconds_per_batch_max: 0.00044727325439453125
          seconds_per_batch_mean: 0.00016894102096557618
          seconds_per_batch_min: 0.0001506805419921875
          seconds_per_batch_std: 3.088935312532219e-05
      on_device_inference:
        human_readable:
          batch_latency: 83.567 ms +/- 3.049 ms [80.335 ms, 94.604 ms]
          batches_per_second: 11.98 +/- 0.42 [10.57, 12.45]
        metrics:
          batches_per_second_max: 12.447889739990325
          batches_per_second_mean: 11.9817189731647
          batches_per_second_min: 10.570402498002757
          batches_per_second_std: 0.41989212627134065
          seconds_per_batch_max: 0.0946037769317627
          seconds_per_batch_mean: 0.08356725215911866
          seconds_per_batch_min: 0.08033490180969238
          seconds_per_batch_std: 0.003049407740026341
      total:
        human_readable:
          batch_latency: 83.953 ms +/- 3.076 ms [80.693 ms, 95.119 ms]
          batches_per_second: 11.93 +/- 0.42 [10.51, 12.39]
        metrics:
          batches_per_second_max: 12.392647680166405
          batches_per_second_mean: 11.926743214505581
          batches_per_second_min: 10.513094044515741
          batches_per_second_std: 0.4196615826987162
          seconds_per_batch_max: 0.09511947631835938
          seconds_per_batch_mean: 0.08395333290100097
          seconds_per_batch_min: 0.08069300651550293
          seconds_per_batch_std: 0.003076207627231075
    batch_size_64:
      cpu_to_gpu:
        human_readable:
          batch_latency: "6.864 ms +/- 455.624 \xB5s [5.088 ms, 7.936 ms]"
          batches_per_second: 146.40 +/- 10.76 [126.00, 196.53]
        metrics:
          batches_per_second_max: 196.52816043482335
          batches_per_second_mean: 146.3972824253526
          batches_per_second_min: 126.0042659296422
          batches_per_second_std: 10.763631500596755
          seconds_per_batch_max: 0.007936239242553711
          seconds_per_batch_mean: 0.006863775253295898
          seconds_per_batch_min: 0.005088329315185547
          seconds_per_batch_std: 0.0004556240966671552
      gpu_to_cpu:
        human_readable:
          batch_latency: 84.244 ms +/- 4.883 ms [82.220 ms, 107.100 ms]
          batches_per_second: 11.90 +/- 0.58 [9.34, 12.16]
        metrics:
          batches_per_second_max: 12.162444143514557
          batches_per_second_mean: 11.903752090957214
          batches_per_second_min: 9.33710886716176
          batches_per_second_std: 0.5770264619108062
          seconds_per_batch_max: 0.10709953308105469
          seconds_per_batch_mean: 0.08424356937408448
          seconds_per_batch_min: 0.08222031593322754
          seconds_per_batch_std: 0.004883231413976559
      on_device_inference:
        human_readable:
          batch_latency: 1.282 s +/- 50.520 ms [1.261 s, 1.530 s]
          batches_per_second: 0.78 +/- 0.03 [0.65, 0.79]
        metrics:
          batches_per_second_max: 0.7931859786078458
          batches_per_second_mean: 0.7814071797609634
          batches_per_second_min: 0.6536227931700278
          batches_per_second_std: 0.02737430655155078
          seconds_per_batch_max: 1.5299344062805176
          seconds_per_batch_mean: 1.2815102982521056
          seconds_per_batch_min: 1.2607383728027344
          seconds_per_batch_std: 0.05051963052906258
      total:
        human_readable:
          batch_latency: 1.373 s +/- 53.520 ms [1.351 s, 1.625 s]
          batches_per_second: 0.73 +/- 0.03 [0.62, 0.74]
        metrics:
          batches_per_second_max: 0.740387095179801
          batches_per_second_mean: 0.7295237363292736
          batches_per_second_min: 0.6153412798640421
          batches_per_second_std: 0.025385483584982296
          seconds_per_batch_max: 1.6251144409179688
          seconds_per_batch_mean: 1.372617642879486
          seconds_per_batch_min: 1.350644826889038
          seconds_per_batch_std: 0.05352048931173408

==== Benchmarking CoX3DLearner (l) ====
== Benchmarking learner.infer ==
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'

Machine info:
  cpu:
    architecture: aarch64
    cores:
      physical: 8
      total: 8
    frequency: 2.27 GHz
    model: ARMv8 Processor rev 0 (v8l)
  gpus: null
  memory:
    available: 2.63 GB
    total: 31.17 GB
    used: 28.06 GB
  system:
    node: xavier
    release: 4.9.140-tegra
    system: Linux

Model device: cuda:0
Allocated GPU memory prior to inference: 7125865984 (6.64 GB)
Allocated GPU memory after to inference: 10256752640 (9.55 GB)
Max allocated GPU memory during inference: 10273395200 (9.57 GB)
Warming up with batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=8:  10%|█         | 1/10 [00:00<00:02,  4.23it/s]Warming up with batch_size=8:  20%|██        | 2/10 [00:00<00:01,  4.62it/s]Warming up with batch_size=8:  30%|███       | 3/10 [00:00<00:01,  4.79it/s]Warming up with batch_size=8:  40%|████      | 4/10 [00:00<00:01,  4.90it/s]Warming up with batch_size=8:  50%|█████     | 5/10 [00:01<00:01,  4.92it/s]Warming up with batch_size=8:  60%|██████    | 6/10 [00:01<00:00,  4.97it/s]Warming up with batch_size=8:  70%|███████   | 7/10 [00:01<00:00,  5.02it/s]Warming up with batch_size=8:  80%|████████  | 8/10 [00:01<00:00,  5.06it/s]Warming up with batch_size=8:  90%|█████████ | 9/10 [00:01<00:00,  5.05it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:02<00:00,  5.02it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:02<00:00,  4.94it/s]
Measuring inference with batch_size=8:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=8:   1%|          | 1/100 [00:00<00:21,  4.68it/s]Measuring inference with batch_size=8:   2%|▏         | 2/100 [00:00<00:19,  4.91it/s]Measuring inference with batch_size=8:   3%|▎         | 3/100 [00:00<00:19,  5.00it/s]Measuring inference with batch_size=8:   4%|▍         | 4/100 [00:00<00:20,  4.75it/s]Measuring inference with batch_size=8:   5%|▌         | 5/100 [00:01<00:20,  4.65it/s]Measuring inference with batch_size=8:   6%|▌         | 6/100 [00:01<00:20,  4.58it/s]Measuring inference with batch_size=8:   7%|▋         | 7/100 [00:01<00:20,  4.55it/s]Measuring inference with batch_size=8:   8%|▊         | 8/100 [00:01<00:20,  4.58it/s]Measuring inference with batch_size=8:   9%|▉         | 9/100 [00:01<00:19,  4.71it/s]Measuring inference with batch_size=8:  10%|█         | 10/100 [00:02<00:19,  4.72it/s]Measuring inference with batch_size=8:  11%|█         | 11/100 [00:02<00:19,  4.68it/s]Measuring inference with batch_size=8:  12%|█▏        | 12/100 [00:02<00:19,  4.63it/s]Measuring inference with batch_size=8:  13%|█▎        | 13/100 [00:02<00:19,  4.47it/s]Measuring inference with batch_size=8:  14%|█▍        | 14/100 [00:03<00:18,  4.54it/s]Measuring inference with batch_size=8:  15%|█▌        | 15/100 [00:03<00:19,  4.45it/s]Measuring inference with batch_size=8:  16%|█▌        | 16/100 [00:03<00:18,  4.44it/s]Measuring inference with batch_size=8:  17%|█▋        | 17/100 [00:03<00:19,  4.37it/s]Measuring inference with batch_size=8:  18%|█▊        | 18/100 [00:03<00:18,  4.40it/s]Measuring inference with batch_size=8:  19%|█▉        | 19/100 [00:04<00:18,  4.31it/s]Measuring inference with batch_size=8:  20%|██        | 20/100 [00:04<00:17,  4.51it/s]Measuring inference with batch_size=8:  21%|██        | 21/100 [00:04<00:17,  4.53it/s]Measuring inference with batch_size=8:  22%|██▏       | 22/100 [00:04<00:16,  4.69it/s]Measuring inference with batch_size=8:  23%|██▎       | 23/100 [00:05<00:16,  4.72it/s]Measuring inference with batch_size=8:  24%|██▍       | 24/100 [00:05<00:16,  4.63it/s]Measuring inference with batch_size=8:  25%|██▌       | 25/100 [00:05<00:16,  4.61it/s]Measuring inference with batch_size=8:  26%|██▌       | 26/100 [00:05<00:16,  4.53it/s]Measuring inference with batch_size=8:  27%|██▋       | 27/100 [00:05<00:16,  4.54it/s]Measuring inference with batch_size=8:  28%|██▊       | 28/100 [00:06<00:15,  4.72it/s]Measuring inference with batch_size=8:  29%|██▉       | 29/100 [00:06<00:15,  4.49it/s]Measuring inference with batch_size=8:  30%|███       | 30/100 [00:06<00:15,  4.62it/s]Measuring inference with batch_size=8:  31%|███       | 31/100 [00:06<00:14,  4.73it/s]Measuring inference with batch_size=8:  32%|███▏      | 32/100 [00:06<00:14,  4.75it/s]Measuring inference with batch_size=8:  33%|███▎      | 33/100 [00:07<00:14,  4.65it/s]Measuring inference with batch_size=8:  34%|███▍      | 34/100 [00:07<00:14,  4.51it/s]Measuring inference with batch_size=8:  35%|███▌      | 35/100 [00:07<00:14,  4.42it/s]Measuring inference with batch_size=8:  36%|███▌      | 36/100 [00:07<00:14,  4.35it/s]Measuring inference with batch_size=8:  37%|███▋      | 37/100 [00:08<00:13,  4.53it/s]Measuring inference with batch_size=8:  38%|███▊      | 38/100 [00:08<00:13,  4.64it/s]Measuring inference with batch_size=8:  39%|███▉      | 39/100 [00:08<00:12,  4.74it/s]Measuring inference with batch_size=8:  40%|████      | 40/100 [00:08<00:12,  4.78it/s]Measuring inference with batch_size=8:  41%|████      | 41/100 [00:08<00:12,  4.88it/s]Measuring inference with batch_size=8:  42%|████▏     | 42/100 [00:09<00:11,  4.87it/s]Measuring inference with batch_size=8:  43%|████▎     | 43/100 [00:09<00:12,  4.73it/s]Measuring inference with batch_size=8:  44%|████▍     | 44/100 [00:09<00:11,  4.80it/s]Measuring inference with batch_size=8:  45%|████▌     | 45/100 [00:09<00:11,  4.77it/s]Measuring inference with batch_size=8:  46%|████▌     | 46/100 [00:09<00:11,  4.57it/s]Measuring inference with batch_size=8:  47%|████▋     | 47/100 [00:10<00:11,  4.48it/s]Measuring inference with batch_size=8:  48%|████▊     | 48/100 [00:10<00:11,  4.43it/s]Measuring inference with batch_size=8:  49%|████▉     | 49/100 [00:10<00:11,  4.46it/s]Measuring inference with batch_size=8:  50%|█████     | 50/100 [00:10<00:11,  4.37it/s]Measuring inference with batch_size=8:  51%|█████     | 51/100 [00:11<00:11,  4.37it/s]Measuring inference with batch_size=8:  52%|█████▏    | 52/100 [00:11<00:10,  4.57it/s]Measuring inference with batch_size=8:  53%|█████▎    | 53/100 [00:11<00:09,  4.70it/s]Measuring inference with batch_size=8:  54%|█████▍    | 54/100 [00:11<00:09,  4.81it/s]Measuring inference with batch_size=8:  55%|█████▌    | 55/100 [00:11<00:09,  4.80it/s]Measuring inference with batch_size=8:  56%|█████▌    | 56/100 [00:12<00:09,  4.87it/s]Measuring inference with batch_size=8:  57%|█████▋    | 57/100 [00:12<00:08,  4.95it/s]Measuring inference with batch_size=8:  58%|█████▊    | 58/100 [00:12<00:08,  5.02it/s]Measuring inference with batch_size=8:  59%|█████▉    | 59/100 [00:12<00:08,  4.89it/s]Measuring inference with batch_size=8:  60%|██████    | 60/100 [00:12<00:08,  4.82it/s]Measuring inference with batch_size=8:  61%|██████    | 61/100 [00:13<00:08,  4.74it/s]Measuring inference with batch_size=8:  62%|██████▏   | 62/100 [00:13<00:07,  4.79it/s]Measuring inference with batch_size=8:  63%|██████▎   | 63/100 [00:13<00:07,  4.89it/s]Measuring inference with batch_size=8:  64%|██████▍   | 64/100 [00:13<00:07,  4.92it/s]Measuring inference with batch_size=8:  65%|██████▌   | 65/100 [00:13<00:07,  4.79it/s]Measuring inference with batch_size=8:  66%|██████▌   | 66/100 [00:14<00:06,  4.91it/s]Measuring inference with batch_size=8:  67%|██████▋   | 67/100 [00:14<00:06,  4.95it/s]Measuring inference with batch_size=8:  68%|██████▊   | 68/100 [00:14<00:06,  4.85it/s]Measuring inference with batch_size=8:  69%|██████▉   | 69/100 [00:14<00:06,  4.89it/s]Measuring inference with batch_size=8:  70%|███████   | 70/100 [00:14<00:06,  4.90it/s]Measuring inference with batch_size=8:  71%|███████   | 71/100 [00:15<00:05,  4.94it/s]Measuring inference with batch_size=8:  72%|███████▏  | 72/100 [00:15<00:05,  4.98it/s]Measuring inference with batch_size=8:  73%|███████▎  | 73/100 [00:15<00:05,  4.98it/s]Measuring inference with batch_size=8:  74%|███████▍  | 74/100 [00:15<00:05,  5.00it/s]Measuring inference with batch_size=8:  75%|███████▌  | 75/100 [00:15<00:04,  5.01it/s]Measuring inference with batch_size=8:  76%|███████▌  | 76/100 [00:16<00:04,  5.05it/s]Measuring inference with batch_size=8:  77%|███████▋  | 77/100 [00:16<00:04,  4.99it/s]Measuring inference with batch_size=8:  78%|███████▊  | 78/100 [00:16<00:04,  5.03it/s]Measuring inference with batch_size=8:  79%|███████▉  | 79/100 [00:16<00:04,  5.02it/s]Measuring inference with batch_size=8:  80%|████████  | 80/100 [00:17<00:04,  4.87it/s]Measuring inference with batch_size=8:  81%|████████  | 81/100 [00:17<00:04,  4.71it/s]Measuring inference with batch_size=8:  82%|████████▏ | 82/100 [00:17<00:04,  4.47it/s]Measuring inference with batch_size=8:  83%|████████▎ | 83/100 [00:17<00:03,  4.48it/s]Measuring inference with batch_size=8:  84%|████████▍ | 84/100 [00:17<00:03,  4.52it/s]Measuring inference with batch_size=8:  85%|████████▌ | 85/100 [00:18<00:03,  4.65it/s]Measuring inference with batch_size=8:  86%|████████▌ | 86/100 [00:18<00:03,  4.63it/s]Measuring inference with batch_size=8:  87%|████████▋ | 87/100 [00:18<00:02,  4.61it/s]Measuring inference with batch_size=8:  88%|████████▊ | 88/100 [00:18<00:02,  4.56it/s]Measuring inference with batch_size=8:  89%|████████▉ | 89/100 [00:19<00:02,  4.56it/s]Measuring inference with batch_size=8:  90%|█████████ | 90/100 [00:19<00:02,  4.54it/s]Measuring inference with batch_size=8:  91%|█████████ | 91/100 [00:19<00:02,  4.40it/s]Measuring inference with batch_size=8:  92%|█████████▏| 92/100 [00:19<00:01,  4.24it/s]Measuring inference with batch_size=8:  93%|█████████▎| 93/100 [00:19<00:01,  4.32it/s]Measuring inference with batch_size=8:  94%|█████████▍| 94/100 [00:20<00:01,  4.33it/s]Measuring inference with batch_size=8:  95%|█████████▌| 95/100 [00:20<00:01,  4.35it/s]Measuring inference with batch_size=8:  96%|█████████▌| 96/100 [00:20<00:00,  4.30it/s]Measuring inference with batch_size=8:  97%|█████████▋| 97/100 [00:20<00:00,  4.30it/s]Measuring inference with batch_size=8:  98%|█████████▊| 98/100 [00:21<00:00,  4.48it/s]Measuring inference with batch_size=8:  99%|█████████▉| 99/100 [00:21<00:00,  4.39it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [00:21<00:00,  4.35it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [00:21<00:00,  4.64it/s]
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "9.873 \xB5s +/- 4.145 \xB5s [4.768 \xB5s, 33.379 \xB5s]"
      batches_per_second: 116.33 K +/- 39.92 K [29.96 K, 209.72 K]
    metrics:
      batches_per_second_max: 209715.2
      batches_per_second_mean: 116330.49035257033
      batches_per_second_min: 29959.314285714285
      batches_per_second_std: 39923.377009784796
      seconds_per_batch_max: 3.337860107421875e-05
      seconds_per_batch_mean: 9.872913360595702e-06
      seconds_per_batch_min: 4.76837158203125e-06
      seconds_per_batch_std: 4.145343595423436e-06
  gpu_to_cpu:
    human_readable:
      batch_latency: "291.023 \xB5s +/- 143.604 \xB5s [208.378 \xB5s, 1.370 ms]"
      batches_per_second: 3.70 K +/- 652.68 [729.82, 4.80 K]
    metrics:
      batches_per_second_max: 4798.974828375286
      batches_per_second_mean: 3703.605607315591
      batches_per_second_min: 729.8249521489473
      batches_per_second_std: 652.6817434703397
      seconds_per_batch_max: 0.0013701915740966797
      seconds_per_batch_mean: 0.0002910232543945313
      seconds_per_batch_min: 0.00020837783813476562
      seconds_per_batch_std: 0.00014360365472912174
  on_device_inference:
    human_readable:
      batch_latency: 214.314 ms +/- 16.024 ms [190.746 ms, 255.057 ms]
      batches_per_second: 4.69 +/- 0.34 [3.92, 5.24]
    metrics:
      batches_per_second_max: 5.242571998895065
      batches_per_second_mean: 4.691769515694409
      batches_per_second_min: 3.9206870894046295
      batches_per_second_std: 0.34481980998434575
      seconds_per_batch_max: 0.25505733489990234
      seconds_per_batch_mean: 0.21431397199630736
      seconds_per_batch_min: 0.19074606895446777
      seconds_per_batch_std: 0.01602402268296239
  total:
    human_readable:
      batch_latency: 214.615 ms +/- 16.038 ms [191.080 ms, 255.312 ms]
      batches_per_second: 4.69 +/- 0.34 [3.92, 5.23]
    metrics:
      batches_per_second_max: 5.2334205922794546
      batches_per_second_mean: 4.6851703001055425
      batches_per_second_min: 3.9167768588867222
      batches_per_second_std: 0.3442278508296459
      seconds_per_batch_max: 0.2553119659423828
      seconds_per_batch_mean: 0.2146148681640625
      seconds_per_batch_min: 0.19107961654663086
      seconds_per_batch_std: 0.016038174787993447

Energy results (batch_size=1):
  joules: 20.784355624024073
  kWh: 5.773432117784464e-06

Warming up with batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=8:  10%|█         | 1/10 [00:00<00:06,  1.34it/s]Warming up with batch_size=8:  20%|██        | 2/10 [00:01<00:05,  1.34it/s]Warming up with batch_size=8:  30%|███       | 3/10 [00:02<00:05,  1.35it/s]Warming up with batch_size=8:  40%|████      | 4/10 [00:02<00:04,  1.35it/s]Warming up with batch_size=8:  50%|█████     | 5/10 [00:03<00:03,  1.31it/s]Warming up with batch_size=8:  60%|██████    | 6/10 [00:04<00:03,  1.32it/s]Warming up with batch_size=8:  70%|███████   | 7/10 [00:05<00:02,  1.33it/s]Warming up with batch_size=8:  80%|████████  | 8/10 [00:06<00:01,  1.32it/s]Warming up with batch_size=8:  90%|█████████ | 9/10 [00:06<00:00,  1.33it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:07<00:00,  1.33it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:07<00:00,  1.33it/s]
Measuring inference with batch_size=8:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=8:   1%|          | 1/100 [00:00<01:13,  1.35it/s]Measuring inference with batch_size=8:   2%|▏         | 2/100 [00:01<01:15,  1.30it/s]Measuring inference with batch_size=8:   3%|▎         | 3/100 [00:02<01:15,  1.29it/s]Measuring inference with batch_size=8:   4%|▍         | 4/100 [00:03<01:14,  1.29it/s]Measuring inference with batch_size=8:   5%|▌         | 5/100 [00:03<01:14,  1.27it/s]Measuring inference with batch_size=8:   6%|▌         | 6/100 [00:04<01:15,  1.24it/s]Measuring inference with batch_size=8:   7%|▋         | 7/100 [00:05<01:13,  1.27it/s]Measuring inference with batch_size=8:   8%|▊         | 8/100 [00:06<01:10,  1.30it/s]Measuring inference with batch_size=8:   9%|▉         | 9/100 [00:07<01:12,  1.25it/s]Measuring inference with batch_size=8:  10%|█         | 10/100 [00:07<01:14,  1.22it/s]Measuring inference with batch_size=8:  11%|█         | 11/100 [00:08<01:13,  1.21it/s]Measuring inference with batch_size=8:  12%|█▏        | 12/100 [00:09<01:13,  1.20it/s]Measuring inference with batch_size=8:  13%|█▎        | 13/100 [00:10<01:13,  1.19it/s]Measuring inference with batch_size=8:  14%|█▍        | 14/100 [00:11<01:10,  1.21it/s]Measuring inference with batch_size=8:  15%|█▌        | 15/100 [00:12<01:12,  1.17it/s]Measuring inference with batch_size=8:  16%|█▌        | 16/100 [00:12<01:09,  1.21it/s]Measuring inference with batch_size=8:  17%|█▋        | 17/100 [00:13<01:06,  1.25it/s]Measuring inference with batch_size=8:  18%|█▊        | 18/100 [00:14<01:04,  1.27it/s]Measuring inference with batch_size=8:  19%|█▉        | 19/100 [00:15<01:02,  1.30it/s]Measuring inference with batch_size=8:  20%|██        | 20/100 [00:15<01:00,  1.31it/s]Measuring inference with batch_size=8:  21%|██        | 21/100 [00:16<00:59,  1.32it/s]Measuring inference with batch_size=8:  22%|██▏       | 22/100 [00:17<00:59,  1.31it/s]Measuring inference with batch_size=8:  23%|██▎       | 23/100 [00:18<00:59,  1.29it/s]Measuring inference with batch_size=8:  24%|██▍       | 24/100 [00:19<01:01,  1.23it/s]Measuring inference with batch_size=8:  25%|██▌       | 25/100 [00:20<01:02,  1.21it/s]Measuring inference with batch_size=8:  26%|██▌       | 26/100 [00:20<00:59,  1.25it/s]Measuring inference with batch_size=8:  27%|██▋       | 27/100 [00:21<00:57,  1.28it/s]Measuring inference with batch_size=8:  28%|██▊       | 28/100 [00:22<00:55,  1.30it/s]Measuring inference with batch_size=8:  29%|██▉       | 29/100 [00:23<00:54,  1.31it/s]Measuring inference with batch_size=8:  30%|███       | 30/100 [00:23<00:56,  1.25it/s]Measuring inference with batch_size=8:  31%|███       | 31/100 [00:24<00:56,  1.23it/s]Measuring inference with batch_size=8:  32%|███▏      | 32/100 [00:25<00:54,  1.25it/s]Measuring inference with batch_size=8:  33%|███▎      | 33/100 [00:26<00:55,  1.20it/s]Measuring inference with batch_size=8:  34%|███▍      | 34/100 [00:27<00:56,  1.17it/s]Measuring inference with batch_size=8:  35%|███▌      | 35/100 [00:28<00:56,  1.16it/s]Measuring inference with batch_size=8:  36%|███▌      | 36/100 [00:28<00:53,  1.19it/s]Measuring inference with batch_size=8:  37%|███▋      | 37/100 [00:29<00:51,  1.22it/s]Measuring inference with batch_size=8:  38%|███▊      | 38/100 [00:30<00:50,  1.23it/s]Measuring inference with batch_size=8:  39%|███▉      | 39/100 [00:31<00:48,  1.25it/s]Measuring inference with batch_size=8:  40%|████      | 40/100 [00:32<00:47,  1.26it/s]Measuring inference with batch_size=8:  41%|████      | 41/100 [00:32<00:46,  1.26it/s]Measuring inference with batch_size=8:  42%|████▏     | 42/100 [00:33<00:45,  1.29it/s]Measuring inference with batch_size=8:  43%|████▎     | 43/100 [00:34<00:43,  1.30it/s]Measuring inference with batch_size=8:  44%|████▍     | 44/100 [00:35<00:42,  1.31it/s]Measuring inference with batch_size=8:  45%|████▌     | 45/100 [00:35<00:42,  1.31it/s]Measuring inference with batch_size=8:  46%|████▌     | 46/100 [00:36<00:41,  1.32it/s]Measuring inference with batch_size=8:  47%|████▋     | 47/100 [00:37<00:40,  1.32it/s]Measuring inference with batch_size=8:  48%|████▊     | 48/100 [00:38<00:39,  1.33it/s]Measuring inference with batch_size=8:  49%|████▉     | 49/100 [00:38<00:38,  1.33it/s]Measuring inference with batch_size=8:  50%|█████     | 50/100 [00:39<00:40,  1.23it/s]Measuring inference with batch_size=8:  51%|█████     | 51/100 [00:40<00:41,  1.19it/s]Measuring inference with batch_size=8:  52%|█████▏    | 52/100 [00:41<00:39,  1.21it/s]Measuring inference with batch_size=8:  53%|█████▎    | 53/100 [00:42<00:37,  1.25it/s]Measuring inference with batch_size=8:  54%|█████▍    | 54/100 [00:43<00:36,  1.28it/s]Measuring inference with batch_size=8:  55%|█████▌    | 55/100 [00:43<00:34,  1.29it/s]Measuring inference with batch_size=8:  56%|█████▌    | 56/100 [00:44<00:33,  1.31it/s]Measuring inference with batch_size=8:  57%|█████▋    | 57/100 [00:45<00:32,  1.32it/s]Measuring inference with batch_size=8:  58%|█████▊    | 58/100 [00:46<00:31,  1.32it/s]Measuring inference with batch_size=8:  59%|█████▉    | 59/100 [00:46<00:30,  1.33it/s]Measuring inference with batch_size=8:  60%|██████    | 60/100 [00:47<00:30,  1.33it/s]Measuring inference with batch_size=8:  61%|██████    | 61/100 [00:48<00:29,  1.34it/s]Measuring inference with batch_size=8:  62%|██████▏   | 62/100 [00:48<00:28,  1.34it/s]Measuring inference with batch_size=8:  63%|██████▎   | 63/100 [00:49<00:27,  1.34it/s]Measuring inference with batch_size=8:  64%|██████▍   | 64/100 [00:50<00:26,  1.34it/s]Measuring inference with batch_size=8:  65%|██████▌   | 65/100 [00:51<00:26,  1.34it/s]Measuring inference with batch_size=8:  66%|██████▌   | 66/100 [00:51<00:25,  1.34it/s]Measuring inference with batch_size=8:  67%|██████▋   | 67/100 [00:52<00:24,  1.34it/s]Measuring inference with batch_size=8:  68%|██████▊   | 68/100 [00:53<00:23,  1.34it/s]Measuring inference with batch_size=8:  69%|██████▉   | 69/100 [00:54<00:23,  1.34it/s]Measuring inference with batch_size=8:  70%|███████   | 70/100 [00:54<00:22,  1.34it/s]Measuring inference with batch_size=8:  71%|███████   | 71/100 [00:55<00:21,  1.34it/s]Measuring inference with batch_size=8:  72%|███████▏  | 72/100 [00:56<00:20,  1.34it/s]Measuring inference with batch_size=8:  73%|███████▎  | 73/100 [00:57<00:20,  1.34it/s]Measuring inference with batch_size=8:  74%|███████▍  | 74/100 [00:57<00:19,  1.34it/s]Measuring inference with batch_size=8:  75%|███████▌  | 75/100 [00:58<00:18,  1.34it/s]Measuring inference with batch_size=8:  76%|███████▌  | 76/100 [00:59<00:17,  1.34it/s]Measuring inference with batch_size=8:  77%|███████▋  | 77/100 [01:00<00:17,  1.34it/s]Measuring inference with batch_size=8:  78%|███████▊  | 78/100 [01:00<00:16,  1.34it/s]Measuring inference with batch_size=8:  79%|███████▉  | 79/100 [01:01<00:15,  1.34it/s]Measuring inference with batch_size=8:  80%|████████  | 80/100 [01:02<00:14,  1.34it/s]Measuring inference with batch_size=8:  81%|████████  | 81/100 [01:03<00:14,  1.34it/s]Measuring inference with batch_size=8:  82%|████████▏ | 82/100 [01:03<00:13,  1.34it/s]Measuring inference with batch_size=8:  83%|████████▎ | 83/100 [01:04<00:12,  1.34it/s]Measuring inference with batch_size=8:  84%|████████▍ | 84/100 [01:05<00:11,  1.34it/s]Measuring inference with batch_size=8:  85%|████████▌ | 85/100 [01:06<00:11,  1.34it/s]Measuring inference with batch_size=8:  86%|████████▌ | 86/100 [01:06<00:10,  1.34it/s]Measuring inference with batch_size=8:  87%|████████▋ | 87/100 [01:07<00:09,  1.34it/s]Measuring inference with batch_size=8:  88%|████████▊ | 88/100 [01:08<00:08,  1.34it/s]Measuring inference with batch_size=8:  89%|████████▉ | 89/100 [01:09<00:08,  1.34it/s]Measuring inference with batch_size=8:  90%|█████████ | 90/100 [01:09<00:07,  1.34it/s]Measuring inference with batch_size=8:  91%|█████████ | 91/100 [01:10<00:06,  1.34it/s]Measuring inference with batch_size=8:  92%|█████████▏| 92/100 [01:11<00:05,  1.34it/s]Measuring inference with batch_size=8:  93%|█████████▎| 93/100 [01:12<00:05,  1.34it/s]Measuring inference with batch_size=8:  94%|█████████▍| 94/100 [01:12<00:04,  1.34it/s]Measuring inference with batch_size=8:  95%|█████████▌| 95/100 [01:13<00:03,  1.34it/s]Measuring inference with batch_size=8:  96%|█████████▌| 96/100 [01:14<00:02,  1.34it/s]Measuring inference with batch_size=8:  97%|█████████▋| 97/100 [01:15<00:02,  1.34it/s]Measuring inference with batch_size=8:  98%|█████████▊| 98/100 [01:15<00:01,  1.34it/s]Measuring inference with batch_size=8:  99%|█████████▉| 99/100 [01:16<00:00,  1.34it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [01:17<00:00,  1.34it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [01:17<00:00,  1.29it/s]Timing results (batch_size=8):
  cpu_to_gpu:
    human_readable:
      batch_latency: "12.312 \xB5s +/- 7.127 \xB5s [5.722 \xB5s, 50.545 \xB5s]"
      batches_per_second: 98.72 K +/- 37.14 K [19.78 K, 174.76 K]
    metrics:
      batches_per_second_max: 174762.66666666666
      batches_per_second_mean: 98717.61071733765
      batches_per_second_min: 19784.45283018868
      batches_per_second_std: 37137.551155791465
      seconds_per_batch_max: 5.054473876953125e-05
      seconds_per_batch_mean: 1.2311935424804688e-05
      seconds_per_batch_min: 5.7220458984375e-06
      seconds_per_batch_std: 7.126962643989356e-06
  gpu_to_cpu:
    human_readable:
      batch_latency: "951.707 \xB5s +/- 239.571 \xB5s [684.977 \xB5s, 1.852 ms]"
      batches_per_second: 1.11 K +/- 223.18 [540.09, 1.46 K]
    metrics:
      batches_per_second_max: 1459.9039331709016
      batches_per_second_mean: 1105.3604860516969
      batches_per_second_min: 540.085500901365
      batches_per_second_std: 223.18448647329717
      seconds_per_batch_max: 0.0018515586853027344
      seconds_per_batch_mean: 0.0009517073631286621
      seconds_per_batch_min: 0.0006849765777587891
      seconds_per_batch_std: 0.00023957057725363168
  on_device_inference:
    human_readable:
      batch_latency: 771.225 ms +/- 50.720 ms [737.320 ms, 946.486 ms]
      batches_per_second: 1.30 +/- 0.08 [1.06, 1.36]
    metrics:
      batches_per_second_max: 1.3562633223650173
      batches_per_second_mean: 1.3016831930683463
      batches_per_second_min: 1.0565394016047454
      batches_per_second_std: 0.07683548248398872
      seconds_per_batch_max: 0.946486234664917
      seconds_per_batch_mean: 0.771225426197052
      seconds_per_batch_min: 0.7373199462890625
      seconds_per_batch_std: 0.05071952247689826
  total:
    human_readable:
      batch_latency: 772.189 ms +/- 50.795 ms [738.600 ms, 948.067 ms]
      batches_per_second: 1.30 +/- 0.08 [1.05, 1.35]
    metrics:
      batches_per_second_max: 1.3539127793453205
      batches_per_second_mean: 1.3000606979425728
      batches_per_second_min: 1.0547775646916042
      batches_per_second_std: 0.07676082985378596
      seconds_per_batch_max: 0.9480671882629395
      seconds_per_batch_mean: 0.7721894454956054
      seconds_per_batch_min: 0.7386000156402588
      seconds_per_batch_std: 0.050794866386548614

Energy results (batch_size=8):
  joules: 16.062201796515783
  kWh: 4.461722721254384e-06

learner.infer:
  device: cuda
  energy:
    batch_size_1:
      joules: 20.784355624024073
      kWh: 5.773432117784464e-06
    batch_size_8:
      joules: 16.062201796515783
      kWh: 4.461722721254384e-06
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 2.63 GB
      total: 31.17 GB
      used: 28.06 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  max_inference_memory: 10273395200
  post_inference_memory: 10256752640
  pre_inference_memory: 7125865984
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "9.873 \xB5s +/- 4.145 \xB5s [4.768 \xB5s, 33.379 \xB5s]"
          batches_per_second: 116.33 K +/- 39.92 K [29.96 K, 209.72 K]
        metrics:
          batches_per_second_max: 209715.2
          batches_per_second_mean: 116330.49035257033
          batches_per_second_min: 29959.314285714285
          batches_per_second_std: 39923.377009784796
          seconds_per_batch_max: 3.337860107421875e-05
          seconds_per_batch_mean: 9.872913360595702e-06
          seconds_per_batch_min: 4.76837158203125e-06
          seconds_per_batch_std: 4.145343595423436e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: "291.023 \xB5s +/- 143.604 \xB5s [208.378 \xB5s, 1.370 ms]"
          batches_per_second: 3.70 K +/- 652.68 [729.82, 4.80 K]
        metrics:
          batches_per_second_max: 4798.974828375286
          batches_per_second_mean: 3703.605607315591
          batches_per_second_min: 729.8249521489473
          batches_per_second_std: 652.6817434703397
          seconds_per_batch_max: 0.0013701915740966797
          seconds_per_batch_mean: 0.0002910232543945313
          seconds_per_batch_min: 0.00020837783813476562
          seconds_per_batch_std: 0.00014360365472912174
      on_device_inference:
        human_readable:
          batch_latency: 214.314 ms +/- 16.024 ms [190.746 ms, 255.057 ms]
          batches_per_second: 4.69 +/- 0.34 [3.92, 5.24]
        metrics:
          batches_per_second_max: 5.242571998895065
          batches_per_second_mean: 4.691769515694409
          batches_per_second_min: 3.9206870894046295
          batches_per_second_std: 0.34481980998434575
          seconds_per_batch_max: 0.25505733489990234
          seconds_per_batch_mean: 0.21431397199630736
          seconds_per_batch_min: 0.19074606895446777
          seconds_per_batch_std: 0.01602402268296239
      total:
        human_readable:
          batch_latency: 214.615 ms +/- 16.038 ms [191.080 ms, 255.312 ms]
          batches_per_second: 4.69 +/- 0.34 [3.92, 5.23]
        metrics:
          batches_per_second_max: 5.2334205922794546
          batches_per_second_mean: 4.6851703001055425
          batches_per_second_min: 3.9167768588867222
          batches_per_second_std: 0.3442278508296459
          seconds_per_batch_max: 0.2553119659423828
          seconds_per_batch_mean: 0.2146148681640625
          seconds_per_batch_min: 0.19107961654663086
          seconds_per_batch_std: 0.016038174787993447
    batch_size_8:
      cpu_to_gpu:
        human_readable:
          batch_latency: "12.312 \xB5s +/- 7.127 \xB5s [5.722 \xB5s, 50.545 \xB5s]"
          batches_per_second: 98.72 K +/- 37.14 K [19.78 K, 174.76 K]
        metrics:
          batches_per_second_max: 174762.66666666666
          batches_per_second_mean: 98717.61071733765
          batches_per_second_min: 19784.45283018868
          batches_per_second_std: 37137.551155791465
          seconds_per_batch_max: 5.054473876953125e-05
          seconds_per_batch_mean: 1.2311935424804688e-05
          seconds_per_batch_min: 5.7220458984375e-06
          seconds_per_batch_std: 7.126962643989356e-06
      gpu_to_cpu:
        human_readable:
          batch_latency: "951.707 \xB5s +/- 239.571 \xB5s [684.977 \xB5s, 1.852 ms]"
          batches_per_second: 1.11 K +/- 223.18 [540.09, 1.46 K]
        metrics:
          batches_per_second_max: 1459.9039331709016
          batches_per_second_mean: 1105.3604860516969
          batches_per_second_min: 540.085500901365
          batches_per_second_std: 223.18448647329717
          seconds_per_batch_max: 0.0018515586853027344
          seconds_per_batch_mean: 0.0009517073631286621
          seconds_per_batch_min: 0.0006849765777587891
          seconds_per_batch_std: 0.00023957057725363168
      on_device_inference:
        human_readable:
          batch_latency: 771.225 ms +/- 50.720 ms [737.320 ms, 946.486 ms]
          batches_per_second: 1.30 +/- 0.08 [1.06, 1.36]
        metrics:
          batches_per_second_max: 1.3562633223650173
          batches_per_second_mean: 1.3016831930683463
          batches_per_second_min: 1.0565394016047454
          batches_per_second_std: 0.07683548248398872
          seconds_per_batch_max: 0.946486234664917
          seconds_per_batch_mean: 0.771225426197052
          seconds_per_batch_min: 0.7373199462890625
          seconds_per_batch_std: 0.05071952247689826
      total:
        human_readable:
          batch_latency: 772.189 ms +/- 50.795 ms [738.600 ms, 948.067 ms]
          batches_per_second: 1.30 +/- 0.08 [1.05, 1.35]
        metrics:
          batches_per_second_max: 1.3539127793453205
          batches_per_second_mean: 1.3000606979425728
          batches_per_second_min: 1.0547775646916042
          batches_per_second_std: 0.07676082985378596
          seconds_per_batch_max: 0.9480671882629395
          seconds_per_batch_mean: 0.7721894454956054
          seconds_per_batch_min: 0.7386000156402588
          seconds_per_batch_std: 0.050794866386548614

== Benchmarking model directly ==

Machine info:
  cpu:
    architecture: aarch64
    cores:
      physical: 8
      total: 8
    frequency: 2.27 GHz
    model: ARMv8 Processor rev 0 (v8l)
  gpus: null
  memory:
    available: 2.59 GB
    total: 31.17 GB
    used: 28.10 GB
  system:
    node: xavier
    release: 4.9.140-tegra
    system: Linux

Model device: cuda:0
Model parameters: 6153432 (6.15 M)
Model FLOPs: 1032310310 (1.03 G)
Allocated GPU memory prior to inference: 7514144768 (7.00 GB)
Allocated GPU memory after to inference: 10256752640 (9.55 GB)
Max allocated GPU memory during inference: 10273394688 (9.57 GB)
Warming up with batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=8:  10%|█         | 1/10 [00:00<00:01,  4.66it/s]Warming up with batch_size=8:  20%|██        | 2/10 [00:00<00:01,  4.98it/s]Warming up with batch_size=8:  30%|███       | 3/10 [00:00<00:01,  5.13it/s]Warming up with batch_size=8:  40%|████      | 4/10 [00:00<00:01,  5.20it/s]Warming up with batch_size=8:  50%|█████     | 5/10 [00:00<00:00,  5.21it/s]Warming up with batch_size=8:  60%|██████    | 6/10 [00:01<00:00,  5.24it/s]Warming up with batch_size=8:  70%|███████   | 7/10 [00:01<00:00,  5.25it/s]Warming up with batch_size=8:  80%|████████  | 8/10 [00:01<00:00,  5.23it/s]Warming up with batch_size=8:  90%|█████████ | 9/10 [00:01<00:00,  5.26it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:01<00:00,  5.28it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:01<00:00,  5.21it/s]
Measuring inference with batch_size=8:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=8:   1%|          | 1/100 [00:00<00:18,  5.28it/s]Measuring inference with batch_size=8:   2%|▏         | 2/100 [00:00<00:18,  5.29it/s]Measuring inference with batch_size=8:   3%|▎         | 3/100 [00:00<00:18,  5.36it/s]Measuring inference with batch_size=8:   4%|▍         | 4/100 [00:00<00:17,  5.35it/s]Measuring inference with batch_size=8:   5%|▌         | 5/100 [00:00<00:17,  5.39it/s]Measuring inference with batch_size=8:   6%|▌         | 6/100 [00:01<00:17,  5.46it/s]Measuring inference with batch_size=8:   7%|▋         | 7/100 [00:01<00:16,  5.50it/s]Measuring inference with batch_size=8:   8%|▊         | 8/100 [00:01<00:16,  5.51it/s]Measuring inference with batch_size=8:   9%|▉         | 9/100 [00:01<00:16,  5.54it/s]Measuring inference with batch_size=8:  10%|█         | 10/100 [00:01<00:16,  5.57it/s]Measuring inference with batch_size=8:  11%|█         | 11/100 [00:02<00:16,  5.56it/s]Measuring inference with batch_size=8:  12%|█▏        | 12/100 [00:02<00:15,  5.58it/s]Measuring inference with batch_size=8:  13%|█▎        | 13/100 [00:02<00:15,  5.59it/s]Measuring inference with batch_size=8:  14%|█▍        | 14/100 [00:02<00:15,  5.57it/s]Measuring inference with batch_size=8:  15%|█▌        | 15/100 [00:02<00:15,  5.59it/s]Measuring inference with batch_size=8:  16%|█▌        | 16/100 [00:02<00:15,  5.59it/s]Measuring inference with batch_size=8:  17%|█▋        | 17/100 [00:03<00:14,  5.60it/s]Measuring inference with batch_size=8:  18%|█▊        | 18/100 [00:03<00:14,  5.60it/s]Measuring inference with batch_size=8:  19%|█▉        | 19/100 [00:03<00:14,  5.59it/s]Measuring inference with batch_size=8:  20%|██        | 20/100 [00:03<00:14,  5.60it/s]Measuring inference with batch_size=8:  21%|██        | 21/100 [00:03<00:14,  5.58it/s]Measuring inference with batch_size=8:  22%|██▏       | 22/100 [00:03<00:13,  5.58it/s]Measuring inference with batch_size=8:  23%|██▎       | 23/100 [00:04<00:13,  5.59it/s]Measuring inference with batch_size=8:  24%|██▍       | 24/100 [00:04<00:13,  5.60it/s]Measuring inference with batch_size=8:  25%|██▌       | 25/100 [00:04<00:13,  5.56it/s]Measuring inference with batch_size=8:  26%|██▌       | 26/100 [00:04<00:13,  5.58it/s]Measuring inference with batch_size=8:  27%|██▋       | 27/100 [00:04<00:13,  5.60it/s]Measuring inference with batch_size=8:  28%|██▊       | 28/100 [00:05<00:12,  5.58it/s]Measuring inference with batch_size=8:  29%|██▉       | 29/100 [00:05<00:12,  5.60it/s]Measuring inference with batch_size=8:  30%|███       | 30/100 [00:05<00:12,  5.59it/s]Measuring inference with batch_size=8:  31%|███       | 31/100 [00:05<00:12,  5.61it/s]Measuring inference with batch_size=8:  32%|███▏      | 32/100 [00:05<00:12,  5.60it/s]Measuring inference with batch_size=8:  33%|███▎      | 33/100 [00:05<00:12,  5.58it/s]Measuring inference with batch_size=8:  34%|███▍      | 34/100 [00:06<00:11,  5.60it/s]Measuring inference with batch_size=8:  35%|███▌      | 35/100 [00:06<00:11,  5.58it/s]Measuring inference with batch_size=8:  36%|███▌      | 36/100 [00:06<00:11,  5.59it/s]Measuring inference with batch_size=8:  37%|███▋      | 37/100 [00:06<00:11,  5.60it/s]Measuring inference with batch_size=8:  38%|███▊      | 38/100 [00:06<00:11,  5.60it/s]Measuring inference with batch_size=8:  39%|███▉      | 39/100 [00:07<00:10,  5.57it/s]Measuring inference with batch_size=8:  40%|████      | 40/100 [00:07<00:10,  5.59it/s]Measuring inference with batch_size=8:  41%|████      | 41/100 [00:07<00:10,  5.60it/s]Measuring inference with batch_size=8:  42%|████▏     | 42/100 [00:07<00:10,  5.56it/s]Measuring inference with batch_size=8:  43%|████▎     | 43/100 [00:07<00:10,  5.57it/s]Measuring inference with batch_size=8:  44%|████▍     | 44/100 [00:07<00:10,  5.57it/s]Measuring inference with batch_size=8:  45%|████▌     | 45/100 [00:08<00:09,  5.59it/s]Measuring inference with batch_size=8:  46%|████▌     | 46/100 [00:08<00:09,  5.60it/s]Measuring inference with batch_size=8:  47%|████▋     | 47/100 [00:08<00:09,  5.60it/s]Measuring inference with batch_size=8:  48%|████▊     | 48/100 [00:08<00:09,  5.61it/s]Measuring inference with batch_size=8:  49%|████▉     | 49/100 [00:08<00:09,  5.62it/s]Measuring inference with batch_size=8:  50%|█████     | 50/100 [00:08<00:08,  5.60it/s]Measuring inference with batch_size=8:  51%|█████     | 51/100 [00:09<00:08,  5.61it/s]Measuring inference with batch_size=8:  52%|█████▏    | 52/100 [00:09<00:08,  5.60it/s]Measuring inference with batch_size=8:  53%|█████▎    | 53/100 [00:09<00:08,  5.58it/s]Measuring inference with batch_size=8:  54%|█████▍    | 54/100 [00:09<00:08,  5.60it/s]Measuring inference with batch_size=8:  55%|█████▌    | 55/100 [00:09<00:08,  5.59it/s]Measuring inference with batch_size=8:  56%|█████▌    | 56/100 [00:10<00:07,  5.57it/s]Measuring inference with batch_size=8:  57%|█████▋    | 57/100 [00:10<00:07,  5.58it/s]Measuring inference with batch_size=8:  58%|█████▊    | 58/100 [00:10<00:07,  5.58it/s]Measuring inference with batch_size=8:  59%|█████▉    | 59/100 [00:10<00:07,  5.60it/s]Measuring inference with batch_size=8:  60%|██████    | 60/100 [00:10<00:07,  5.61it/s]Measuring inference with batch_size=8:  61%|██████    | 61/100 [00:10<00:06,  5.61it/s]Measuring inference with batch_size=8:  62%|██████▏   | 62/100 [00:11<00:06,  5.60it/s]Measuring inference with batch_size=8:  63%|██████▎   | 63/100 [00:11<00:06,  5.60it/s]Measuring inference with batch_size=8:  64%|██████▍   | 64/100 [00:11<00:06,  5.60it/s]Measuring inference with batch_size=8:  65%|██████▌   | 65/100 [00:11<00:06,  5.61it/s]Measuring inference with batch_size=8:  66%|██████▌   | 66/100 [00:11<00:06,  5.62it/s]Measuring inference with batch_size=8:  67%|██████▋   | 67/100 [00:12<00:05,  5.60it/s]Measuring inference with batch_size=8:  68%|██████▊   | 68/100 [00:12<00:05,  5.61it/s]Measuring inference with batch_size=8:  69%|██████▉   | 69/100 [00:12<00:05,  5.62it/s]Measuring inference with batch_size=8:  70%|███████   | 70/100 [00:12<00:05,  5.60it/s]Measuring inference with batch_size=8:  71%|███████   | 71/100 [00:12<00:05,  5.61it/s]Measuring inference with batch_size=8:  72%|███████▏  | 72/100 [00:12<00:04,  5.61it/s]Measuring inference with batch_size=8:  73%|███████▎  | 73/100 [00:13<00:04,  5.61it/s]Measuring inference with batch_size=8:  74%|███████▍  | 74/100 [00:13<00:04,  5.61it/s]Measuring inference with batch_size=8:  75%|███████▌  | 75/100 [00:13<00:04,  5.61it/s]Measuring inference with batch_size=8:  76%|███████▌  | 76/100 [00:13<00:04,  5.62it/s]Measuring inference with batch_size=8:  77%|███████▋  | 77/100 [00:13<00:04,  5.62it/s]Measuring inference with batch_size=8:  78%|███████▊  | 78/100 [00:13<00:03,  5.61it/s]Measuring inference with batch_size=8:  79%|███████▉  | 79/100 [00:14<00:03,  5.63it/s]Measuring inference with batch_size=8:  80%|████████  | 80/100 [00:14<00:03,  5.61it/s]Measuring inference with batch_size=8:  81%|████████  | 81/100 [00:14<00:03,  5.60it/s]Measuring inference with batch_size=8:  82%|████████▏ | 82/100 [00:14<00:03,  5.62it/s]Measuring inference with batch_size=8:  83%|████████▎ | 83/100 [00:14<00:03,  5.64it/s]Measuring inference with batch_size=8:  84%|████████▍ | 84/100 [00:15<00:02,  5.62it/s]Measuring inference with batch_size=8:  85%|████████▌ | 85/100 [00:15<00:02,  5.64it/s]Measuring inference with batch_size=8:  86%|████████▌ | 86/100 [00:15<00:02,  5.63it/s]Measuring inference with batch_size=8:  87%|████████▋ | 87/100 [00:15<00:02,  5.65it/s]Measuring inference with batch_size=8:  88%|████████▊ | 88/100 [00:15<00:02,  5.64it/s]Measuring inference with batch_size=8:  89%|████████▉ | 89/100 [00:15<00:01,  5.61it/s]Measuring inference with batch_size=8:  90%|█████████ | 90/100 [00:16<00:01,  5.62it/s]Measuring inference with batch_size=8:  91%|█████████ | 91/100 [00:16<00:01,  5.63it/s]Measuring inference with batch_size=8:  92%|█████████▏| 92/100 [00:16<00:01,  5.62it/s]Measuring inference with batch_size=8:  93%|█████████▎| 93/100 [00:16<00:01,  5.60it/s]Measuring inference with batch_size=8:  94%|█████████▍| 94/100 [00:16<00:01,  5.60it/s]Measuring inference with batch_size=8:  95%|█████████▌| 95/100 [00:17<00:00,  5.58it/s]Measuring inference with batch_size=8:  96%|█████████▌| 96/100 [00:17<00:00,  5.61it/s]Measuring inference with batch_size=8:  97%|█████████▋| 97/100 [00:17<00:00,  5.61it/s]Measuring inference with batch_size=8:  98%|█████████▊| 98/100 [00:17<00:00,  5.59it/s]Measuring inference with batch_size=8:  99%|█████████▉| 99/100 [00:17<00:00,  5.61it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [00:17<00:00,  5.62it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [00:17<00:00,  5.59it/s]
Timing results (batch_size=1):
  cpu_to_gpu:
    human_readable:
      batch_latency: "322.738 \xB5s +/- 94.794 \xB5s [253.916 \xB5s, 715.971 \xB5\
        s]"
      batches_per_second: 3.25 K +/- 550.01 [1.40 K, 3.94 K]
    metrics:
      batches_per_second_max: 3938.313615023474
      batches_per_second_mean: 3254.90447580796
      batches_per_second_min: 1396.7046287046287
      batches_per_second_std: 550.0138813152251
      seconds_per_batch_max: 0.0007159709930419922
      seconds_per_batch_mean: 0.0003227376937866211
      seconds_per_batch_min: 0.00025391578674316406
      seconds_per_batch_std: 9.47935193683234e-05
  gpu_to_cpu:
    human_readable:
      batch_latency: "185.843 \xB5s +/- 64.899 \xB5s [140.429 \xB5s, 804.186 \xB5\
        s]"
      batches_per_second: 5.58 K +/- 678.89 [1.24 K, 7.12 K]
    metrics:
      batches_per_second_max: 7121.059422750424
      batches_per_second_mean: 5578.28716948968
      batches_per_second_min: 1243.493625852357
      batches_per_second_std: 678.8874966597248
      seconds_per_batch_max: 0.0008041858673095703
      seconds_per_batch_mean: 0.00018584251403808594
      seconds_per_batch_min: 0.0001404285430908203
      seconds_per_batch_std: 6.489890781723908e-05
  on_device_inference:
    human_readable:
      batch_latency: 177.597 ms +/- 2.275 ms [174.455 ms, 188.852 ms]
      batches_per_second: 5.63 +/- 0.07 [5.30, 5.73]
    metrics:
      batches_per_second_max: 5.732139611352476
      batches_per_second_mean: 5.631608835210532
      batches_per_second_min: 5.2951430620221585
      batches_per_second_std: 0.07007196342366251
      seconds_per_batch_max: 0.18885231018066406
      seconds_per_batch_mean: 0.17759745121002196
      seconds_per_batch_min: 0.174454927444458
      seconds_per_batch_std: 0.0022751832857059403
  total:
    human_readable:
      batch_latency: 178.106 ms +/- 2.323 ms [174.994 ms, 189.441 ms]
      batches_per_second: 5.62 +/- 0.07 [5.28, 5.71]
    metrics:
      batches_per_second_max: 5.7144896910938625
      batches_per_second_mean: 5.61555933291854
      batches_per_second_min: 5.278695952034554
      batches_per_second_std: 0.07105670929997435
      seconds_per_batch_max: 0.18944072723388672
      seconds_per_batch_mean: 0.17810603141784667
      seconds_per_batch_min: 0.17499375343322754
      seconds_per_batch_std: 0.002322665095073143

Energy results (batch_size=1):
  joules: 17.663767215220133
  kWh: 4.906602004227814e-06

Warming up with batch_size=8:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=8:  10%|█         | 1/10 [00:00<00:06,  1.38it/s]Warming up with batch_size=8:  20%|██        | 2/10 [00:01<00:05,  1.38it/s]Warming up with batch_size=8:  30%|███       | 3/10 [00:02<00:05,  1.38it/s]Warming up with batch_size=8:  40%|████      | 4/10 [00:02<00:04,  1.38it/s]Warming up with batch_size=8:  50%|█████     | 5/10 [00:03<00:03,  1.38it/s]Warming up with batch_size=8:  60%|██████    | 6/10 [00:04<00:02,  1.38it/s]Warming up with batch_size=8:  70%|███████   | 7/10 [00:05<00:02,  1.38it/s]Warming up with batch_size=8:  80%|████████  | 8/10 [00:05<00:01,  1.38it/s]Warming up with batch_size=8:  90%|█████████ | 9/10 [00:06<00:00,  1.38it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:07<00:00,  1.38it/s]Warming up with batch_size=8: 100%|██████████| 10/10 [00:07<00:00,  1.38it/s]
Measuring inference with batch_size=8:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=8:   1%|          | 1/100 [00:00<01:11,  1.38it/s]Measuring inference with batch_size=8:   2%|▏         | 2/100 [00:01<01:11,  1.38it/s]Measuring inference with batch_size=8:   3%|▎         | 3/100 [00:02<01:10,  1.38it/s]Measuring inference with batch_size=8:   4%|▍         | 4/100 [00:02<01:09,  1.38it/s]Measuring inference with batch_size=8:   5%|▌         | 5/100 [00:03<01:08,  1.38it/s]Measuring inference with batch_size=8:   6%|▌         | 6/100 [00:04<01:08,  1.38it/s]Measuring inference with batch_size=8:   7%|▋         | 7/100 [00:05<01:07,  1.38it/s]Measuring inference with batch_size=8:   8%|▊         | 8/100 [00:05<01:06,  1.38it/s]Measuring inference with batch_size=8:   9%|▉         | 9/100 [00:06<01:06,  1.38it/s]Measuring inference with batch_size=8:  10%|█         | 10/100 [00:07<01:05,  1.38it/s]Measuring inference with batch_size=8:  11%|█         | 11/100 [00:07<01:04,  1.38it/s]Measuring inference with batch_size=8:  12%|█▏        | 12/100 [00:08<01:03,  1.38it/s]Measuring inference with batch_size=8:  13%|█▎        | 13/100 [00:09<01:03,  1.37it/s]Measuring inference with batch_size=8:  14%|█▍        | 14/100 [00:10<01:02,  1.38it/s]Measuring inference with batch_size=8:  15%|█▌        | 15/100 [00:10<01:01,  1.38it/s]Measuring inference with batch_size=8:  16%|█▌        | 16/100 [00:11<01:00,  1.38it/s]Measuring inference with batch_size=8:  17%|█▋        | 17/100 [00:12<01:00,  1.38it/s]Measuring inference with batch_size=8:  18%|█▊        | 18/100 [00:13<00:59,  1.38it/s]Measuring inference with batch_size=8:  19%|█▉        | 19/100 [00:13<00:58,  1.38it/s]Measuring inference with batch_size=8:  20%|██        | 20/100 [00:14<00:58,  1.38it/s]Measuring inference with batch_size=8:  21%|██        | 21/100 [00:15<00:57,  1.38it/s]Measuring inference with batch_size=8:  22%|██▏       | 22/100 [00:15<00:56,  1.38it/s]Measuring inference with batch_size=8:  23%|██▎       | 23/100 [00:16<00:55,  1.38it/s]Measuring inference with batch_size=8:  24%|██▍       | 24/100 [00:17<00:55,  1.38it/s]Measuring inference with batch_size=8:  25%|██▌       | 25/100 [00:18<00:54,  1.38it/s]Measuring inference with batch_size=8:  26%|██▌       | 26/100 [00:18<00:53,  1.38it/s]Measuring inference with batch_size=8:  27%|██▋       | 27/100 [00:19<00:52,  1.38it/s]Measuring inference with batch_size=8:  28%|██▊       | 28/100 [00:20<00:52,  1.38it/s]Measuring inference with batch_size=8:  29%|██▉       | 29/100 [00:21<00:51,  1.38it/s]Measuring inference with batch_size=8:  30%|███       | 30/100 [00:21<00:50,  1.38it/s]Measuring inference with batch_size=8:  31%|███       | 31/100 [00:22<00:50,  1.38it/s]Measuring inference with batch_size=8:  32%|███▏      | 32/100 [00:23<00:49,  1.38it/s]Measuring inference with batch_size=8:  33%|███▎      | 33/100 [00:23<00:48,  1.38it/s]Measuring inference with batch_size=8:  34%|███▍      | 34/100 [00:24<00:47,  1.38it/s]Measuring inference with batch_size=8:  35%|███▌      | 35/100 [00:25<00:47,  1.38it/s]Measuring inference with batch_size=8:  36%|███▌      | 36/100 [00:26<00:46,  1.38it/s]Measuring inference with batch_size=8:  37%|███▋      | 37/100 [00:26<00:45,  1.38it/s]Measuring inference with batch_size=8:  38%|███▊      | 38/100 [00:27<00:45,  1.38it/s]Measuring inference with batch_size=8:  39%|███▉      | 39/100 [00:28<00:44,  1.38it/s]Measuring inference with batch_size=8:  40%|████      | 40/100 [00:29<00:43,  1.38it/s]Measuring inference with batch_size=8:  41%|████      | 41/100 [00:29<00:42,  1.38it/s]Measuring inference with batch_size=8:  42%|████▏     | 42/100 [00:30<00:42,  1.38it/s]Measuring inference with batch_size=8:  43%|████▎     | 43/100 [00:31<00:41,  1.38it/s]Measuring inference with batch_size=8:  44%|████▍     | 44/100 [00:31<00:40,  1.38it/s]Measuring inference with batch_size=8:  45%|████▌     | 45/100 [00:32<00:39,  1.38it/s]Measuring inference with batch_size=8:  46%|████▌     | 46/100 [00:33<00:39,  1.38it/s]Measuring inference with batch_size=8:  47%|████▋     | 47/100 [00:34<00:38,  1.38it/s]Measuring inference with batch_size=8:  48%|████▊     | 48/100 [00:34<00:37,  1.38it/s]Measuring inference with batch_size=8:  49%|████▉     | 49/100 [00:35<00:37,  1.38it/s]Measuring inference with batch_size=8:  50%|█████     | 50/100 [00:36<00:36,  1.38it/s]Measuring inference with batch_size=8:  51%|█████     | 51/100 [00:37<00:35,  1.38it/s]Measuring inference with batch_size=8:  52%|█████▏    | 52/100 [00:37<00:34,  1.38it/s]Measuring inference with batch_size=8:  53%|█████▎    | 53/100 [00:38<00:34,  1.38it/s]Measuring inference with batch_size=8:  54%|█████▍    | 54/100 [00:39<00:33,  1.38it/s]Measuring inference with batch_size=8:  55%|█████▌    | 55/100 [00:39<00:32,  1.38it/s]Measuring inference with batch_size=8:  56%|█████▌    | 56/100 [00:40<00:31,  1.38it/s]Measuring inference with batch_size=8:  57%|█████▋    | 57/100 [00:41<00:31,  1.38it/s]Measuring inference with batch_size=8:  58%|█████▊    | 58/100 [00:42<00:30,  1.38it/s]Measuring inference with batch_size=8:  59%|█████▉    | 59/100 [00:42<00:29,  1.38it/s]Measuring inference with batch_size=8:  60%|██████    | 60/100 [00:43<00:29,  1.38it/s]Measuring inference with batch_size=8:  61%|██████    | 61/100 [00:44<00:28,  1.38it/s]Measuring inference with batch_size=8:  62%|██████▏   | 62/100 [00:44<00:27,  1.38it/s]Measuring inference with batch_size=8:  63%|██████▎   | 63/100 [00:45<00:26,  1.38it/s]Measuring inference with batch_size=8:  64%|██████▍   | 64/100 [00:46<00:26,  1.38it/s]Measuring inference with batch_size=8:  65%|██████▌   | 65/100 [00:47<00:25,  1.38it/s]Measuring inference with batch_size=8:  66%|██████▌   | 66/100 [00:47<00:24,  1.38it/s]Measuring inference with batch_size=8:  67%|██████▋   | 67/100 [00:48<00:23,  1.38it/s]Measuring inference with batch_size=8:  68%|██████▊   | 68/100 [00:49<00:23,  1.38it/s]Measuring inference with batch_size=8:  69%|██████▉   | 69/100 [00:50<00:22,  1.38it/s]Measuring inference with batch_size=8:  70%|███████   | 70/100 [00:50<00:21,  1.38it/s]Measuring inference with batch_size=8:  71%|███████   | 71/100 [00:51<00:21,  1.38it/s]Measuring inference with batch_size=8:  72%|███████▏  | 72/100 [00:52<00:20,  1.38it/s]Measuring inference with batch_size=8:  73%|███████▎  | 73/100 [00:52<00:19,  1.38it/s]Measuring inference with batch_size=8:  74%|███████▍  | 74/100 [00:53<00:18,  1.38it/s]Measuring inference with batch_size=8:  75%|███████▌  | 75/100 [00:54<00:18,  1.38it/s]Measuring inference with batch_size=8:  76%|███████▌  | 76/100 [00:55<00:17,  1.38it/s]Measuring inference with batch_size=8:  77%|███████▋  | 77/100 [00:55<00:16,  1.38it/s]Measuring inference with batch_size=8:  78%|███████▊  | 78/100 [00:56<00:15,  1.38it/s]Measuring inference with batch_size=8:  79%|███████▉  | 79/100 [00:57<00:15,  1.38it/s]Measuring inference with batch_size=8:  80%|████████  | 80/100 [00:58<00:14,  1.38it/s]Measuring inference with batch_size=8:  81%|████████  | 81/100 [00:58<00:13,  1.38it/s]Measuring inference with batch_size=8:  82%|████████▏ | 82/100 [00:59<00:13,  1.38it/s]Measuring inference with batch_size=8:  83%|████████▎ | 83/100 [01:00<00:12,  1.38it/s]Measuring inference with batch_size=8:  84%|████████▍ | 84/100 [01:00<00:11,  1.38it/s]Measuring inference with batch_size=8:  85%|████████▌ | 85/100 [01:01<00:10,  1.38it/s]Measuring inference with batch_size=8:  86%|████████▌ | 86/100 [01:02<00:10,  1.38it/s]Measuring inference with batch_size=8:  87%|████████▋ | 87/100 [01:03<00:09,  1.38it/s]Measuring inference with batch_size=8:  88%|████████▊ | 88/100 [01:03<00:08,  1.38it/s]Measuring inference with batch_size=8:  89%|████████▉ | 89/100 [01:04<00:07,  1.38it/s]Measuring inference with batch_size=8:  90%|█████████ | 90/100 [01:05<00:07,  1.38it/s]Measuring inference with batch_size=8:  91%|█████████ | 91/100 [01:06<00:06,  1.38it/s]Measuring inference with batch_size=8:  92%|█████████▏| 92/100 [01:06<00:05,  1.38it/s]Measuring inference with batch_size=8:  93%|█████████▎| 93/100 [01:07<00:05,  1.38it/s]Measuring inference with batch_size=8:  94%|█████████▍| 94/100 [01:08<00:04,  1.38it/s]Measuring inference with batch_size=8:  95%|█████████▌| 95/100 [01:08<00:03,  1.38it/s]Measuring inference with batch_size=8:  96%|█████████▌| 96/100 [01:09<00:02,  1.38it/s]Measuring inference with batch_size=8:  97%|█████████▋| 97/100 [01:10<00:02,  1.38it/s]Measuring inference with batch_size=8:  98%|█████████▊| 98/100 [01:11<00:01,  1.38it/s]Measuring inference with batch_size=8:  99%|█████████▉| 99/100 [01:11<00:00,  1.38it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [01:12<00:00,  1.38it/s]Measuring inference with batch_size=8: 100%|██████████| 100/100 [01:12<00:00,  1.38it/s]Timing results (batch_size=8):
  cpu_to_gpu:
    human_readable:
      batch_latency: "1.689 ms +/- 278.952 \xB5s [1.321 ms, 2.412 ms]"
      batches_per_second: 607.10 +/- 92.31 [414.54, 756.96]
    metrics:
      batches_per_second_max: 756.9579498285508
      batches_per_second_mean: 607.1035934081043
      batches_per_second_min: 414.5388416683139
      batches_per_second_std: 92.31482767154867
      seconds_per_batch_max: 0.0024123191833496094
      seconds_per_batch_mean: 0.001689321994781494
      seconds_per_batch_min: 0.0013210773468017578
      seconds_per_batch_std: 0.0002789522632561969
  gpu_to_cpu:
    human_readable:
      batch_latency: "45.815 ms +/- 926.769 \xB5s [44.984 ms, 49.020 ms]"
      batches_per_second: 21.84 +/- 0.42 [20.40, 22.23]
    metrics:
      batches_per_second_max: 22.229957917722256
      batches_per_second_mean: 21.835316287539214
      batches_per_second_min: 20.399715961596453
      batches_per_second_std: 0.42135143138309333
      seconds_per_batch_max: 0.04902029037475586
      seconds_per_batch_mean: 0.04581524848937988
      seconds_per_batch_min: 0.04498434066772461
      seconds_per_batch_std: 0.0009267690058080421
  on_device_inference:
    human_readable:
      batch_latency: 676.947 ms +/- 1.956 ms [671.669 ms, 682.581 ms]
      batches_per_second: 1.48 +/- 0.00 [1.47, 1.49]
    metrics:
      batches_per_second_max: 1.4888290892000982
      batches_per_second_mean: 1.4772318604521073
      batches_per_second_min: 1.4650266822310103
      batches_per_second_std: 0.004264676641681541
      seconds_per_batch_max: 0.6825814247131348
      seconds_per_batch_mean: 0.6769474506378174
      seconds_per_batch_min: 0.6716687679290771
      seconds_per_batch_std: 0.001956064391170407
  total:
    human_readable:
      batch_latency: 724.452 ms +/- 1.852 ms [720.836 ms, 729.366 ms]
      batches_per_second: 1.38 +/- 0.00 [1.37, 1.39]
    metrics:
      batches_per_second_max: 1.387278250846314
      batches_per_second_mean: 1.3803626768471882
      batches_per_second_min: 1.371054092237729
      batches_per_second_std: 0.0035268935217509095
      seconds_per_batch_max: 0.7293658256530762
      seconds_per_batch_mean: 0.7244520211219787
      seconds_per_batch_min: 0.7208359241485596
      seconds_per_batch_std: 0.0018518113777832355

Energy results (batch_size=8):
  joules: 15.847292596626284
  kWh: 4.402025721285079e-06

learner.model.forward:
  device: cuda
  energy:
    batch_size_1:
      joules: 17.663767215220133
      kWh: 4.906602004227814e-06
    batch_size_8:
      joules: 15.847292596626284
      kWh: 4.402025721285079e-06
  flops: 1032310310
  machine_info:
    cpu:
      architecture: aarch64
      cores:
        physical: 8
        total: 8
      frequency: 2.27 GHz
      model: ARMv8 Processor rev 0 (v8l)
    gpus: null
    memory:
      available: 2.59 GB
      total: 31.17 GB
      used: 28.10 GB
    system:
      node: xavier
      release: 4.9.140-tegra
      system: Linux
  max_inference_memory: 10273394688
  params: 6153432
  post_inference_memory: 10256752640
  pre_inference_memory: 7514144768
  timing:
    batch_size_1:
      cpu_to_gpu:
        human_readable:
          batch_latency: "322.738 \xB5s +/- 94.794 \xB5s [253.916 \xB5s, 715.971 \xB5\
            s]"
          batches_per_second: 3.25 K +/- 550.01 [1.40 K, 3.94 K]
        metrics:
          batches_per_second_max: 3938.313615023474
          batches_per_second_mean: 3254.90447580796
          batches_per_second_min: 1396.7046287046287
          batches_per_second_std: 550.0138813152251
          seconds_per_batch_max: 0.0007159709930419922
          seconds_per_batch_mean: 0.0003227376937866211
          seconds_per_batch_min: 0.00025391578674316406
          seconds_per_batch_std: 9.47935193683234e-05
      gpu_to_cpu:
        human_readable:
          batch_latency: "185.843 \xB5s +/- 64.899 \xB5s [140.429 \xB5s, 804.186 \xB5\
            s]"
          batches_per_second: 5.58 K +/- 678.89 [1.24 K, 7.12 K]
        metrics:
          batches_per_second_max: 7121.059422750424
          batches_per_second_mean: 5578.28716948968
          batches_per_second_min: 1243.493625852357
          batches_per_second_std: 678.8874966597248
          seconds_per_batch_max: 0.0008041858673095703
          seconds_per_batch_mean: 0.00018584251403808594
          seconds_per_batch_min: 0.0001404285430908203
          seconds_per_batch_std: 6.489890781723908e-05
      on_device_inference:
        human_readable:
          batch_latency: 177.597 ms +/- 2.275 ms [174.455 ms, 188.852 ms]
          batches_per_second: 5.63 +/- 0.07 [5.30, 5.73]
        metrics:
          batches_per_second_max: 5.732139611352476
          batches_per_second_mean: 5.631608835210532
          batches_per_second_min: 5.2951430620221585
          batches_per_second_std: 0.07007196342366251
          seconds_per_batch_max: 0.18885231018066406
          seconds_per_batch_mean: 0.17759745121002196
          seconds_per_batch_min: 0.174454927444458
          seconds_per_batch_std: 0.0022751832857059403
      total:
        human_readable:
          batch_latency: 178.106 ms +/- 2.323 ms [174.994 ms, 189.441 ms]
          batches_per_second: 5.62 +/- 0.07 [5.28, 5.71]
        metrics:
          batches_per_second_max: 5.7144896910938625
          batches_per_second_mean: 5.61555933291854
          batches_per_second_min: 5.278695952034554
          batches_per_second_std: 0.07105670929997435
          seconds_per_batch_max: 0.18944072723388672
          seconds_per_batch_mean: 0.17810603141784667
          seconds_per_batch_min: 0.17499375343322754
          seconds_per_batch_std: 0.002322665095073143
    batch_size_8:
      cpu_to_gpu:
        human_readable:
          batch_latency: "1.689 ms +/- 278.952 \xB5s [1.321 ms, 2.412 ms]"
          batches_per_second: 607.10 +/- 92.31 [414.54, 756.96]
        metrics:
          batches_per_second_max: 756.9579498285508
          batches_per_second_mean: 607.1035934081043
          batches_per_second_min: 414.5388416683139
          batches_per_second_std: 92.31482767154867
          seconds_per_batch_max: 0.0024123191833496094
          seconds_per_batch_mean: 0.001689321994781494
          seconds_per_batch_min: 0.0013210773468017578
          seconds_per_batch_std: 0.0002789522632561969
      gpu_to_cpu:
        human_readable:
          batch_latency: "45.815 ms +/- 926.769 \xB5s [44.984 ms, 49.020 ms]"
          batches_per_second: 21.84 +/- 0.42 [20.40, 22.23]
        metrics:
          batches_per_second_max: 22.229957917722256
          batches_per_second_mean: 21.835316287539214
          batches_per_second_min: 20.399715961596453
          batches_per_second_std: 0.42135143138309333
          seconds_per_batch_max: 0.04902029037475586
          seconds_per_batch_mean: 0.04581524848937988
          seconds_per_batch_min: 0.04498434066772461
          seconds_per_batch_std: 0.0009267690058080421
      on_device_inference:
        human_readable:
          batch_latency: 676.947 ms +/- 1.956 ms [671.669 ms, 682.581 ms]
          batches_per_second: 1.48 +/- 0.00 [1.47, 1.49]
        metrics:
          batches_per_second_max: 1.4888290892000982
          batches_per_second_mean: 1.4772318604521073
          batches_per_second_min: 1.4650266822310103
          batches_per_second_std: 0.004264676641681541
          seconds_per_batch_max: 0.6825814247131348
          seconds_per_batch_mean: 0.6769474506378174
          seconds_per_batch_min: 0.6716687679290771
          seconds_per_batch_std: 0.001956064391170407
      total:
        human_readable:
          batch_latency: 724.452 ms +/- 1.852 ms [720.836 ms, 729.366 ms]
          batches_per_second: 1.38 +/- 0.00 [1.37, 1.39]
        metrics:
          batches_per_second_max: 1.387278250846314
          batches_per_second_mean: 1.3803626768471882
          batches_per_second_min: 1.371054092237729
          batches_per_second_std: 0.0035268935217509095
          seconds_per_batch_max: 0.7293658256530762
          seconds_per_batch_mean: 0.7244520211219787
          seconds_per_batch_min: 0.7208359241485596
          seconds_per_batch_std: 0.0018518113777832355


