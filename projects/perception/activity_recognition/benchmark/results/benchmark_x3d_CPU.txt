INFO:benchmark:==== Benchmarking X3DLearner (xs) ====
INFO:benchmark:== Benchmarking learner.infer ==
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
WARNING:torch-benchmark:Measurement of allocated memory is only available on CUDA devices
Warming up with batch_size=1:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=1:  10%|█         | 1/10 [00:00<00:02,  3.84it/s]Warming up with batch_size=1:  20%|██        | 2/10 [00:00<00:01,  5.12it/s]Warming up with batch_size=1:  30%|███       | 3/10 [00:00<00:01,  5.80it/s]Warming up with batch_size=1:  40%|████      | 4/10 [00:00<00:00,  6.05it/s]Warming up with batch_size=1:  50%|█████     | 5/10 [00:00<00:00,  6.18it/s]Warming up with batch_size=1:  60%|██████    | 6/10 [00:01<00:00,  6.15it/s]Warming up with batch_size=1:  70%|███████   | 7/10 [00:01<00:00,  6.17it/s]Warming up with batch_size=1:  80%|████████  | 8/10 [00:01<00:00,  6.24it/s]Warming up with batch_size=1:  90%|█████████ | 9/10 [00:01<00:00,  6.33it/s]Warming up with batch_size=1: 100%|██████████| 10/10 [00:01<00:00,  6.46it/s]Warming up with batch_size=1: 100%|██████████| 10/10 [00:01<00:00,  6.07it/s]
Measuring inference with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=1:   1%|          | 1/100 [00:00<00:13,  7.22it/s]Measuring inference with batch_size=1:   2%|▏         | 2/100 [00:00<00:14,  6.78it/s]Measuring inference with batch_size=1:   3%|▎         | 3/100 [00:00<00:14,  6.62it/s]Measuring inference with batch_size=1:   4%|▍         | 4/100 [00:00<00:14,  6.82it/s]Measuring inference with batch_size=1:   5%|▌         | 5/100 [00:00<00:13,  7.08it/s]Measuring inference with batch_size=1:   6%|▌         | 6/100 [00:00<00:13,  7.10it/s]Measuring inference with batch_size=1:   7%|▋         | 7/100 [00:01<00:13,  7.08it/s]Measuring inference with batch_size=1:   8%|▊         | 8/100 [00:01<00:13,  6.85it/s]Measuring inference with batch_size=1:   9%|▉         | 9/100 [00:01<00:13,  6.96it/s]Measuring inference with batch_size=1:  10%|█         | 10/100 [00:01<00:14,  6.05it/s]Measuring inference with batch_size=1:  11%|█         | 11/100 [00:01<00:15,  5.74it/s]Measuring inference with batch_size=1:  12%|█▏        | 12/100 [00:01<00:16,  5.29it/s]Measuring inference with batch_size=1:  13%|█▎        | 13/100 [00:02<00:16,  5.22it/s]Measuring inference with batch_size=1:  14%|█▍        | 14/100 [00:02<00:16,  5.20it/s]Measuring inference with batch_size=1:  15%|█▌        | 15/100 [00:02<00:15,  5.38it/s]Measuring inference with batch_size=1:  16%|█▌        | 16/100 [00:02<00:15,  5.56it/s]Measuring inference with batch_size=1:  17%|█▋        | 17/100 [00:02<00:14,  5.74it/s]Measuring inference with batch_size=1:  18%|█▊        | 18/100 [00:02<00:14,  5.82it/s]Measuring inference with batch_size=1:  19%|█▉        | 19/100 [00:03<00:14,  5.76it/s]Measuring inference with batch_size=1:  20%|██        | 20/100 [00:03<00:13,  5.84it/s]Measuring inference with batch_size=1:  21%|██        | 21/100 [00:03<00:14,  5.54it/s]Measuring inference with batch_size=1:  22%|██▏       | 22/100 [00:03<00:14,  5.56it/s]Measuring inference with batch_size=1:  23%|██▎       | 23/100 [00:03<00:13,  5.90it/s]Measuring inference with batch_size=1:  24%|██▍       | 24/100 [00:03<00:12,  6.23it/s]Measuring inference with batch_size=1:  25%|██▌       | 25/100 [00:04<00:11,  6.44it/s]Measuring inference with batch_size=1:  26%|██▌       | 26/100 [00:04<00:11,  6.46it/s]Measuring inference with batch_size=1:  27%|██▋       | 27/100 [00:04<00:11,  6.63it/s]Measuring inference with batch_size=1:  28%|██▊       | 28/100 [00:04<00:10,  6.78it/s]Measuring inference with batch_size=1:  29%|██▉       | 29/100 [00:04<00:10,  6.59it/s]Measuring inference with batch_size=1:  30%|███       | 30/100 [00:04<00:10,  6.47it/s]Measuring inference with batch_size=1:  31%|███       | 31/100 [00:05<00:10,  6.66it/s]Measuring inference with batch_size=1:  32%|███▏      | 32/100 [00:05<00:09,  6.81it/s]Measuring inference with batch_size=1:  33%|███▎      | 33/100 [00:05<00:09,  6.94it/s]Measuring inference with batch_size=1:  34%|███▍      | 34/100 [00:05<00:09,  7.05it/s]Measuring inference with batch_size=1:  35%|███▌      | 35/100 [00:05<00:09,  7.14it/s]Measuring inference with batch_size=1:  36%|███▌      | 36/100 [00:05<00:08,  7.31it/s]Measuring inference with batch_size=1:  37%|███▋      | 37/100 [00:05<00:08,  7.34it/s]Measuring inference with batch_size=1:  38%|███▊      | 38/100 [00:05<00:08,  7.18it/s]Measuring inference with batch_size=1:  39%|███▉      | 39/100 [00:06<00:08,  7.14it/s]Measuring inference with batch_size=1:  40%|████      | 40/100 [00:06<00:08,  6.96it/s]Measuring inference with batch_size=1:  41%|████      | 41/100 [00:06<00:08,  6.86it/s]Measuring inference with batch_size=1:  42%|████▏     | 42/100 [00:06<00:08,  6.96it/s]Measuring inference with batch_size=1:  43%|████▎     | 43/100 [00:06<00:08,  6.99it/s]Measuring inference with batch_size=1:  44%|████▍     | 44/100 [00:06<00:08,  6.50it/s]Measuring inference with batch_size=1:  45%|████▌     | 45/100 [00:07<00:08,  6.34it/s]Measuring inference with batch_size=1:  46%|████▌     | 46/100 [00:07<00:08,  6.21it/s]Measuring inference with batch_size=1:  47%|████▋     | 47/100 [00:07<00:08,  6.19it/s]Measuring inference with batch_size=1:  48%|████▊     | 48/100 [00:07<00:08,  6.23it/s]Measuring inference with batch_size=1:  49%|████▉     | 49/100 [00:07<00:07,  6.56it/s]Measuring inference with batch_size=1:  50%|█████     | 50/100 [00:07<00:07,  6.82it/s]Measuring inference with batch_size=1:  51%|█████     | 51/100 [00:07<00:06,  7.07it/s]Measuring inference with batch_size=1:  52%|█████▏    | 52/100 [00:08<00:06,  7.26it/s]Measuring inference with batch_size=1:  53%|█████▎    | 53/100 [00:08<00:06,  7.39it/s]Measuring inference with batch_size=1:  54%|█████▍    | 54/100 [00:08<00:06,  7.01it/s]Measuring inference with batch_size=1:  55%|█████▌    | 55/100 [00:08<00:07,  6.38it/s]Measuring inference with batch_size=1:  56%|█████▌    | 56/100 [00:08<00:07,  5.88it/s]Measuring inference with batch_size=1:  57%|█████▋    | 57/100 [00:08<00:07,  5.59it/s]Measuring inference with batch_size=1:  58%|█████▊    | 58/100 [00:09<00:07,  5.77it/s]Measuring inference with batch_size=1:  59%|█████▉    | 59/100 [00:09<00:06,  6.00it/s]Measuring inference with batch_size=1:  60%|██████    | 60/100 [00:09<00:06,  6.33it/s]Measuring inference with batch_size=1:  61%|██████    | 61/100 [00:09<00:05,  6.61it/s]Measuring inference with batch_size=1:  62%|██████▏   | 62/100 [00:09<00:05,  6.75it/s]Measuring inference with batch_size=1:  63%|██████▎   | 63/100 [00:09<00:05,  6.83it/s]Measuring inference with batch_size=1:  64%|██████▍   | 64/100 [00:09<00:05,  6.79it/s]Measuring inference with batch_size=1:  65%|██████▌   | 65/100 [00:10<00:04,  7.03it/s]Measuring inference with batch_size=1:  66%|██████▌   | 66/100 [00:10<00:04,  7.16it/s]Measuring inference with batch_size=1:  67%|██████▋   | 67/100 [00:10<00:04,  7.17it/s]Measuring inference with batch_size=1:  68%|██████▊   | 68/100 [00:10<00:04,  6.71it/s]Measuring inference with batch_size=1:  69%|██████▉   | 69/100 [00:10<00:04,  6.67it/s]Measuring inference with batch_size=1:  70%|███████   | 70/100 [00:10<00:04,  6.56it/s]Measuring inference with batch_size=1:  71%|███████   | 71/100 [00:11<00:04,  6.57it/s]Measuring inference with batch_size=1:  72%|███████▏  | 72/100 [00:11<00:04,  6.55it/s]Measuring inference with batch_size=1:  73%|███████▎  | 73/100 [00:11<00:04,  6.61it/s]Measuring inference with batch_size=1:  74%|███████▍  | 74/100 [00:11<00:03,  6.73it/s]Measuring inference with batch_size=1:  75%|███████▌  | 75/100 [00:11<00:03,  6.36it/s]Measuring inference with batch_size=1:  76%|███████▌  | 76/100 [00:11<00:04,  5.98it/s]Measuring inference with batch_size=1:  77%|███████▋  | 77/100 [00:12<00:04,  5.73it/s]Measuring inference with batch_size=1:  78%|███████▊  | 78/100 [00:12<00:03,  5.74it/s]Measuring inference with batch_size=1:  79%|███████▉  | 79/100 [00:12<00:03,  5.73it/s]Measuring inference with batch_size=1:  80%|████████  | 80/100 [00:12<00:03,  5.91it/s]Measuring inference with batch_size=1:  81%|████████  | 81/100 [00:12<00:03,  6.02it/s]Measuring inference with batch_size=1:  82%|████████▏ | 82/100 [00:12<00:03,  5.66it/s]Measuring inference with batch_size=1:  83%|████████▎ | 83/100 [00:13<00:02,  5.76it/s]Measuring inference with batch_size=1:  84%|████████▍ | 84/100 [00:13<00:02,  5.81it/s]Measuring inference with batch_size=1:  85%|████████▌ | 85/100 [00:13<00:02,  5.83it/s]Measuring inference with batch_size=1:  86%|████████▌ | 86/100 [00:13<00:02,  5.80it/s]Measuring inference with batch_size=1:  87%|████████▋ | 87/100 [00:13<00:02,  5.72it/s]Measuring inference with batch_size=1:  88%|████████▊ | 88/100 [00:13<00:01,  6.04it/s]Measuring inference with batch_size=1:  89%|████████▉ | 89/100 [00:14<00:01,  6.27it/s]Measuring inference with batch_size=1:  90%|█████████ | 90/100 [00:14<00:01,  6.50it/s]Measuring inference with batch_size=1:  91%|█████████ | 91/100 [00:14<00:01,  6.63it/s]Measuring inference with batch_size=1:  92%|█████████▏| 92/100 [00:14<00:01,  6.76it/s]Measuring inference with batch_size=1:  93%|█████████▎| 93/100 [00:14<00:01,  6.99it/s]Measuring inference with batch_size=1:  94%|█████████▍| 94/100 [00:14<00:00,  6.91it/s]Measuring inference with batch_size=1:  95%|█████████▌| 95/100 [00:14<00:00,  6.75it/s]Measuring inference with batch_size=1:  96%|█████████▌| 96/100 [00:15<00:00,  6.66it/s]Measuring inference with batch_size=1:  97%|█████████▋| 97/100 [00:15<00:00,  6.62it/s]Measuring inference with batch_size=1:  98%|█████████▊| 98/100 [00:15<00:00,  6.57it/s]Measuring inference with batch_size=1:  99%|█████████▉| 99/100 [00:15<00:00,  6.66it/s]Measuring inference with batch_size=1: 100%|██████████| 100/100 [00:15<00:00,  6.73it/s]Measuring inference with batch_size=1: 100%|██████████| 100/100 [00:15<00:00,  6.39it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
INFO:benchmark:learner.infer:
  device: cpu
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 6
        total: 12
      frequency: 2.60 GHz
      model: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz
    gpus: null
    memory:
      available: 5.03 GB
      total: 16.00 GB
      used: 8.63 GB
    system:
      node: d40049
      release: 21.2.0
      system: Darwin
  timing:
    batch_size_1:
      on_device_inference:
        human_readable:
          batch_latency: 156.241 ms +/- 20.940 ms [128.689 ms, 222.620 ms]
          batches_per_second: 6.51 +/- 0.79 [4.49, 7.77]
        metrics:
          batches_per_second_max: 7.770683266637024
          batches_per_second_mean: 6.505562929905494
          batches_per_second_min: 4.491959183323766
          batches_per_second_std: 0.7932388435610077
          seconds_per_batch_max: 0.22262001037597656
          seconds_per_batch_mean: 0.15624139308929444
          seconds_per_batch_min: 0.12868881225585938
          seconds_per_batch_std: 0.020939546794576207

INFO:benchmark:== Benchmarking model directly ==
WARNING:torch-benchmark:Measurement of allocated memory is only available on CUDA devices
Warming up with batch_size=1:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=1:  10%|█         | 1/10 [00:00<00:01,  5.90it/s]Warming up with batch_size=1:  20%|██        | 2/10 [00:00<00:01,  6.12it/s]Warming up with batch_size=1:  30%|███       | 3/10 [00:00<00:01,  6.40it/s]Warming up with batch_size=1:  40%|████      | 4/10 [00:00<00:00,  6.41it/s]Warming up with batch_size=1:  50%|█████     | 5/10 [00:00<00:00,  6.63it/s]Warming up with batch_size=1:  60%|██████    | 6/10 [00:00<00:00,  6.89it/s]Warming up with batch_size=1:  70%|███████   | 7/10 [00:01<00:00,  6.34it/s]Warming up with batch_size=1:  80%|████████  | 8/10 [00:01<00:00,  6.51it/s]Warming up with batch_size=1:  90%|█████████ | 9/10 [00:01<00:00,  6.59it/s]Warming up with batch_size=1: 100%|██████████| 10/10 [00:01<00:00,  6.65it/s]Warming up with batch_size=1: 100%|██████████| 10/10 [00:01<00:00,  6.53it/s]
Measuring inference with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=1:   1%|          | 1/100 [00:00<00:13,  7.17it/s]Measuring inference with batch_size=1:   2%|▏         | 2/100 [00:00<00:14,  6.73it/s]Measuring inference with batch_size=1:   3%|▎         | 3/100 [00:00<00:14,  6.85it/s]Measuring inference with batch_size=1:   4%|▍         | 4/100 [00:00<00:13,  6.95it/s]Measuring inference with batch_size=1:   5%|▌         | 5/100 [00:00<00:13,  6.92it/s]Measuring inference with batch_size=1:   6%|▌         | 6/100 [00:00<00:13,  7.13it/s]Measuring inference with batch_size=1:   7%|▋         | 7/100 [00:00<00:12,  7.31it/s]Measuring inference with batch_size=1:   8%|▊         | 8/100 [00:01<00:12,  7.44it/s]Measuring inference with batch_size=1:   9%|▉         | 9/100 [00:01<00:12,  7.54it/s]Measuring inference with batch_size=1:  10%|█         | 10/100 [00:01<00:11,  7.56it/s]Measuring inference with batch_size=1:  11%|█         | 11/100 [00:01<00:11,  7.59it/s]Measuring inference with batch_size=1:  12%|█▏        | 12/100 [00:01<00:11,  7.63it/s]Measuring inference with batch_size=1:  13%|█▎        | 13/100 [00:01<00:11,  7.66it/s]Measuring inference with batch_size=1:  14%|█▍        | 14/100 [00:01<00:11,  7.69it/s]Measuring inference with batch_size=1:  15%|█▌        | 15/100 [00:02<00:11,  7.71it/s]Measuring inference with batch_size=1:  16%|█▌        | 16/100 [00:02<00:10,  7.72it/s]Measuring inference with batch_size=1:  17%|█▋        | 17/100 [00:02<00:10,  7.73it/s]Measuring inference with batch_size=1:  18%|█▊        | 18/100 [00:02<00:10,  7.74it/s]Measuring inference with batch_size=1:  19%|█▉        | 19/100 [00:02<00:10,  7.72it/s]Measuring inference with batch_size=1:  20%|██        | 20/100 [00:02<00:10,  7.72it/s]Measuring inference with batch_size=1:  21%|██        | 21/100 [00:02<00:10,  7.65it/s]Measuring inference with batch_size=1:  22%|██▏       | 22/100 [00:02<00:10,  7.67it/s]Measuring inference with batch_size=1:  23%|██▎       | 23/100 [00:03<00:10,  7.69it/s]Measuring inference with batch_size=1:  24%|██▍       | 24/100 [00:03<00:09,  7.70it/s]Measuring inference with batch_size=1:  25%|██▌       | 25/100 [00:03<00:09,  7.70it/s]Measuring inference with batch_size=1:  26%|██▌       | 26/100 [00:03<00:09,  7.71it/s]Measuring inference with batch_size=1:  27%|██▋       | 27/100 [00:03<00:09,  7.68it/s]Measuring inference with batch_size=1:  28%|██▊       | 28/100 [00:03<00:09,  7.70it/s]Measuring inference with batch_size=1:  29%|██▉       | 29/100 [00:03<00:09,  7.69it/s]Measuring inference with batch_size=1:  30%|███       | 30/100 [00:03<00:09,  7.69it/s]Measuring inference with batch_size=1:  31%|███       | 31/100 [00:04<00:08,  7.71it/s]Measuring inference with batch_size=1:  32%|███▏      | 32/100 [00:04<00:09,  7.36it/s]Measuring inference with batch_size=1:  33%|███▎      | 33/100 [00:04<00:09,  7.11it/s]Measuring inference with batch_size=1:  34%|███▍      | 34/100 [00:04<00:09,  7.15it/s]Measuring inference with batch_size=1:  35%|███▌      | 35/100 [00:04<00:08,  7.25it/s]Measuring inference with batch_size=1:  36%|███▌      | 36/100 [00:04<00:09,  6.93it/s]Measuring inference with batch_size=1:  37%|███▋      | 37/100 [00:04<00:09,  6.86it/s]Measuring inference with batch_size=1:  38%|███▊      | 38/100 [00:05<00:09,  6.67it/s]Measuring inference with batch_size=1:  39%|███▉      | 39/100 [00:05<00:09,  6.33it/s]Measuring inference with batch_size=1:  40%|████      | 40/100 [00:05<00:09,  6.10it/s]Measuring inference with batch_size=1:  41%|████      | 41/100 [00:05<00:10,  5.69it/s]Measuring inference with batch_size=1:  42%|████▏     | 42/100 [00:05<00:09,  5.82it/s]Measuring inference with batch_size=1:  43%|████▎     | 43/100 [00:06<00:09,  6.07it/s]Measuring inference with batch_size=1:  44%|████▍     | 44/100 [00:06<00:08,  6.31it/s]Measuring inference with batch_size=1:  45%|████▌     | 45/100 [00:06<00:08,  6.54it/s]Measuring inference with batch_size=1:  46%|████▌     | 46/100 [00:06<00:08,  6.62it/s]Measuring inference with batch_size=1:  47%|████▋     | 47/100 [00:06<00:07,  6.87it/s]Measuring inference with batch_size=1:  48%|████▊     | 48/100 [00:06<00:07,  7.11it/s]Measuring inference with batch_size=1:  49%|████▉     | 49/100 [00:06<00:07,  7.10it/s]Measuring inference with batch_size=1:  50%|█████     | 50/100 [00:06<00:07,  7.11it/s]Measuring inference with batch_size=1:  51%|█████     | 51/100 [00:07<00:06,  7.24it/s]Measuring inference with batch_size=1:  52%|█████▏    | 52/100 [00:07<00:06,  7.16it/s]Measuring inference with batch_size=1:  53%|█████▎    | 53/100 [00:07<00:06,  7.29it/s]Measuring inference with batch_size=1:  54%|█████▍    | 54/100 [00:07<00:06,  7.43it/s]Measuring inference with batch_size=1:  55%|█████▌    | 55/100 [00:07<00:06,  7.49it/s]Measuring inference with batch_size=1:  56%|█████▌    | 56/100 [00:07<00:05,  7.51it/s]Measuring inference with batch_size=1:  57%|█████▋    | 57/100 [00:07<00:06,  7.15it/s]Measuring inference with batch_size=1:  58%|█████▊    | 58/100 [00:08<00:05,  7.25it/s]Measuring inference with batch_size=1:  59%|█████▉    | 59/100 [00:08<00:05,  7.39it/s]Measuring inference with batch_size=1:  60%|██████    | 60/100 [00:08<00:05,  7.51it/s]Measuring inference with batch_size=1:  61%|██████    | 61/100 [00:08<00:05,  7.57it/s]Measuring inference with batch_size=1:  62%|██████▏   | 62/100 [00:08<00:04,  7.62it/s]Measuring inference with batch_size=1:  63%|██████▎   | 63/100 [00:08<00:04,  7.64it/s]Measuring inference with batch_size=1:  64%|██████▍   | 64/100 [00:08<00:04,  7.65it/s]Measuring inference with batch_size=1:  65%|██████▌   | 65/100 [00:09<00:04,  7.24it/s]Measuring inference with batch_size=1:  66%|██████▌   | 66/100 [00:09<00:04,  7.38it/s]Measuring inference with batch_size=1:  67%|██████▋   | 67/100 [00:09<00:04,  7.49it/s]Measuring inference with batch_size=1:  68%|██████▊   | 68/100 [00:09<00:04,  7.57it/s]Measuring inference with batch_size=1:  69%|██████▉   | 69/100 [00:09<00:04,  7.62it/s]Measuring inference with batch_size=1:  70%|███████   | 70/100 [00:09<00:03,  7.61it/s]Measuring inference with batch_size=1:  71%|███████   | 71/100 [00:09<00:03,  7.64it/s]Measuring inference with batch_size=1:  72%|███████▏  | 72/100 [00:09<00:03,  7.56it/s]Measuring inference with batch_size=1:  73%|███████▎  | 73/100 [00:10<00:03,  7.49it/s]Measuring inference with batch_size=1:  74%|███████▍  | 74/100 [00:10<00:03,  7.23it/s]Measuring inference with batch_size=1:  75%|███████▌  | 75/100 [00:10<00:03,  7.37it/s]Measuring inference with batch_size=1:  76%|███████▌  | 76/100 [00:10<00:03,  7.48it/s]Measuring inference with batch_size=1:  77%|███████▋  | 77/100 [00:10<00:03,  7.56it/s]Measuring inference with batch_size=1:  78%|███████▊  | 78/100 [00:10<00:02,  7.62it/s]Measuring inference with batch_size=1:  79%|███████▉  | 79/100 [00:10<00:02,  7.61it/s]Measuring inference with batch_size=1:  80%|████████  | 80/100 [00:10<00:02,  7.65it/s]Measuring inference with batch_size=1:  81%|████████  | 81/100 [00:11<00:02,  7.10it/s]Measuring inference with batch_size=1:  82%|████████▏ | 82/100 [00:11<00:02,  7.14it/s]Measuring inference with batch_size=1:  83%|████████▎ | 83/100 [00:11<00:02,  7.28it/s]Measuring inference with batch_size=1:  84%|████████▍ | 84/100 [00:11<00:02,  7.42it/s]Measuring inference with batch_size=1:  85%|████████▌ | 85/100 [00:11<00:01,  7.52it/s]Measuring inference with batch_size=1:  86%|████████▌ | 86/100 [00:11<00:01,  7.59it/s]Measuring inference with batch_size=1:  87%|████████▋ | 87/100 [00:11<00:01,  7.63it/s]Measuring inference with batch_size=1:  88%|████████▊ | 88/100 [00:12<00:01,  7.68it/s]Measuring inference with batch_size=1:  89%|████████▉ | 89/100 [00:12<00:01,  7.69it/s]Measuring inference with batch_size=1:  90%|█████████ | 90/100 [00:12<00:01,  7.31it/s]Measuring inference with batch_size=1:  91%|█████████ | 91/100 [00:12<00:01,  7.43it/s]Measuring inference with batch_size=1:  92%|█████████▏| 92/100 [00:12<00:01,  7.52it/s]Measuring inference with batch_size=1:  93%|█████████▎| 93/100 [00:12<00:00,  7.58it/s]Measuring inference with batch_size=1:  94%|█████████▍| 94/100 [00:12<00:00,  7.53it/s]Measuring inference with batch_size=1:  95%|█████████▌| 95/100 [00:12<00:00,  7.60it/s]Measuring inference with batch_size=1:  96%|█████████▌| 96/100 [00:13<00:00,  7.64it/s]Measuring inference with batch_size=1:  97%|█████████▋| 97/100 [00:13<00:00,  7.62it/s]Measuring inference with batch_size=1:  98%|█████████▊| 98/100 [00:13<00:00,  7.38it/s]Measuring inference with batch_size=1:  99%|█████████▉| 99/100 [00:13<00:00,  7.48it/s]Measuring inference with batch_size=1: 100%|██████████| 100/100 [00:13<00:00,  7.57it/s]Measuring inference with batch_size=1: 100%|██████████| 100/100 [00:13<00:00,  7.32it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
INFO:benchmark:learner.model.forward:
  device: cpu
  flops: 635560544
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 6
        total: 12
      frequency: 2.60 GHz
      model: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz
    gpus: null
    memory:
      available: 5.10 GB
      total: 16.00 GB
      used: 8.71 GB
    system:
      node: d40049
      release: 21.2.0
      system: Darwin
  params: 3794322
  timing:
    batch_size_1:
      on_device_inference:
        human_readable:
          batch_latency: 136.374 ms +/- 12.678 ms [128.356 ms, 203.153 ms]
          batches_per_second: 7.39 +/- 0.57 [4.92, 7.79]
        metrics:
          batches_per_second_max: 7.790847439367118
          batches_per_second_mean: 7.385476934547006
          batches_per_second_min: 4.922400934179102
          batches_per_second_std: 0.5722117604020649
          seconds_per_batch_max: 0.20315289497375488
          seconds_per_batch_mean: 0.13637424230575562
          seconds_per_batch_min: 0.1283557415008545
          seconds_per_batch_std: 0.012677888890242461

INFO:benchmark:==== Benchmarking X3DLearner (s) ====
INFO:benchmark:== Benchmarking learner.infer ==
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
WARNING:torch-benchmark:Measurement of allocated memory is only available on CUDA devices
Warming up with batch_size=1:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=1:  10%|█         | 1/10 [00:00<00:04,  2.02it/s]Warming up with batch_size=1:  20%|██        | 2/10 [00:00<00:03,  2.03it/s]Warming up with batch_size=1:  30%|███       | 3/10 [00:01<00:03,  2.07it/s]Warming up with batch_size=1:  40%|████      | 4/10 [00:01<00:02,  2.08it/s]Warming up with batch_size=1:  50%|█████     | 5/10 [00:02<00:02,  2.10it/s]Warming up with batch_size=1:  60%|██████    | 6/10 [00:02<00:01,  2.08it/s]Warming up with batch_size=1:  70%|███████   | 7/10 [00:03<00:01,  2.10it/s]Warming up with batch_size=1:  80%|████████  | 8/10 [00:03<00:00,  2.09it/s]Warming up with batch_size=1:  90%|█████████ | 9/10 [00:04<00:00,  2.09it/s]Warming up with batch_size=1: 100%|██████████| 10/10 [00:04<00:00,  2.08it/s]Warming up with batch_size=1: 100%|██████████| 10/10 [00:04<00:00,  2.08it/s]
Measuring inference with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=1:   1%|          | 1/100 [00:00<00:47,  2.09it/s]Measuring inference with batch_size=1:   2%|▏         | 2/100 [00:00<00:46,  2.11it/s]Measuring inference with batch_size=1:   3%|▎         | 3/100 [00:01<00:46,  2.07it/s]Measuring inference with batch_size=1:   4%|▍         | 4/100 [00:01<00:45,  2.10it/s]Measuring inference with batch_size=1:   5%|▌         | 5/100 [00:02<00:45,  2.09it/s]Measuring inference with batch_size=1:   6%|▌         | 6/100 [00:02<00:44,  2.11it/s]Measuring inference with batch_size=1:   7%|▋         | 7/100 [00:03<00:45,  2.05it/s]Measuring inference with batch_size=1:   8%|▊         | 8/100 [00:03<00:44,  2.06it/s]Measuring inference with batch_size=1:   9%|▉         | 9/100 [00:04<00:44,  2.05it/s]Measuring inference with batch_size=1:  10%|█         | 10/100 [00:04<00:43,  2.07it/s]Measuring inference with batch_size=1:  11%|█         | 11/100 [00:05<00:42,  2.08it/s]Measuring inference with batch_size=1:  12%|█▏        | 12/100 [00:05<00:42,  2.07it/s]Measuring inference with batch_size=1:  13%|█▎        | 13/100 [00:06<00:41,  2.08it/s]Measuring inference with batch_size=1:  14%|█▍        | 14/100 [00:06<00:41,  2.06it/s]Measuring inference with batch_size=1:  15%|█▌        | 15/100 [00:07<00:41,  2.06it/s]Measuring inference with batch_size=1:  16%|█▌        | 16/100 [00:07<00:40,  2.05it/s]Measuring inference with batch_size=1:  17%|█▋        | 17/100 [00:08<00:39,  2.08it/s]Measuring inference with batch_size=1:  18%|█▊        | 18/100 [00:08<00:40,  2.02it/s]Measuring inference with batch_size=1:  19%|█▉        | 19/100 [00:09<00:39,  2.05it/s]Measuring inference with batch_size=1:  20%|██        | 20/100 [00:09<00:39,  2.04it/s]Measuring inference with batch_size=1:  21%|██        | 21/100 [00:10<00:38,  2.07it/s]Measuring inference with batch_size=1:  22%|██▏       | 22/100 [00:10<00:37,  2.06it/s]Measuring inference with batch_size=1:  23%|██▎       | 23/100 [00:11<00:37,  2.07it/s]Measuring inference with batch_size=1:  24%|██▍       | 24/100 [00:11<00:36,  2.09it/s]Measuring inference with batch_size=1:  25%|██▌       | 25/100 [00:12<00:35,  2.09it/s]Measuring inference with batch_size=1:  26%|██▌       | 26/100 [00:12<00:35,  2.10it/s]Measuring inference with batch_size=1:  27%|██▋       | 27/100 [00:13<00:35,  2.07it/s]Measuring inference with batch_size=1:  28%|██▊       | 28/100 [00:13<00:34,  2.08it/s]Measuring inference with batch_size=1:  29%|██▉       | 29/100 [00:14<00:34,  2.07it/s]Measuring inference with batch_size=1:  30%|███       | 30/100 [00:14<00:33,  2.06it/s]Measuring inference with batch_size=1:  31%|███       | 31/100 [00:14<00:33,  2.07it/s]Measuring inference with batch_size=1:  32%|███▏      | 32/100 [00:15<00:32,  2.07it/s]Measuring inference with batch_size=1:  33%|███▎      | 33/100 [00:15<00:32,  2.07it/s]Measuring inference with batch_size=1:  34%|███▍      | 34/100 [00:16<00:32,  2.05it/s]Measuring inference with batch_size=1:  35%|███▌      | 35/100 [00:16<00:31,  2.06it/s]Measuring inference with batch_size=1:  36%|███▌      | 36/100 [00:17<00:31,  2.05it/s]Measuring inference with batch_size=1:  37%|███▋      | 37/100 [00:17<00:30,  2.05it/s]Measuring inference with batch_size=1:  38%|███▊      | 38/100 [00:18<00:30,  2.04it/s]Measuring inference with batch_size=1:  39%|███▉      | 39/100 [00:18<00:29,  2.06it/s]Measuring inference with batch_size=1:  40%|████      | 40/100 [00:19<00:28,  2.08it/s]Measuring inference with batch_size=1:  41%|████      | 41/100 [00:19<00:28,  2.06it/s]Measuring inference with batch_size=1:  42%|████▏     | 42/100 [00:20<00:27,  2.08it/s]Measuring inference with batch_size=1:  43%|████▎     | 43/100 [00:20<00:27,  2.08it/s]Measuring inference with batch_size=1:  44%|████▍     | 44/100 [00:21<00:26,  2.08it/s]Measuring inference with batch_size=1:  45%|████▌     | 45/100 [00:21<00:26,  2.05it/s]Measuring inference with batch_size=1:  46%|████▌     | 46/100 [00:22<00:26,  2.07it/s]Measuring inference with batch_size=1:  47%|████▋     | 47/100 [00:22<00:25,  2.06it/s]Measuring inference with batch_size=1:  48%|████▊     | 48/100 [00:23<00:25,  2.08it/s]Measuring inference with batch_size=1:  49%|████▉     | 49/100 [00:23<00:24,  2.06it/s]Measuring inference with batch_size=1:  50%|█████     | 50/100 [00:24<00:25,  2.00it/s]Measuring inference with batch_size=1:  51%|█████     | 51/100 [00:24<00:24,  2.00it/s]Measuring inference with batch_size=1:  52%|█████▏    | 52/100 [00:25<00:23,  2.04it/s]Measuring inference with batch_size=1:  53%|█████▎    | 53/100 [00:25<00:22,  2.06it/s]Measuring inference with batch_size=1:  54%|█████▍    | 54/100 [00:26<00:22,  2.06it/s]Measuring inference with batch_size=1:  55%|█████▌    | 55/100 [00:26<00:21,  2.06it/s]Measuring inference with batch_size=1:  56%|█████▌    | 56/100 [00:27<00:21,  2.05it/s]Measuring inference with batch_size=1:  57%|█████▋    | 57/100 [00:27<00:20,  2.06it/s]Measuring inference with batch_size=1:  58%|█████▊    | 58/100 [00:28<00:20,  2.06it/s]Measuring inference with batch_size=1:  59%|█████▉    | 59/100 [00:28<00:19,  2.08it/s]Measuring inference with batch_size=1:  60%|██████    | 60/100 [00:29<00:19,  2.05it/s]Measuring inference with batch_size=1:  61%|██████    | 61/100 [00:29<00:18,  2.06it/s]Measuring inference with batch_size=1:  62%|██████▏   | 62/100 [00:30<00:18,  2.05it/s]Measuring inference with batch_size=1:  63%|██████▎   | 63/100 [00:30<00:18,  2.03it/s]Measuring inference with batch_size=1:  64%|██████▍   | 64/100 [00:31<00:18,  1.99it/s]Measuring inference with batch_size=1:  65%|██████▌   | 65/100 [00:31<00:17,  2.03it/s]Measuring inference with batch_size=1:  66%|██████▌   | 66/100 [00:32<00:16,  2.05it/s]Measuring inference with batch_size=1:  67%|██████▋   | 67/100 [00:32<00:16,  2.04it/s]Measuring inference with batch_size=1:  68%|██████▊   | 68/100 [00:33<00:15,  2.06it/s]Measuring inference with batch_size=1:  69%|██████▉   | 69/100 [00:33<00:15,  2.04it/s]Measuring inference with batch_size=1:  70%|███████   | 70/100 [00:33<00:14,  2.05it/s]Measuring inference with batch_size=1:  71%|███████   | 71/100 [00:34<00:14,  2.03it/s]Measuring inference with batch_size=1:  72%|███████▏  | 72/100 [00:34<00:13,  2.05it/s]Measuring inference with batch_size=1:  73%|███████▎  | 73/100 [00:35<00:13,  1.98it/s]Measuring inference with batch_size=1:  74%|███████▍  | 74/100 [00:36<00:13,  1.95it/s]Measuring inference with batch_size=1:  75%|███████▌  | 75/100 [00:36<00:13,  1.92it/s]Measuring inference with batch_size=1:  76%|███████▌  | 76/100 [00:37<00:12,  1.97it/s]Measuring inference with batch_size=1:  77%|███████▋  | 77/100 [00:37<00:11,  1.97it/s]Measuring inference with batch_size=1:  78%|███████▊  | 78/100 [00:38<00:10,  2.02it/s]Measuring inference with batch_size=1:  79%|███████▉  | 79/100 [00:38<00:10,  2.02it/s]Measuring inference with batch_size=1:  80%|████████  | 80/100 [00:39<00:09,  2.02it/s]Measuring inference with batch_size=1:  81%|████████  | 81/100 [00:39<00:09,  2.04it/s]Measuring inference with batch_size=1:  82%|████████▏ | 82/100 [00:39<00:08,  2.04it/s]Measuring inference with batch_size=1:  83%|████████▎ | 83/100 [00:40<00:08,  2.06it/s]Measuring inference with batch_size=1:  84%|████████▍ | 84/100 [00:40<00:07,  2.04it/s]Measuring inference with batch_size=1:  85%|████████▌ | 85/100 [00:41<00:07,  2.06it/s]Measuring inference with batch_size=1:  86%|████████▌ | 86/100 [00:41<00:06,  2.05it/s]Measuring inference with batch_size=1:  87%|████████▋ | 87/100 [00:42<00:06,  2.07it/s]Measuring inference with batch_size=1:  88%|████████▊ | 88/100 [00:42<00:05,  2.06it/s]Measuring inference with batch_size=1:  89%|████████▉ | 89/100 [00:43<00:05,  2.08it/s]Measuring inference with batch_size=1:  90%|█████████ | 90/100 [00:43<00:04,  2.07it/s]Measuring inference with batch_size=1:  91%|█████████ | 91/100 [00:44<00:04,  2.08it/s]Measuring inference with batch_size=1:  92%|█████████▏| 92/100 [00:44<00:03,  2.08it/s]Measuring inference with batch_size=1:  93%|█████████▎| 93/100 [00:45<00:03,  2.08it/s]Measuring inference with batch_size=1:  94%|█████████▍| 94/100 [00:45<00:02,  2.07it/s]Measuring inference with batch_size=1:  95%|█████████▌| 95/100 [00:46<00:02,  2.05it/s]Measuring inference with batch_size=1:  96%|█████████▌| 96/100 [00:46<00:01,  2.07it/s]Measuring inference with batch_size=1:  97%|█████████▋| 97/100 [00:47<00:01,  2.07it/s]Measuring inference with batch_size=1:  98%|█████████▊| 98/100 [00:47<00:00,  2.09it/s]Measuring inference with batch_size=1:  99%|█████████▉| 99/100 [00:48<00:00,  2.07it/s]Measuring inference with batch_size=1: 100%|██████████| 100/100 [00:48<00:00,  2.06it/s]Measuring inference with batch_size=1: 100%|██████████| 100/100 [00:48<00:00,  2.05it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
INFO:benchmark:learner.infer:
  device: cpu
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 6
        total: 12
      frequency: 2.60 GHz
      model: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz
    gpus: null
    memory:
      available: 5.13 GB
      total: 16.00 GB
      used: 8.67 GB
    system:
      node: d40049
      release: 21.2.0
      system: Darwin
  timing:
    batch_size_1:
      on_device_inference:
        human_readable:
          batch_latency: 486.700 ms +/- 16.511 ms [464.661 ms, 547.644 ms]
          batches_per_second: 2.06 +/- 0.07 [1.83, 2.15]
        metrics:
          batches_per_second_max: 2.152106027409912
          batches_per_second_mean: 2.0569227790409714
          batches_per_second_min: 1.8260032930109282
          batches_per_second_std: 0.06693302866905068
          seconds_per_batch_max: 0.5476441383361816
          seconds_per_batch_mean: 0.4866997003555298
          seconds_per_batch_min: 0.4646611213684082
          seconds_per_batch_std: 0.016510501503903242

INFO:benchmark:== Benchmarking model directly ==
WARNING:torch-benchmark:Measurement of allocated memory is only available on CUDA devices
Warming up with batch_size=1:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=1:  10%|█         | 1/10 [00:00<00:04,  2.16it/s]Warming up with batch_size=1:  20%|██        | 2/10 [00:00<00:03,  2.08it/s]Warming up with batch_size=1:  30%|███       | 3/10 [00:01<00:03,  2.10it/s]Warming up with batch_size=1:  40%|████      | 4/10 [00:01<00:02,  2.06it/s]Warming up with batch_size=1:  50%|█████     | 5/10 [00:02<00:02,  2.08it/s]Warming up with batch_size=1:  60%|██████    | 6/10 [00:02<00:01,  2.08it/s]Warming up with batch_size=1:  70%|███████   | 7/10 [00:03<00:01,  2.00it/s]Warming up with batch_size=1:  80%|████████  | 8/10 [00:03<00:01,  1.94it/s]Warming up with batch_size=1:  90%|█████████ | 9/10 [00:04<00:00,  1.97it/s]Warming up with batch_size=1: 100%|██████████| 10/10 [00:04<00:00,  1.96it/s]Warming up with batch_size=1: 100%|██████████| 10/10 [00:04<00:00,  2.01it/s]
Measuring inference with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=1:   1%|          | 1/100 [00:00<00:48,  2.03it/s]Measuring inference with batch_size=1:   2%|▏         | 2/100 [00:01<00:49,  1.96it/s]Measuring inference with batch_size=1:   3%|▎         | 3/100 [00:01<00:48,  1.98it/s]Measuring inference with batch_size=1:   4%|▍         | 4/100 [00:01<00:47,  2.04it/s]Measuring inference with batch_size=1:   5%|▌         | 5/100 [00:02<00:47,  1.98it/s]Measuring inference with batch_size=1:   6%|▌         | 6/100 [00:03<00:47,  1.98it/s]Measuring inference with batch_size=1:   7%|▋         | 7/100 [00:03<00:48,  1.90it/s]Measuring inference with batch_size=1:   8%|▊         | 8/100 [00:04<00:50,  1.83it/s]Measuring inference with batch_size=1:   9%|▉         | 9/100 [00:04<00:48,  1.87it/s]Measuring inference with batch_size=1:  10%|█         | 10/100 [00:05<00:45,  1.96it/s]Measuring inference with batch_size=1:  11%|█         | 11/100 [00:05<00:44,  2.01it/s]Measuring inference with batch_size=1:  12%|█▏        | 12/100 [00:06<00:42,  2.07it/s]Measuring inference with batch_size=1:  13%|█▎        | 13/100 [00:06<00:43,  2.00it/s]Measuring inference with batch_size=1:  14%|█▍        | 14/100 [00:07<00:44,  1.94it/s]Measuring inference with batch_size=1:  15%|█▌        | 15/100 [00:07<00:45,  1.88it/s]Measuring inference with batch_size=1:  16%|█▌        | 16/100 [00:08<00:45,  1.86it/s]Measuring inference with batch_size=1:  17%|█▋        | 17/100 [00:08<00:43,  1.90it/s]Measuring inference with batch_size=1:  18%|█▊        | 18/100 [00:09<00:41,  1.96it/s]Measuring inference with batch_size=1:  19%|█▉        | 19/100 [00:09<00:41,  1.94it/s]Measuring inference with batch_size=1:  20%|██        | 20/100 [00:10<00:40,  1.99it/s]Measuring inference with batch_size=1:  21%|██        | 21/100 [00:10<00:41,  1.90it/s]Measuring inference with batch_size=1:  22%|██▏       | 22/100 [00:11<00:39,  1.98it/s]Measuring inference with batch_size=1:  23%|██▎       | 23/100 [00:11<00:39,  1.96it/s]Measuring inference with batch_size=1:  24%|██▍       | 24/100 [00:12<00:37,  2.01it/s]Measuring inference with batch_size=1:  25%|██▌       | 25/100 [00:12<00:37,  2.02it/s]Measuring inference with batch_size=1:  26%|██▌       | 26/100 [00:13<00:36,  2.02it/s]Measuring inference with batch_size=1:  27%|██▋       | 27/100 [00:14<00:43,  1.69it/s]Measuring inference with batch_size=1:  28%|██▊       | 28/100 [00:14<00:39,  1.80it/s]Measuring inference with batch_size=1:  29%|██▉       | 29/100 [00:15<00:38,  1.87it/s]Measuring inference with batch_size=1:  30%|███       | 30/100 [00:15<00:35,  1.97it/s]Measuring inference with batch_size=1:  31%|███       | 31/100 [00:15<00:34,  2.01it/s]Measuring inference with batch_size=1:  32%|███▏      | 32/100 [00:16<00:33,  2.04it/s]Measuring inference with batch_size=1:  33%|███▎      | 33/100 [00:16<00:33,  2.03it/s]Measuring inference with batch_size=1:  34%|███▍      | 34/100 [00:17<00:33,  1.96it/s]Measuring inference with batch_size=1:  35%|███▌      | 35/100 [00:18<00:35,  1.85it/s]Measuring inference with batch_size=1:  36%|███▌      | 36/100 [00:18<00:34,  1.84it/s]Measuring inference with batch_size=1:  37%|███▋      | 37/100 [00:19<00:32,  1.92it/s]Measuring inference with batch_size=1:  38%|███▊      | 38/100 [00:19<00:31,  1.96it/s]Measuring inference with batch_size=1:  39%|███▉      | 39/100 [00:20<00:30,  1.99it/s]Measuring inference with batch_size=1:  40%|████      | 40/100 [00:20<00:29,  2.02it/s]Measuring inference with batch_size=1:  41%|████      | 41/100 [00:21<00:28,  2.06it/s]Measuring inference with batch_size=1:  42%|████▏     | 42/100 [00:21<00:28,  2.05it/s]Measuring inference with batch_size=1:  43%|████▎     | 43/100 [00:21<00:27,  2.09it/s]Measuring inference with batch_size=1:  44%|████▍     | 44/100 [00:22<00:26,  2.08it/s]Measuring inference with batch_size=1:  45%|████▌     | 45/100 [00:22<00:26,  2.11it/s]Measuring inference with batch_size=1:  46%|████▌     | 46/100 [00:23<00:25,  2.13it/s]Measuring inference with batch_size=1:  47%|████▋     | 47/100 [00:23<00:24,  2.12it/s]Measuring inference with batch_size=1:  48%|████▊     | 48/100 [00:24<00:24,  2.14it/s]Measuring inference with batch_size=1:  49%|████▉     | 49/100 [00:24<00:24,  2.11it/s]Measuring inference with batch_size=1:  50%|█████     | 50/100 [00:25<00:23,  2.13it/s]Measuring inference with batch_size=1:  51%|█████     | 51/100 [00:25<00:23,  2.10it/s]Measuring inference with batch_size=1:  52%|█████▏    | 52/100 [00:26<00:22,  2.12it/s]Measuring inference with batch_size=1:  53%|█████▎    | 53/100 [00:26<00:22,  2.10it/s]Measuring inference with batch_size=1:  54%|█████▍    | 54/100 [00:27<00:21,  2.11it/s]Measuring inference with batch_size=1:  55%|█████▌    | 55/100 [00:27<00:21,  2.12it/s]Measuring inference with batch_size=1:  56%|█████▌    | 56/100 [00:28<00:20,  2.11it/s]Measuring inference with batch_size=1:  57%|█████▋    | 57/100 [00:28<00:20,  2.13it/s]Measuring inference with batch_size=1:  58%|█████▊    | 58/100 [00:29<00:19,  2.11it/s]Measuring inference with batch_size=1:  59%|█████▉    | 59/100 [00:29<00:19,  2.12it/s]Measuring inference with batch_size=1:  60%|██████    | 60/100 [00:30<00:19,  2.09it/s]Measuring inference with batch_size=1:  61%|██████    | 61/100 [00:30<00:18,  2.12it/s]Measuring inference with batch_size=1:  62%|██████▏   | 62/100 [00:30<00:17,  2.12it/s]Measuring inference with batch_size=1:  63%|██████▎   | 63/100 [00:31<00:17,  2.13it/s]Measuring inference with batch_size=1:  64%|██████▍   | 64/100 [00:31<00:16,  2.13it/s]Measuring inference with batch_size=1:  65%|██████▌   | 65/100 [00:32<00:16,  2.12it/s]Measuring inference with batch_size=1:  66%|██████▌   | 66/100 [00:32<00:15,  2.14it/s]Measuring inference with batch_size=1:  67%|██████▋   | 67/100 [00:33<00:15,  2.09it/s]Measuring inference with batch_size=1:  68%|██████▊   | 68/100 [00:33<00:15,  2.12it/s]Measuring inference with batch_size=1:  69%|██████▉   | 69/100 [00:34<00:14,  2.10it/s]Measuring inference with batch_size=1:  70%|███████   | 70/100 [00:34<00:14,  2.12it/s]Measuring inference with batch_size=1:  71%|███████   | 71/100 [00:35<00:13,  2.12it/s]Measuring inference with batch_size=1:  72%|███████▏  | 72/100 [00:35<00:13,  2.12it/s]Measuring inference with batch_size=1:  73%|███████▎  | 73/100 [00:36<00:12,  2.14it/s]Measuring inference with batch_size=1:  74%|███████▍  | 74/100 [00:36<00:12,  2.12it/s]Measuring inference with batch_size=1:  75%|███████▌  | 75/100 [00:37<00:11,  2.14it/s]Measuring inference with batch_size=1:  76%|███████▌  | 76/100 [00:37<00:11,  2.12it/s]Measuring inference with batch_size=1:  77%|███████▋  | 77/100 [00:38<00:10,  2.13it/s]Measuring inference with batch_size=1:  78%|███████▊  | 78/100 [00:38<00:10,  2.11it/s]Measuring inference with batch_size=1:  79%|███████▉  | 79/100 [00:38<00:09,  2.13it/s]Measuring inference with batch_size=1:  80%|████████  | 80/100 [00:39<00:09,  2.13it/s]Measuring inference with batch_size=1:  81%|████████  | 81/100 [00:39<00:08,  2.14it/s]Measuring inference with batch_size=1:  82%|████████▏ | 82/100 [00:40<00:08,  2.15it/s]Measuring inference with batch_size=1:  83%|████████▎ | 83/100 [00:40<00:07,  2.13it/s]Measuring inference with batch_size=1:  84%|████████▍ | 84/100 [00:41<00:07,  2.13it/s]Measuring inference with batch_size=1:  85%|████████▌ | 85/100 [00:41<00:07,  2.11it/s]Measuring inference with batch_size=1:  86%|████████▌ | 86/100 [00:42<00:06,  2.12it/s]Measuring inference with batch_size=1:  87%|████████▋ | 87/100 [00:42<00:06,  2.11it/s]Measuring inference with batch_size=1:  88%|████████▊ | 88/100 [00:43<00:05,  2.12it/s]Measuring inference with batch_size=1:  89%|████████▉ | 89/100 [00:43<00:05,  2.09it/s]Measuring inference with batch_size=1:  90%|█████████ | 90/100 [00:44<00:04,  2.07it/s]Measuring inference with batch_size=1:  91%|█████████ | 91/100 [00:44<00:04,  2.09it/s]Measuring inference with batch_size=1:  92%|█████████▏| 92/100 [00:45<00:03,  2.07it/s]Measuring inference with batch_size=1:  93%|█████████▎| 93/100 [00:45<00:03,  2.10it/s]Measuring inference with batch_size=1:  94%|█████████▍| 94/100 [00:46<00:02,  2.08it/s]Measuring inference with batch_size=1:  95%|█████████▌| 95/100 [00:46<00:02,  2.11it/s]Measuring inference with batch_size=1:  96%|█████████▌| 96/100 [00:47<00:01,  2.08it/s]Measuring inference with batch_size=1:  97%|█████████▋| 97/100 [00:47<00:01,  2.09it/s]Measuring inference with batch_size=1:  98%|█████████▊| 98/100 [00:47<00:00,  2.12it/s]Measuring inference with batch_size=1:  99%|█████████▉| 99/100 [00:48<00:00,  2.11it/s]Measuring inference with batch_size=1: 100%|██████████| 100/100 [00:48<00:00,  2.12it/s]Measuring inference with batch_size=1: 100%|██████████| 100/100 [00:48<00:00,  2.04it/s]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
INFO:benchmark:learner.model.forward:
  device: cpu
  flops: 2061365744
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 6
        total: 12
      frequency: 2.60 GHz
      model: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz
    gpus: null
    memory:
      available: 5.05 GB
      total: 16.00 GB
      used: 8.76 GB
    system:
      node: d40049
      release: 21.2.0
      system: Darwin
  params: 3794322
  timing:
    batch_size_1:
      on_device_inference:
        human_readable:
          batch_latency: 488.938 ms +/- 46.275 ms [439.599 ms, 818.704 ms]
          batches_per_second: 2.06 +/- 0.15 [1.22, 2.27]
        metrics:
          batches_per_second_max: 2.274800250784787
          batches_per_second_mean: 2.059175816976442
          batches_per_second_min: 1.2214431904090168
          batches_per_second_std: 0.15063435798838107
          seconds_per_batch_max: 0.8187036514282227
          seconds_per_batch_mean: 0.4889383840560913
          seconds_per_batch_min: 0.43959903717041016
          seconds_per_batch_std: 0.046274977023567476

INFO:benchmark:==== Benchmarking X3DLearner (m) ====
INFO:benchmark:== Benchmarking learner.infer ==
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
WARNING:torch-benchmark:Measurement of allocated memory is only available on CUDA devices
Warming up with batch_size=1:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=1:  10%|█         | 1/10 [00:01<00:11,  1.28s/it]Warming up with batch_size=1:  20%|██        | 2/10 [00:02<00:10,  1.28s/it]Warming up with batch_size=1:  30%|███       | 3/10 [00:03<00:08,  1.28s/it]Warming up with batch_size=1:  40%|████      | 4/10 [00:05<00:07,  1.27s/it]Warming up with batch_size=1:  50%|█████     | 5/10 [00:06<00:06,  1.27s/it]Warming up with batch_size=1:  60%|██████    | 6/10 [00:07<00:05,  1.27s/it]Warming up with batch_size=1:  70%|███████   | 7/10 [00:08<00:03,  1.27s/it]Warming up with batch_size=1:  80%|████████  | 8/10 [00:10<00:02,  1.26s/it]Warming up with batch_size=1:  90%|█████████ | 9/10 [00:11<00:01,  1.26s/it]Warming up with batch_size=1: 100%|██████████| 10/10 [00:12<00:00,  1.26s/it]Warming up with batch_size=1: 100%|██████████| 10/10 [00:12<00:00,  1.26s/it]
Measuring inference with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=1:   1%|          | 1/100 [00:01<02:07,  1.28s/it]Measuring inference with batch_size=1:   2%|▏         | 2/100 [00:02<02:04,  1.27s/it]Measuring inference with batch_size=1:   3%|▎         | 3/100 [00:03<02:02,  1.26s/it]Measuring inference with batch_size=1:   4%|▍         | 4/100 [00:05<02:01,  1.26s/it]Measuring inference with batch_size=1:   5%|▌         | 5/100 [00:06<02:01,  1.28s/it]Measuring inference with batch_size=1:   6%|▌         | 6/100 [00:07<01:59,  1.27s/it]Measuring inference with batch_size=1:   7%|▋         | 7/100 [00:08<01:58,  1.27s/it]Measuring inference with batch_size=1:   8%|▊         | 8/100 [00:10<01:56,  1.27s/it]Measuring inference with batch_size=1:   9%|▉         | 9/100 [00:11<01:57,  1.30s/it]Measuring inference with batch_size=1:  10%|█         | 10/100 [00:12<01:58,  1.32s/it]Measuring inference with batch_size=1:  11%|█         | 11/100 [00:14<01:57,  1.31s/it]Measuring inference with batch_size=1:  12%|█▏        | 12/100 [00:15<01:55,  1.31s/it]Measuring inference with batch_size=1:  13%|█▎        | 13/100 [00:16<01:52,  1.30s/it]Measuring inference with batch_size=1:  14%|█▍        | 14/100 [00:18<01:52,  1.31s/it]Measuring inference with batch_size=1:  15%|█▌        | 15/100 [00:19<01:49,  1.29s/it]Measuring inference with batch_size=1:  16%|█▌        | 16/100 [00:20<01:48,  1.29s/it]Measuring inference with batch_size=1:  17%|█▋        | 17/100 [00:21<01:46,  1.28s/it]Measuring inference with batch_size=1:  18%|█▊        | 18/100 [00:23<01:44,  1.27s/it]Measuring inference with batch_size=1:  19%|█▉        | 19/100 [00:24<01:42,  1.27s/it]Measuring inference with batch_size=1:  20%|██        | 20/100 [00:25<01:42,  1.28s/it]Measuring inference with batch_size=1:  21%|██        | 21/100 [00:26<01:40,  1.27s/it]Measuring inference with batch_size=1:  22%|██▏       | 22/100 [00:28<01:39,  1.27s/it]Measuring inference with batch_size=1:  23%|██▎       | 23/100 [00:29<01:38,  1.27s/it]Measuring inference with batch_size=1:  24%|██▍       | 24/100 [00:30<01:36,  1.26s/it]Measuring inference with batch_size=1:  25%|██▌       | 25/100 [00:32<01:37,  1.30s/it]Measuring inference with batch_size=1:  26%|██▌       | 26/100 [00:33<01:37,  1.32s/it]Measuring inference with batch_size=1:  27%|██▋       | 27/100 [00:34<01:34,  1.30s/it]Measuring inference with batch_size=1:  28%|██▊       | 28/100 [00:36<01:33,  1.30s/it]Measuring inference with batch_size=1:  29%|██▉       | 29/100 [00:37<01:34,  1.32s/it]Measuring inference with batch_size=1:  30%|███       | 30/100 [00:38<01:33,  1.33s/it]Measuring inference with batch_size=1:  31%|███       | 31/100 [00:40<01:33,  1.35s/it]Measuring inference with batch_size=1:  32%|███▏      | 32/100 [00:41<01:31,  1.35s/it]Measuring inference with batch_size=1:  33%|███▎      | 33/100 [00:42<01:30,  1.35s/it]Measuring inference with batch_size=1:  34%|███▍      | 34/100 [00:44<01:27,  1.33s/it]Measuring inference with batch_size=1:  35%|███▌      | 35/100 [00:45<01:25,  1.31s/it]Measuring inference with batch_size=1:  36%|███▌      | 36/100 [00:46<01:22,  1.29s/it]Measuring inference with batch_size=1:  37%|███▋      | 37/100 [00:47<01:21,  1.29s/it]Measuring inference with batch_size=1:  38%|███▊      | 38/100 [00:49<01:20,  1.30s/it]Measuring inference with batch_size=1:  39%|███▉      | 39/100 [00:50<01:18,  1.28s/it]Measuring inference with batch_size=1:  40%|████      | 40/100 [00:51<01:16,  1.28s/it]Measuring inference with batch_size=1:  41%|████      | 41/100 [00:53<01:15,  1.28s/it]Measuring inference with batch_size=1:  42%|████▏     | 42/100 [00:54<01:13,  1.27s/it]Measuring inference with batch_size=1:  43%|████▎     | 43/100 [00:55<01:12,  1.27s/it]Measuring inference with batch_size=1:  44%|████▍     | 44/100 [00:56<01:12,  1.29s/it]Measuring inference with batch_size=1:  45%|████▌     | 45/100 [00:58<01:11,  1.30s/it]Measuring inference with batch_size=1:  46%|████▌     | 46/100 [00:59<01:10,  1.31s/it]Measuring inference with batch_size=1:  47%|████▋     | 47/100 [01:00<01:10,  1.33s/it]Measuring inference with batch_size=1:  48%|████▊     | 48/100 [01:02<01:09,  1.34s/it]Measuring inference with batch_size=1:  49%|████▉     | 49/100 [01:03<01:07,  1.31s/it]Measuring inference with batch_size=1:  50%|█████     | 50/100 [01:04<01:05,  1.31s/it]Measuring inference with batch_size=1:  51%|█████     | 51/100 [01:06<01:03,  1.29s/it]Measuring inference with batch_size=1:  52%|█████▏    | 52/100 [01:07<01:02,  1.29s/it]Measuring inference with batch_size=1:  53%|█████▎    | 53/100 [01:08<01:00,  1.28s/it]Measuring inference with batch_size=1:  54%|█████▍    | 54/100 [01:09<00:58,  1.28s/it]Measuring inference with batch_size=1:  55%|█████▌    | 55/100 [01:11<00:57,  1.28s/it]Measuring inference with batch_size=1:  56%|█████▌    | 56/100 [01:12<00:55,  1.27s/it]Measuring inference with batch_size=1:  57%|█████▋    | 57/100 [01:13<00:54,  1.27s/it]Measuring inference with batch_size=1:  58%|█████▊    | 58/100 [01:15<00:53,  1.28s/it]Measuring inference with batch_size=1:  59%|█████▉    | 59/100 [01:16<00:52,  1.28s/it]Measuring inference with batch_size=1:  60%|██████    | 60/100 [01:17<00:50,  1.27s/it]Measuring inference with batch_size=1:  61%|██████    | 61/100 [01:18<00:49,  1.28s/it]Measuring inference with batch_size=1:  62%|██████▏   | 62/100 [01:20<00:48,  1.28s/it]Measuring inference with batch_size=1:  63%|██████▎   | 63/100 [01:21<00:47,  1.27s/it]Measuring inference with batch_size=1:  64%|██████▍   | 64/100 [01:22<00:46,  1.28s/it]Measuring inference with batch_size=1:  65%|██████▌   | 65/100 [01:24<00:45,  1.31s/it]Measuring inference with batch_size=1:  66%|██████▌   | 66/100 [01:25<00:44,  1.32s/it]Measuring inference with batch_size=1:  67%|██████▋   | 67/100 [01:26<00:43,  1.33s/it]Measuring inference with batch_size=1:  68%|██████▊   | 68/100 [01:28<00:42,  1.33s/it]Measuring inference with batch_size=1:  69%|██████▉   | 69/100 [01:29<00:40,  1.32s/it]Measuring inference with batch_size=1:  70%|███████   | 70/100 [01:30<00:40,  1.34s/it]Measuring inference with batch_size=1:  71%|███████   | 71/100 [01:32<00:39,  1.37s/it]Measuring inference with batch_size=1:  72%|███████▏  | 72/100 [01:33<00:38,  1.38s/it]Measuring inference with batch_size=1:  73%|███████▎  | 73/100 [01:35<00:41,  1.52s/it]Measuring inference with batch_size=1:  74%|███████▍  | 74/100 [01:36<00:38,  1.48s/it]Measuring inference with batch_size=1:  75%|███████▌  | 75/100 [01:38<00:35,  1.42s/it]Measuring inference with batch_size=1:  76%|███████▌  | 76/100 [01:39<00:33,  1.40s/it]Measuring inference with batch_size=1:  77%|███████▋  | 77/100 [01:40<00:31,  1.39s/it]Measuring inference with batch_size=1:  78%|███████▊  | 78/100 [01:42<00:29,  1.36s/it]Measuring inference with batch_size=1:  79%|███████▉  | 79/100 [01:43<00:28,  1.37s/it]Measuring inference with batch_size=1:  80%|████████  | 80/100 [01:45<00:28,  1.41s/it]Measuring inference with batch_size=1:  81%|████████  | 81/100 [01:46<00:26,  1.39s/it]Measuring inference with batch_size=1:  82%|████████▏ | 82/100 [01:47<00:24,  1.37s/it]Measuring inference with batch_size=1:  83%|████████▎ | 83/100 [01:49<00:23,  1.38s/it]Measuring inference with batch_size=1:  84%|████████▍ | 84/100 [01:50<00:22,  1.38s/it]Measuring inference with batch_size=1:  85%|████████▌ | 85/100 [01:51<00:20,  1.37s/it]Measuring inference with batch_size=1:  86%|████████▌ | 86/100 [01:53<00:19,  1.37s/it]Measuring inference with batch_size=1:  87%|████████▋ | 87/100 [01:54<00:17,  1.34s/it]Measuring inference with batch_size=1:  88%|████████▊ | 88/100 [01:55<00:15,  1.32s/it]Measuring inference with batch_size=1:  89%|████████▉ | 89/100 [01:57<00:15,  1.41s/it]Measuring inference with batch_size=1:  90%|█████████ | 90/100 [01:58<00:14,  1.42s/it]Measuring inference with batch_size=1:  91%|█████████ | 91/100 [02:00<00:13,  1.45s/it]Measuring inference with batch_size=1:  92%|█████████▏| 92/100 [02:01<00:11,  1.43s/it]Measuring inference with batch_size=1:  93%|█████████▎| 93/100 [02:03<00:09,  1.41s/it]Measuring inference with batch_size=1:  94%|█████████▍| 94/100 [02:04<00:08,  1.39s/it]Measuring inference with batch_size=1:  95%|█████████▌| 95/100 [02:05<00:07,  1.41s/it]Measuring inference with batch_size=1:  96%|█████████▌| 96/100 [02:07<00:05,  1.43s/it]Measuring inference with batch_size=1:  97%|█████████▋| 97/100 [02:09<00:04,  1.48s/it]Measuring inference with batch_size=1:  98%|█████████▊| 98/100 [02:10<00:02,  1.49s/it]Measuring inference with batch_size=1:  99%|█████████▉| 99/100 [02:11<00:01,  1.47s/it]Measuring inference with batch_size=1: 100%|██████████| 100/100 [02:13<00:00,  1.48s/it]Measuring inference with batch_size=1: 100%|██████████| 100/100 [02:13<00:00,  1.33s/it]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
INFO:benchmark:learner.infer:
  device: cpu
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 6
        total: 12
      frequency: 2.60 GHz
      model: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz
    gpus: null
    memory:
      available: 5.39 GB
      total: 16.00 GB
      used: 8.20 GB
    system:
      node: d40049
      release: 21.2.0
      system: Darwin
  timing:
    batch_size_1:
      on_device_inference:
        human_readable:
          batch_latency: 1.334 s +/- 93.695 ms [1.243 s, 1.854 s]
          batches_per_second: 0.75 +/- 0.05 [0.54, 0.80]
        metrics:
          batches_per_second_max: 0.8043879500141918
          batches_per_second_mean: 0.7525876763506579
          batches_per_second_min: 0.5393586238273272
          batches_per_second_std: 0.046340774451849025
          seconds_per_batch_max: 1.8540539741516113
          seconds_per_batch_mean: 1.3344705748558043
          seconds_per_batch_min: 1.2431812286376953
          seconds_per_batch_std: 0.09369509683394324

INFO:benchmark:== Benchmarking model directly ==
WARNING:torch-benchmark:Measurement of allocated memory is only available on CUDA devices
Warming up with batch_size=1:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=1:  10%|█         | 1/10 [00:01<00:10,  1.21s/it]Warming up with batch_size=1:  20%|██        | 2/10 [00:02<00:10,  1.30s/it]Warming up with batch_size=1:  30%|███       | 3/10 [00:03<00:09,  1.29s/it]Warming up with batch_size=1:  40%|████      | 4/10 [00:05<00:07,  1.26s/it]Warming up with batch_size=1:  50%|█████     | 5/10 [00:06<00:06,  1.30s/it]Warming up with batch_size=1:  60%|██████    | 6/10 [00:07<00:05,  1.28s/it]Warming up with batch_size=1:  70%|███████   | 7/10 [00:08<00:03,  1.28s/it]Warming up with batch_size=1:  80%|████████  | 8/10 [00:10<00:02,  1.26s/it]Warming up with batch_size=1:  90%|█████████ | 9/10 [00:11<00:01,  1.24s/it]Warming up with batch_size=1: 100%|██████████| 10/10 [00:12<00:00,  1.30s/it]Warming up with batch_size=1: 100%|██████████| 10/10 [00:12<00:00,  1.28s/it]
Measuring inference with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=1:   1%|          | 1/100 [00:01<02:23,  1.45s/it]Measuring inference with batch_size=1:   2%|▏         | 2/100 [00:02<02:09,  1.32s/it]Measuring inference with batch_size=1:   3%|▎         | 3/100 [00:03<02:02,  1.27s/it]Measuring inference with batch_size=1:   4%|▍         | 4/100 [00:04<01:54,  1.19s/it]Measuring inference with batch_size=1:   5%|▌         | 5/100 [00:06<01:56,  1.22s/it]Measuring inference with batch_size=1:   6%|▌         | 6/100 [00:07<01:59,  1.28s/it]Measuring inference with batch_size=1:   7%|▋         | 7/100 [00:08<02:00,  1.30s/it]Measuring inference with batch_size=1:   8%|▊         | 8/100 [00:10<01:56,  1.27s/it]Measuring inference with batch_size=1:   9%|▉         | 9/100 [00:11<01:53,  1.25s/it]Measuring inference with batch_size=1:  10%|█         | 10/100 [00:12<01:51,  1.24s/it]Measuring inference with batch_size=1:  11%|█         | 11/100 [00:13<01:49,  1.23s/it]Measuring inference with batch_size=1:  12%|█▏        | 12/100 [00:14<01:47,  1.22s/it]Measuring inference with batch_size=1:  13%|█▎        | 13/100 [00:16<01:47,  1.24s/it]Measuring inference with batch_size=1:  14%|█▍        | 14/100 [00:17<01:48,  1.26s/it]Measuring inference with batch_size=1:  15%|█▌        | 15/100 [00:18<01:50,  1.30s/it]Measuring inference with batch_size=1:  16%|█▌        | 16/100 [00:20<01:48,  1.29s/it]Measuring inference with batch_size=1:  17%|█▋        | 17/100 [00:21<01:46,  1.28s/it]Measuring inference with batch_size=1:  18%|█▊        | 18/100 [00:22<01:45,  1.28s/it]Measuring inference with batch_size=1:  19%|█▉        | 19/100 [00:24<01:43,  1.28s/it]Measuring inference with batch_size=1:  20%|██        | 20/100 [00:25<01:41,  1.27s/it]Measuring inference with batch_size=1:  21%|██        | 21/100 [00:26<01:39,  1.26s/it]Measuring inference with batch_size=1:  22%|██▏       | 22/100 [00:27<01:38,  1.27s/it]Measuring inference with batch_size=1:  23%|██▎       | 23/100 [00:29<01:36,  1.26s/it]Measuring inference with batch_size=1:  24%|██▍       | 24/100 [00:30<01:35,  1.25s/it]Measuring inference with batch_size=1:  25%|██▌       | 25/100 [00:31<01:32,  1.24s/it]Measuring inference with batch_size=1:  26%|██▌       | 26/100 [00:32<01:30,  1.23s/it]Measuring inference with batch_size=1:  27%|██▋       | 27/100 [00:33<01:29,  1.22s/it]Measuring inference with batch_size=1:  28%|██▊       | 28/100 [00:35<01:29,  1.24s/it]Measuring inference with batch_size=1:  29%|██▉       | 29/100 [00:36<01:29,  1.26s/it]Measuring inference with batch_size=1:  30%|███       | 30/100 [00:37<01:28,  1.26s/it]Measuring inference with batch_size=1:  31%|███       | 31/100 [00:39<01:26,  1.25s/it]Measuring inference with batch_size=1:  32%|███▏      | 32/100 [00:40<01:24,  1.24s/it]Measuring inference with batch_size=1:  33%|███▎      | 33/100 [00:41<01:23,  1.24s/it]Measuring inference with batch_size=1:  34%|███▍      | 34/100 [00:42<01:21,  1.24s/it]Measuring inference with batch_size=1:  35%|███▌      | 35/100 [00:43<01:19,  1.23s/it]Measuring inference with batch_size=1:  36%|███▌      | 36/100 [00:45<01:18,  1.22s/it]Measuring inference with batch_size=1:  37%|███▋      | 37/100 [00:46<01:17,  1.22s/it]Measuring inference with batch_size=1:  38%|███▊      | 38/100 [00:47<01:16,  1.24s/it]Measuring inference with batch_size=1:  39%|███▉      | 39/100 [00:48<01:16,  1.25s/it]Measuring inference with batch_size=1:  40%|████      | 40/100 [00:50<01:19,  1.33s/it]Measuring inference with batch_size=1:  41%|████      | 41/100 [00:51<01:17,  1.31s/it]Measuring inference with batch_size=1:  42%|████▏     | 42/100 [00:52<01:14,  1.28s/it]Measuring inference with batch_size=1:  43%|████▎     | 43/100 [00:54<01:11,  1.26s/it]Measuring inference with batch_size=1:  44%|████▍     | 44/100 [00:55<01:09,  1.24s/it]Measuring inference with batch_size=1:  45%|████▌     | 45/100 [00:56<01:07,  1.23s/it]Measuring inference with batch_size=1:  46%|████▌     | 46/100 [00:57<01:06,  1.23s/it]Measuring inference with batch_size=1:  47%|████▋     | 47/100 [00:59<01:10,  1.33s/it]Measuring inference with batch_size=1:  48%|████▊     | 48/100 [01:00<01:10,  1.36s/it]Measuring inference with batch_size=1:  49%|████▉     | 49/100 [01:02<01:09,  1.35s/it]Measuring inference with batch_size=1:  50%|█████     | 50/100 [01:03<01:08,  1.37s/it]Measuring inference with batch_size=1:  51%|█████     | 51/100 [01:04<01:06,  1.35s/it]Measuring inference with batch_size=1:  52%|█████▏    | 52/100 [01:06<01:04,  1.34s/it]Measuring inference with batch_size=1:  53%|█████▎    | 53/100 [01:07<01:03,  1.35s/it]Measuring inference with batch_size=1:  54%|█████▍    | 54/100 [01:08<01:03,  1.39s/it]Measuring inference with batch_size=1:  55%|█████▌    | 55/100 [01:10<01:02,  1.38s/it]Measuring inference with batch_size=1:  56%|█████▌    | 56/100 [01:11<00:59,  1.36s/it]Measuring inference with batch_size=1:  57%|█████▋    | 57/100 [01:12<00:57,  1.33s/it]Measuring inference with batch_size=1:  58%|█████▊    | 58/100 [01:14<00:54,  1.30s/it]Measuring inference with batch_size=1:  59%|█████▉    | 59/100 [01:15<00:52,  1.27s/it]Measuring inference with batch_size=1:  60%|██████    | 60/100 [01:16<00:50,  1.27s/it]Measuring inference with batch_size=1:  61%|██████    | 61/100 [01:17<00:50,  1.30s/it]Measuring inference with batch_size=1:  62%|██████▏   | 62/100 [01:19<00:49,  1.29s/it]Measuring inference with batch_size=1:  63%|██████▎   | 63/100 [01:20<00:48,  1.31s/it]Measuring inference with batch_size=1:  64%|██████▍   | 64/100 [01:21<00:47,  1.32s/it]Measuring inference with batch_size=1:  65%|██████▌   | 65/100 [01:23<00:46,  1.31s/it]Measuring inference with batch_size=1:  66%|██████▌   | 66/100 [01:24<00:44,  1.30s/it]Measuring inference with batch_size=1:  67%|██████▋   | 67/100 [01:25<00:42,  1.29s/it]Measuring inference with batch_size=1:  68%|██████▊   | 68/100 [01:27<00:42,  1.32s/it]Measuring inference with batch_size=1:  69%|██████▉   | 69/100 [01:28<00:40,  1.31s/it]Measuring inference with batch_size=1:  70%|███████   | 70/100 [01:29<00:39,  1.32s/it]Measuring inference with batch_size=1:  71%|███████   | 71/100 [01:31<00:39,  1.36s/it]Measuring inference with batch_size=1:  72%|███████▏  | 72/100 [01:32<00:38,  1.38s/it]Measuring inference with batch_size=1:  73%|███████▎  | 73/100 [01:34<00:37,  1.39s/it]Measuring inference with batch_size=1:  74%|███████▍  | 74/100 [01:35<00:35,  1.37s/it]Measuring inference with batch_size=1:  75%|███████▌  | 75/100 [01:36<00:34,  1.38s/it]Measuring inference with batch_size=1:  76%|███████▌  | 76/100 [01:38<00:33,  1.40s/it]Measuring inference with batch_size=1:  77%|███████▋  | 77/100 [01:39<00:31,  1.39s/it]Measuring inference with batch_size=1:  78%|███████▊  | 78/100 [01:40<00:29,  1.36s/it]Measuring inference with batch_size=1:  79%|███████▉  | 79/100 [01:42<00:27,  1.32s/it]Measuring inference with batch_size=1:  80%|████████  | 80/100 [01:43<00:25,  1.29s/it]Measuring inference with batch_size=1:  81%|████████  | 81/100 [01:44<00:24,  1.27s/it]Measuring inference with batch_size=1:  82%|████████▏ | 82/100 [01:45<00:22,  1.25s/it]Measuring inference with batch_size=1:  83%|████████▎ | 83/100 [01:47<00:21,  1.26s/it]Measuring inference with batch_size=1:  84%|████████▍ | 84/100 [01:48<00:20,  1.29s/it]Measuring inference with batch_size=1:  85%|████████▌ | 85/100 [01:49<00:19,  1.33s/it]Measuring inference with batch_size=1:  86%|████████▌ | 86/100 [01:51<00:18,  1.32s/it]Measuring inference with batch_size=1:  87%|████████▋ | 87/100 [01:52<00:17,  1.34s/it]Measuring inference with batch_size=1:  88%|████████▊ | 88/100 [01:53<00:15,  1.29s/it]Measuring inference with batch_size=1:  89%|████████▉ | 89/100 [01:54<00:13,  1.25s/it]Measuring inference with batch_size=1:  90%|█████████ | 90/100 [01:55<00:12,  1.22s/it]Measuring inference with batch_size=1:  91%|█████████ | 91/100 [01:57<00:10,  1.20s/it]Measuring inference with batch_size=1:  92%|█████████▏| 92/100 [01:58<00:09,  1.18s/it]Measuring inference with batch_size=1:  93%|█████████▎| 93/100 [01:59<00:08,  1.21s/it]Measuring inference with batch_size=1:  94%|█████████▍| 94/100 [02:00<00:07,  1.26s/it]Measuring inference with batch_size=1:  95%|█████████▌| 95/100 [02:02<00:06,  1.30s/it]Measuring inference with batch_size=1:  96%|█████████▌| 96/100 [02:03<00:05,  1.33s/it]Measuring inference with batch_size=1:  97%|█████████▋| 97/100 [02:05<00:04,  1.34s/it]Measuring inference with batch_size=1:  98%|█████████▊| 98/100 [02:06<00:02,  1.34s/it]Measuring inference with batch_size=1:  99%|█████████▉| 99/100 [02:07<00:01,  1.33s/it]Measuring inference with batch_size=1: 100%|██████████| 100/100 [02:09<00:00,  1.36s/it]Measuring inference with batch_size=1: 100%|██████████| 100/100 [02:09<00:00,  1.29s/it]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
INFO:benchmark:learner.model.forward:
  device: cpu
  flops: 4970008352
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 6
        total: 12
      frequency: 2.60 GHz
      model: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz
    gpus: null
    memory:
      available: 4.70 GB
      total: 16.00 GB
      used: 8.83 GB
    system:
      node: d40049
      release: 21.2.0
      system: Darwin
  params: 3794322
  timing:
    batch_size_1:
      on_device_inference:
        human_readable:
          batch_latency: 1.291 s +/- 88.427 ms [1.084 s, 1.551 s]
          batches_per_second: 0.78 +/- 0.05 [0.64, 0.92]
        metrics:
          batches_per_second_max: 0.9228558139002728
          batches_per_second_mean: 0.7782871560343233
          batches_per_second_min: 0.6446862124309979
          batches_per_second_std: 0.05204598476816697
          seconds_per_batch_max: 1.5511422157287598
          seconds_per_batch_mean: 1.2907642865180968
          seconds_per_batch_min: 1.0835928916931152
          seconds_per_batch_std: 0.08842676397051864

INFO:benchmark:==== Benchmarking X3DLearner (l) ====
INFO:benchmark:== Benchmarking learner.infer ==
ERROR:torch-benchmark:Unable to measure model params due to error: 'function' object has no attribute 'parameters'
ERROR:torch-benchmark:Unable to measure model FLOPs due to error: 'list' object has no attribute 'shape'
WARNING:torch-benchmark:Measurement of allocated memory is only available on CUDA devices
Warming up with batch_size=1:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=1:  10%|█         | 1/10 [00:04<00:41,  4.60s/it]Warming up with batch_size=1:  20%|██        | 2/10 [00:09<00:37,  4.75s/it]Warming up with batch_size=1:  30%|███       | 3/10 [00:14<00:33,  4.85s/it]Warming up with batch_size=1:  40%|████      | 4/10 [00:19<00:29,  4.93s/it]Warming up with batch_size=1:  50%|█████     | 5/10 [00:24<00:24,  4.82s/it]Warming up with batch_size=1:  60%|██████    | 6/10 [00:28<00:18,  4.70s/it]Warming up with batch_size=1:  70%|███████   | 7/10 [00:33<00:14,  4.86s/it]Warming up with batch_size=1:  80%|████████  | 8/10 [00:38<00:09,  4.90s/it]Warming up with batch_size=1:  90%|█████████ | 9/10 [00:43<00:04,  4.78s/it]Warming up with batch_size=1: 100%|██████████| 10/10 [00:47<00:00,  4.68s/it]Warming up with batch_size=1: 100%|██████████| 10/10 [00:47<00:00,  4.77s/it]
Measuring inference with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=1:   1%|          | 1/100 [00:04<07:46,  4.71s/it]Measuring inference with batch_size=1:   2%|▏         | 2/100 [00:09<07:34,  4.64s/it]Measuring inference with batch_size=1:   3%|▎         | 3/100 [00:13<07:29,  4.63s/it]Measuring inference with batch_size=1:   4%|▍         | 4/100 [00:18<07:30,  4.70s/it]Measuring inference with batch_size=1:   5%|▌         | 5/100 [00:23<07:35,  4.79s/it]Measuring inference with batch_size=1:   6%|▌         | 6/100 [00:28<07:36,  4.86s/it]Measuring inference with batch_size=1:   7%|▋         | 7/100 [00:33<07:23,  4.77s/it]Measuring inference with batch_size=1:   8%|▊         | 8/100 [00:38<07:19,  4.78s/it]Measuring inference with batch_size=1:   9%|▉         | 9/100 [00:42<07:11,  4.74s/it]Measuring inference with batch_size=1:  10%|█         | 10/100 [00:47<07:02,  4.69s/it]Measuring inference with batch_size=1:  11%|█         | 11/100 [00:51<06:54,  4.66s/it]Measuring inference with batch_size=1:  12%|█▏        | 12/100 [00:56<06:56,  4.73s/it]Measuring inference with batch_size=1:  13%|█▎        | 13/100 [01:01<06:49,  4.71s/it]Measuring inference with batch_size=1:  14%|█▍        | 14/100 [01:06<06:45,  4.71s/it]Measuring inference with batch_size=1:  15%|█▌        | 15/100 [01:10<06:42,  4.73s/it]Measuring inference with batch_size=1:  16%|█▌        | 16/100 [01:16<06:47,  4.85s/it]Measuring inference with batch_size=1:  17%|█▋        | 17/100 [01:20<06:39,  4.82s/it]Measuring inference with batch_size=1:  18%|█▊        | 18/100 [01:25<06:35,  4.82s/it]Measuring inference with batch_size=1:  19%|█▉        | 19/100 [01:30<06:21,  4.71s/it]Measuring inference with batch_size=1:  20%|██        | 20/100 [01:35<06:22,  4.78s/it]Measuring inference with batch_size=1:  21%|██        | 21/100 [01:39<06:16,  4.76s/it]Measuring inference with batch_size=1:  22%|██▏       | 22/100 [01:44<06:03,  4.66s/it]Measuring inference with batch_size=1:  23%|██▎       | 23/100 [01:48<05:50,  4.55s/it]Measuring inference with batch_size=1:  24%|██▍       | 24/100 [01:52<05:42,  4.51s/it]Measuring inference with batch_size=1:  25%|██▌       | 25/100 [01:57<05:33,  4.45s/it]Measuring inference with batch_size=1:  26%|██▌       | 26/100 [02:01<05:25,  4.40s/it]Measuring inference with batch_size=1:  27%|██▋       | 27/100 [02:05<05:18,  4.37s/it]Measuring inference with batch_size=1:  28%|██▊       | 28/100 [02:10<05:16,  4.40s/it]Measuring inference with batch_size=1:  29%|██▉       | 29/100 [02:14<05:10,  4.38s/it]Measuring inference with batch_size=1:  30%|███       | 30/100 [02:19<05:12,  4.47s/it]Measuring inference with batch_size=1:  31%|███       | 31/100 [02:23<05:12,  4.53s/it]Measuring inference with batch_size=1:  32%|███▏      | 32/100 [02:28<05:05,  4.49s/it]Measuring inference with batch_size=1:  33%|███▎      | 33/100 [02:32<04:57,  4.43s/it]Measuring inference with batch_size=1:  34%|███▍      | 34/100 [02:37<04:53,  4.45s/it]Measuring inference with batch_size=1:  35%|███▌      | 35/100 [02:41<04:54,  4.53s/it]Measuring inference with batch_size=1:  36%|███▌      | 36/100 [02:46<04:50,  4.53s/it]Measuring inference with batch_size=1:  37%|███▋      | 37/100 [02:50<04:47,  4.56s/it]Measuring inference with batch_size=1:  38%|███▊      | 38/100 [02:55<04:50,  4.68s/it]Measuring inference with batch_size=1:  39%|███▉      | 39/100 [03:00<04:48,  4.74s/it]Measuring inference with batch_size=1:  40%|████      | 40/100 [03:06<04:54,  4.90s/it]Measuring inference with batch_size=1:  41%|████      | 41/100 [03:11<04:53,  4.98s/it]Measuring inference with batch_size=1:  42%|████▏     | 42/100 [03:15<04:40,  4.83s/it]Measuring inference with batch_size=1:  43%|████▎     | 43/100 [03:20<04:38,  4.89s/it]Measuring inference with batch_size=1:  44%|████▍     | 44/100 [03:25<04:38,  4.97s/it]Measuring inference with batch_size=1:  45%|████▌     | 45/100 [03:30<04:28,  4.88s/it]Measuring inference with batch_size=1:  46%|████▌     | 46/100 [03:34<04:14,  4.71s/it]Measuring inference with batch_size=1:  47%|████▋     | 47/100 [03:39<04:04,  4.62s/it]Measuring inference with batch_size=1:  48%|████▊     | 48/100 [03:43<03:56,  4.55s/it]Measuring inference with batch_size=1:  49%|████▉     | 49/100 [03:48<03:49,  4.50s/it]Measuring inference with batch_size=1:  50%|█████     | 50/100 [03:52<03:43,  4.46s/it]Measuring inference with batch_size=1:  51%|█████     | 51/100 [03:56<03:36,  4.42s/it]Measuring inference with batch_size=1:  52%|█████▏    | 52/100 [04:01<03:39,  4.58s/it]Measuring inference with batch_size=1:  53%|█████▎    | 53/100 [04:06<03:41,  4.71s/it]Measuring inference with batch_size=1:  54%|█████▍    | 54/100 [04:11<03:34,  4.67s/it]Measuring inference with batch_size=1:  55%|█████▌    | 55/100 [04:15<03:29,  4.65s/it]Measuring inference with batch_size=1:  56%|█████▌    | 56/100 [04:20<03:25,  4.66s/it]Measuring inference with batch_size=1:  57%|█████▋    | 57/100 [04:25<03:21,  4.69s/it]Measuring inference with batch_size=1:  58%|█████▊    | 58/100 [04:29<03:14,  4.64s/it]Measuring inference with batch_size=1:  59%|█████▉    | 59/100 [04:34<03:13,  4.71s/it]Measuring inference with batch_size=1:  60%|██████    | 60/100 [04:39<03:07,  4.69s/it]Measuring inference with batch_size=1:  61%|██████    | 61/100 [04:43<03:00,  4.62s/it]Measuring inference with batch_size=1:  62%|██████▏   | 62/100 [04:48<02:52,  4.54s/it]Measuring inference with batch_size=1:  63%|██████▎   | 63/100 [04:52<02:47,  4.53s/it]Measuring inference with batch_size=1:  64%|██████▍   | 64/100 [04:57<02:44,  4.57s/it]Measuring inference with batch_size=1:  65%|██████▌   | 65/100 [05:02<02:41,  4.61s/it]Measuring inference with batch_size=1:  66%|██████▌   | 66/100 [05:06<02:36,  4.61s/it]Measuring inference with batch_size=1:  67%|██████▋   | 67/100 [05:11<02:30,  4.57s/it]Measuring inference with batch_size=1:  68%|██████▊   | 68/100 [05:15<02:26,  4.57s/it]Measuring inference with batch_size=1:  69%|██████▉   | 69/100 [05:20<02:19,  4.50s/it]Measuring inference with batch_size=1:  70%|███████   | 70/100 [05:24<02:13,  4.44s/it]Measuring inference with batch_size=1:  71%|███████   | 71/100 [05:28<02:07,  4.40s/it]Measuring inference with batch_size=1:  72%|███████▏  | 72/100 [05:33<02:02,  4.37s/it]Measuring inference with batch_size=1:  73%|███████▎  | 73/100 [05:37<01:59,  4.41s/it]Measuring inference with batch_size=1:  74%|███████▍  | 74/100 [05:42<01:55,  4.44s/it]Measuring inference with batch_size=1:  75%|███████▌  | 75/100 [05:46<01:52,  4.49s/it]Measuring inference with batch_size=1:  76%|███████▌  | 76/100 [05:51<01:48,  4.53s/it]Measuring inference with batch_size=1:  77%|███████▋  | 77/100 [05:55<01:45,  4.57s/it]Measuring inference with batch_size=1:  78%|███████▊  | 78/100 [06:00<01:40,  4.58s/it]Measuring inference with batch_size=1:  79%|███████▉  | 79/100 [06:04<01:34,  4.51s/it]Measuring inference with batch_size=1:  80%|████████  | 80/100 [06:09<01:30,  4.52s/it]Measuring inference with batch_size=1:  81%|████████  | 81/100 [06:14<01:26,  4.57s/it]Measuring inference with batch_size=1:  82%|████████▏ | 82/100 [06:18<01:22,  4.56s/it]Measuring inference with batch_size=1:  83%|████████▎ | 83/100 [06:23<01:18,  4.61s/it]Measuring inference with batch_size=1:  84%|████████▍ | 84/100 [06:27<01:13,  4.59s/it]Measuring inference with batch_size=1:  85%|████████▌ | 85/100 [06:32<01:08,  4.59s/it]Measuring inference with batch_size=1:  86%|████████▌ | 86/100 [06:36<01:03,  4.51s/it]Measuring inference with batch_size=1:  87%|████████▋ | 87/100 [06:41<00:58,  4.50s/it]Measuring inference with batch_size=1:  88%|████████▊ | 88/100 [06:46<00:56,  4.69s/it]Measuring inference with batch_size=1:  89%|████████▉ | 89/100 [06:51<00:51,  4.71s/it]Measuring inference with batch_size=1:  90%|█████████ | 90/100 [06:55<00:46,  4.70s/it]Measuring inference with batch_size=1:  91%|█████████ | 91/100 [07:00<00:42,  4.72s/it]Measuring inference with batch_size=1:  92%|█████████▏| 92/100 [07:05<00:38,  4.86s/it]Measuring inference with batch_size=1:  93%|█████████▎| 93/100 [07:10<00:32,  4.71s/it]Measuring inference with batch_size=1:  94%|█████████▍| 94/100 [07:15<00:28,  4.82s/it]Measuring inference with batch_size=1:  95%|█████████▌| 95/100 [07:19<00:23,  4.75s/it]Measuring inference with batch_size=1:  96%|█████████▌| 96/100 [07:24<00:18,  4.68s/it]Measuring inference with batch_size=1:  97%|█████████▋| 97/100 [07:28<00:13,  4.58s/it]Measuring inference with batch_size=1:  98%|█████████▊| 98/100 [07:33<00:09,  4.58s/it]Measuring inference with batch_size=1:  99%|█████████▉| 99/100 [07:37<00:04,  4.54s/it]Measuring inference with batch_size=1: 100%|██████████| 100/100 [07:42<00:00,  4.67s/it]Measuring inference with batch_size=1: 100%|██████████| 100/100 [07:42<00:00,  4.63s/it]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
INFO:benchmark:learner.infer:
  device: cpu
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 6
        total: 12
      frequency: 2.60 GHz
      model: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz
    gpus: null
    memory:
      available: 5.36 GB
      total: 16.00 GB
      used: 8.50 GB
    system:
      node: d40049
      release: 21.2.0
      system: Darwin
  timing:
    batch_size_1:
      on_device_inference:
        human_readable:
          batch_latency: 4.627 s +/- 238.990 ms [4.282 s, 5.285 s]
          batches_per_second: 0.22 +/- 0.01 [0.19, 0.23]
        metrics:
          batches_per_second_max: 0.2335600587566451
          batches_per_second_mean: 0.21669085806150393
          batches_per_second_min: 0.18919750616606548
          batches_per_second_std: 0.010884068969615074
          seconds_per_batch_max: 5.285481929779053
          seconds_per_batch_mean: 4.626852233409881
          seconds_per_batch_min: 4.2815539836883545
          seconds_per_batch_std: 0.23898991250549537

INFO:benchmark:== Benchmarking model directly ==
WARNING:torch-benchmark:Measurement of allocated memory is only available on CUDA devices
Warming up with batch_size=1:   0%|          | 0/10 [00:00<?, ?it/s]Warming up with batch_size=1:  10%|█         | 1/10 [00:04<00:43,  4.88s/it]Warming up with batch_size=1:  20%|██        | 2/10 [00:09<00:37,  4.70s/it]Warming up with batch_size=1:  30%|███       | 3/10 [00:13<00:32,  4.62s/it]Warming up with batch_size=1:  40%|████      | 4/10 [00:18<00:27,  4.61s/it]Warming up with batch_size=1:  50%|█████     | 5/10 [00:23<00:22,  4.56s/it]Warming up with batch_size=1:  60%|██████    | 6/10 [00:27<00:18,  4.58s/it]Warming up with batch_size=1:  70%|███████   | 7/10 [00:32<00:14,  4.74s/it]Warming up with batch_size=1:  80%|████████  | 8/10 [00:37<00:09,  4.79s/it]Warming up with batch_size=1:  90%|█████████ | 9/10 [00:42<00:04,  4.77s/it]Warming up with batch_size=1: 100%|██████████| 10/10 [00:46<00:00,  4.68s/it]Warming up with batch_size=1: 100%|██████████| 10/10 [00:46<00:00,  4.68s/it]
Measuring inference with batch_size=1:   0%|          | 0/100 [00:00<?, ?it/s]Measuring inference with batch_size=1:   1%|          | 1/100 [00:04<07:27,  4.52s/it]Measuring inference with batch_size=1:   2%|▏         | 2/100 [00:09<07:21,  4.50s/it]Measuring inference with batch_size=1:   3%|▎         | 3/100 [00:13<07:29,  4.63s/it]Measuring inference with batch_size=1:   4%|▍         | 4/100 [00:18<07:43,  4.82s/it]Measuring inference with batch_size=1:   5%|▌         | 5/100 [00:23<07:29,  4.73s/it]Measuring inference with batch_size=1:   6%|▌         | 6/100 [00:28<07:27,  4.76s/it]Measuring inference with batch_size=1:   7%|▋         | 7/100 [00:32<07:09,  4.62s/it]Measuring inference with batch_size=1:   8%|▊         | 8/100 [00:36<06:56,  4.53s/it]Measuring inference with batch_size=1:   9%|▉         | 9/100 [00:41<06:53,  4.54s/it]Measuring inference with batch_size=1:  10%|█         | 10/100 [00:46<06:52,  4.59s/it]Measuring inference with batch_size=1:  11%|█         | 11/100 [00:50<06:49,  4.60s/it]Measuring inference with batch_size=1:  12%|█▏        | 12/100 [00:55<06:47,  4.63s/it]Measuring inference with batch_size=1:  13%|█▎        | 13/100 [01:00<06:41,  4.61s/it]Measuring inference with batch_size=1:  14%|█▍        | 14/100 [01:04<06:37,  4.63s/it]Measuring inference with batch_size=1:  15%|█▌        | 15/100 [01:09<06:36,  4.66s/it]Measuring inference with batch_size=1:  16%|█▌        | 16/100 [01:14<06:30,  4.65s/it]Measuring inference with batch_size=1:  17%|█▋        | 17/100 [01:18<06:26,  4.66s/it]Measuring inference with batch_size=1:  18%|█▊        | 18/100 [01:23<06:19,  4.63s/it]Measuring inference with batch_size=1:  19%|█▉        | 19/100 [01:28<06:18,  4.67s/it]Measuring inference with batch_size=1:  20%|██        | 20/100 [01:32<06:11,  4.64s/it]Measuring inference with batch_size=1:  21%|██        | 21/100 [01:37<06:04,  4.61s/it]Measuring inference with batch_size=1:  22%|██▏       | 22/100 [01:41<05:54,  4.55s/it]Measuring inference with batch_size=1:  23%|██▎       | 23/100 [01:46<05:47,  4.51s/it]Measuring inference with batch_size=1:  24%|██▍       | 24/100 [01:50<05:44,  4.54s/it]Measuring inference with batch_size=1:  25%|██▌       | 25/100 [01:55<05:49,  4.66s/it]Measuring inference with batch_size=1:  26%|██▌       | 26/100 [02:00<05:45,  4.67s/it]Measuring inference with batch_size=1:  27%|██▋       | 27/100 [02:04<05:40,  4.67s/it]Measuring inference with batch_size=1:  28%|██▊       | 28/100 [02:09<05:33,  4.64s/it]Measuring inference with batch_size=1:  29%|██▉       | 29/100 [02:14<05:27,  4.61s/it]Measuring inference with batch_size=1:  30%|███       | 30/100 [02:18<05:21,  4.59s/it]Measuring inference with batch_size=1:  31%|███       | 31/100 [02:23<05:13,  4.55s/it]Measuring inference with batch_size=1:  32%|███▏      | 32/100 [02:27<05:09,  4.55s/it]Measuring inference with batch_size=1:  33%|███▎      | 33/100 [02:32<05:08,  4.61s/it]Measuring inference with batch_size=1:  34%|███▍      | 34/100 [02:37<05:04,  4.62s/it]Measuring inference with batch_size=1:  35%|███▌      | 35/100 [02:41<05:01,  4.63s/it]Measuring inference with batch_size=1:  36%|███▌      | 36/100 [02:46<04:52,  4.57s/it]Measuring inference with batch_size=1:  37%|███▋      | 37/100 [02:51<04:59,  4.75s/it]Measuring inference with batch_size=1:  38%|███▊      | 38/100 [02:56<04:55,  4.77s/it]Measuring inference with batch_size=1:  39%|███▉      | 39/100 [03:01<04:56,  4.85s/it]Measuring inference with batch_size=1:  40%|████      | 40/100 [03:05<04:47,  4.79s/it]Measuring inference with batch_size=1:  41%|████      | 41/100 [03:10<04:40,  4.75s/it]Measuring inference with batch_size=1:  42%|████▏     | 42/100 [03:14<04:30,  4.67s/it]Measuring inference with batch_size=1:  43%|████▎     | 43/100 [03:19<04:20,  4.57s/it]Measuring inference with batch_size=1:  44%|████▍     | 44/100 [03:23<04:12,  4.51s/it]Measuring inference with batch_size=1:  45%|████▌     | 45/100 [03:27<04:05,  4.46s/it]Measuring inference with batch_size=1:  46%|████▌     | 46/100 [03:32<03:58,  4.42s/it]Measuring inference with batch_size=1:  47%|████▋     | 47/100 [03:37<04:01,  4.55s/it]Measuring inference with batch_size=1:  48%|████▊     | 48/100 [03:41<03:56,  4.55s/it]Measuring inference with batch_size=1:  49%|████▉     | 49/100 [03:46<03:53,  4.58s/it]Measuring inference with batch_size=1:  50%|█████     | 50/100 [03:50<03:49,  4.58s/it]Measuring inference with batch_size=1:  51%|█████     | 51/100 [03:55<03:44,  4.59s/it]Measuring inference with batch_size=1:  52%|█████▏    | 52/100 [04:00<03:39,  4.56s/it]Measuring inference with batch_size=1:  53%|█████▎    | 53/100 [04:04<03:35,  4.58s/it]Measuring inference with batch_size=1:  54%|█████▍    | 54/100 [04:09<03:33,  4.65s/it]Measuring inference with batch_size=1:  55%|█████▌    | 55/100 [04:14<03:29,  4.66s/it]Measuring inference with batch_size=1:  56%|█████▌    | 56/100 [04:18<03:22,  4.61s/it]Measuring inference with batch_size=1:  57%|█████▋    | 57/100 [04:23<03:16,  4.58s/it]Measuring inference with batch_size=1:  58%|█████▊    | 58/100 [04:27<03:14,  4.62s/it]Measuring inference with batch_size=1:  59%|█████▉    | 59/100 [04:32<03:08,  4.59s/it]Measuring inference with batch_size=1:  60%|██████    | 60/100 [04:37<03:03,  4.59s/it]Measuring inference with batch_size=1:  61%|██████    | 61/100 [04:41<02:57,  4.55s/it]Measuring inference with batch_size=1:  62%|██████▏   | 62/100 [04:46<02:54,  4.59s/it]Measuring inference with batch_size=1:  63%|██████▎   | 63/100 [04:50<02:51,  4.63s/it]Measuring inference with batch_size=1:  64%|██████▍   | 64/100 [04:55<02:46,  4.64s/it]Measuring inference with batch_size=1:  65%|██████▌   | 65/100 [05:00<02:42,  4.65s/it]Measuring inference with batch_size=1:  66%|██████▌   | 66/100 [05:04<02:37,  4.64s/it]Measuring inference with batch_size=1:  67%|██████▋   | 67/100 [05:09<02:33,  4.64s/it]Measuring inference with batch_size=1:  68%|██████▊   | 68/100 [05:14<02:29,  4.68s/it]Measuring inference with batch_size=1:  69%|██████▉   | 69/100 [05:19<02:26,  4.73s/it]Measuring inference with batch_size=1:  70%|███████   | 70/100 [05:23<02:19,  4.66s/it]Measuring inference with batch_size=1:  71%|███████   | 71/100 [05:28<02:14,  4.63s/it]Measuring inference with batch_size=1:  72%|███████▏  | 72/100 [05:32<02:08,  4.60s/it]Measuring inference with batch_size=1:  73%|███████▎  | 73/100 [05:37<02:04,  4.62s/it]Measuring inference with batch_size=1:  74%|███████▍  | 74/100 [05:41<02:00,  4.63s/it]Measuring inference with batch_size=1:  75%|███████▌  | 75/100 [05:46<01:56,  4.68s/it]Measuring inference with batch_size=1:  76%|███████▌  | 76/100 [05:51<01:50,  4.62s/it]Measuring inference with batch_size=1:  77%|███████▋  | 77/100 [05:55<01:45,  4.60s/it]Measuring inference with batch_size=1:  78%|███████▊  | 78/100 [06:00<01:41,  4.62s/it]Measuring inference with batch_size=1:  79%|███████▉  | 79/100 [06:05<01:38,  4.69s/it]Measuring inference with batch_size=1:  80%|████████  | 80/100 [06:10<01:33,  4.69s/it]Measuring inference with batch_size=1:  81%|████████  | 81/100 [06:14<01:29,  4.71s/it]Measuring inference with batch_size=1:  82%|████████▏ | 82/100 [06:19<01:23,  4.64s/it]Measuring inference with batch_size=1:  83%|████████▎ | 83/100 [06:23<01:18,  4.63s/it]Measuring inference with batch_size=1:  84%|████████▍ | 84/100 [06:28<01:14,  4.63s/it]Measuring inference with batch_size=1:  85%|████████▌ | 85/100 [06:33<01:10,  4.68s/it]Measuring inference with batch_size=1:  86%|████████▌ | 86/100 [06:37<01:05,  4.65s/it]Measuring inference with batch_size=1:  87%|████████▋ | 87/100 [06:42<01:00,  4.66s/it]Measuring inference with batch_size=1:  88%|████████▊ | 88/100 [06:47<00:55,  4.65s/it]Measuring inference with batch_size=1:  89%|████████▉ | 89/100 [06:51<00:51,  4.67s/it]Measuring inference with batch_size=1:  90%|█████████ | 90/100 [06:56<00:46,  4.70s/it]Measuring inference with batch_size=1:  91%|█████████ | 91/100 [07:01<00:42,  4.71s/it]Measuring inference with batch_size=1:  92%|█████████▏| 92/100 [07:06<00:37,  4.72s/it]Measuring inference with batch_size=1:  93%|█████████▎| 93/100 [07:10<00:32,  4.68s/it]Measuring inference with batch_size=1:  94%|█████████▍| 94/100 [07:15<00:28,  4.80s/it]Measuring inference with batch_size=1:  95%|█████████▌| 95/100 [07:21<00:24,  4.96s/it]Measuring inference with batch_size=1:  96%|█████████▌| 96/100 [07:25<00:19,  4.92s/it]Measuring inference with batch_size=1:  97%|█████████▋| 97/100 [07:30<00:14,  4.87s/it]Measuring inference with batch_size=1:  98%|█████████▊| 98/100 [07:35<00:09,  4.79s/it]Measuring inference with batch_size=1:  99%|█████████▉| 99/100 [07:39<00:04,  4.70s/it]Measuring inference with batch_size=1: 100%|██████████| 100/100 [07:44<00:00,  4.62s/it]Measuring inference with batch_size=1: 100%|██████████| 100/100 [07:44<00:00,  4.64s/it]
ERROR:torch-benchmark:Unable to measure energy consumption. Device must be a NVIDIA Jetson.
INFO:benchmark:learner.model.forward:
  device: cpu
  flops: 19166052038
  machine_info:
    cpu:
      architecture: x86_64
      cores:
        physical: 6
        total: 12
      frequency: 2.60 GHz
      model: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz
    gpus: null
    memory:
      available: 5.23 GB
      total: 16.00 GB
      used: 8.70 GB
    system:
      node: d40049
      release: 21.2.0
      system: Darwin
  params: 6153432
  timing:
    batch_size_1:
      on_device_inference:
        human_readable:
          batch_latency: 4.642 s +/- 174.082 ms [4.328 s, 5.325 s]
          batches_per_second: 0.22 +/- 0.01 [0.19, 0.23]
        metrics:
          batches_per_second_max: 0.23105515361630194
          batches_per_second_mean: 0.2157151352645609
          batches_per_second_min: 0.18780657682843474
          batches_per_second_std: 0.007825200719261583
          seconds_per_batch_max: 5.324627161026001
          seconds_per_batch_mean: 4.642044770717621
          seconds_per_batch_min: 4.3279709815979
          seconds_per_batch_std: 0.1740818365320548

